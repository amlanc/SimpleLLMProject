{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8a77807f92f26ee",
   "metadata": {},
   "source": [
    "# This is an attempt to learn by building and training an LLM from Scratch\n",
    "## Chapter 01  "
   ]
  },
  {
   "cell_type": "code",
   "id": "fbc121e30a2defb3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-09T19:20:31.851420Z",
     "start_time": "2025-02-09T19:20:31.845005Z"
    }
   },
   "source": [
    "import os\n",
    "import torch\n",
    "from torch import Tensor\n",
    "import urllib.request\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Check which GPU if any is available\n",
    "# torch.cuda.is_available():\n",
    "#     device = torch.device(\"cuda\")\n",
    "#     x: Tensor = torch.ones(1, device=device)\n",
    "#     print(f\"x = {x} using 'cuda:0' backend\")\n",
    "#     \n",
    "# elif \n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "    x: Tensor = torch.ones(1, device=device)\n",
    "    print(f\"x = {x} using {device} backend\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    # x: Tensor = torch.ones(1, device=device)\n",
    " \n",
    "print(\"Running on : \", device)\n",
    "\n",
    "def get_some_text():\n",
    "    # Download a text (book)\n",
    "    bookUrl = \"https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/ch02/01_main-chapter-code/the-verdict.txt\"  \n",
    "    filepath = \"../data/the-verdict.txt\"\n",
    "    # print(file_path)\n",
    "    if not os.path.exists(filepath):\n",
    "        urllib.request.urlretrieve(bookUrl, filepath)\n",
    "\n",
    "    with open(filepath, \"r\", encoding=\"utf-8\") as f:\n",
    "        rawtext = f.read()\n",
    "        \n",
    "    print(\"Total characters in the story: \", len(rawtext))\n",
    "    print(\"Total Lines in raw text: \", rawtext.count(\"\\n\"))\n",
    "    return rawtext\n",
    "\n",
    "raw_text = get_some_text()\n",
    "print(\"Some text: \", raw_text[:49])"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x = tensor([1.], device='mps:0') using mps backend\n",
      "Running on :  mps\n",
      "Total characters in the story:  20479\n",
      "Total Lines in raw text:  164\n",
      "Some text:  I HAD always thought Jack Gisburn rather a cheap \n"
     ]
    }
   ],
   "execution_count": 52
  },
  {
   "cell_type": "code",
   "id": "2c7227e79afbcad7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-09T19:20:31.865055Z",
     "start_time": "2025-02-09T19:20:31.862188Z"
    }
   },
   "source": [
    "# Now we have to tokenize the text. The best way to do that is to use a pre-build tokennizer, but first we will try some \n",
    "# basic python regular expressions to do the same things\n",
    "import re\n",
    "preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', raw_text)\n",
    "preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
    "#\n",
    "print(len(preprocessed))\n",
    "print(preprocessed[:30])"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4690\n",
      "['I', 'HAD', 'always', 'thought', 'Jack', 'Gisburn', 'rather', 'a', 'cheap', 'genius', '--', 'though', 'a', 'good', 'fellow', 'enough', '--', 'so', 'it', 'was', 'no', 'great', 'surprise', 'to', 'me', 'to', 'hear', 'that', ',', 'in']\n"
     ]
    }
   ],
   "execution_count": 53
  },
  {
   "cell_type": "code",
   "id": "b9dc98b584bc6d61",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-09T19:20:43.977262Z",
     "start_time": "2025-02-09T19:20:43.974489Z"
    }
   },
   "source": [
    "# Now we need to generate token IDs\n",
    "# Now let us create a list of all unique tokens and sort them alphabetically to determine the vocabulary size\n",
    "all_uniq_words = sorted(set(preprocessed))\n",
    "vocab_size = len(all_uniq_words)\n",
    "print(\"Vocab size: \", vocab_size)\n",
    "\n",
    "# Now that we know the vocabulary size, lets enumerate and assign some numbers to them\n",
    "vocab = {token:integer for integer,token in enumerate(all_uniq_words)}\n",
    "for i, item in enumerate(vocab.items()):\n",
    "    print(item)\n",
    "    if i >= 20:\n",
    "        break\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size:  1130\n",
      "('!', 0)\n",
      "('\"', 1)\n",
      "(\"'\", 2)\n",
      "('(', 3)\n",
      "(')', 4)\n",
      "(',', 5)\n",
      "('--', 6)\n",
      "('.', 7)\n",
      "(':', 8)\n",
      "(';', 9)\n",
      "('?', 10)\n",
      "('A', 11)\n",
      "('Ah', 12)\n",
      "('Among', 13)\n",
      "('And', 14)\n",
      "('Are', 15)\n",
      "('Arrt', 16)\n",
      "('As', 17)\n",
      "('At', 18)\n",
      "('Be', 19)\n",
      "('Begin', 20)\n"
     ]
    }
   ],
   "execution_count": 54
  },
  {
   "cell_type": "code",
   "id": "b49e9060996c9f10",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-09T19:20:56.224933Z",
     "start_time": "2025-02-09T19:20:56.222683Z"
    }
   },
   "source": [
    "from src.chapter02.SimpleTokenizerV1 import SimpleTokenizerV1\n",
    "\n",
    "# Now we want to apply this vocabulary to convert new text to generate token id\n",
    "# When we want to convert the outputs of an LLM from numbers back into text, we need a way to turn token IDs into text. \n",
    "# For this, we can create an inverse version of the vocabulary that maps token IDs back to the corresponding text tokens.\n",
    "\n",
    "tokenizer = SimpleTokenizerV1(vocab)\n",
    "text = \"\"\"\"It's the last he painted you know,\" \n",
    "        Mrs Gisburn said with pardonable pride.\"\"\"\n",
    "\n",
    "ids = tokenizer.encode(text)\n",
    "print(ids)\n",
    "print(tokenizer.decode(ids))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 56, 2, 850, 988, 602, 533, 746, 1126, 596, 5, 1, 67, 38, 851, 1108, 754, 793, 7]\n",
      "\" It' s the last he painted you know,\" Mrs Gisburn said with pardonable pride.\n"
     ]
    }
   ],
   "execution_count": 55
  },
  {
   "cell_type": "code",
   "id": "92b6519fe1741db8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-09T19:21:08.472763Z",
     "start_time": "2025-02-09T19:21:08.470206Z"
    }
   },
   "source": [
    "all_tokens = sorted(list(set(preprocessed))) # Make preprocessed a list so we can extend it\n",
    "all_tokens.extend([\"<|unk|>\", \"<|endoftext|>\"])\n",
    "# redo the vocab population\n",
    "vocab = {token:integer for integer, token in enumerate(all_tokens)}\n",
    "print(len(vocab.items()))\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1132\n"
     ]
    }
   ],
   "execution_count": 56
  },
  {
   "cell_type": "code",
   "id": "d03ae607c2d20268",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-09T19:21:32.719219Z",
     "start_time": "2025-02-09T19:21:32.717032Z"
    }
   },
   "source": [
    "# Print the last 5 vocab items\n",
    "for i, item in enumerate(list(vocab.items())[-5:]):\n",
    "    print(item)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('younger', 1127)\n",
      "('your', 1128)\n",
      "('yourself', 1129)\n",
      "('<|unk|>', 1130)\n",
      "('<|endoftext|>', 1131)\n"
     ]
    }
   ],
   "execution_count": 57
  },
  {
   "cell_type": "code",
   "id": "dcdf224cd056d26f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-09T19:21:32.745075Z",
     "start_time": "2025-02-09T19:21:32.742825Z"
    }
   },
   "source": [
    "from src.chapter02.SimpleTokenizerV2 import SimpleTokenizerV2\n",
    "\n",
    "text1 = \"Hello, do you like tea?\"\n",
    "text2 = \"In the sunlit terraces of the palace.\"\n",
    "text = \" <|endoftext|> \".join((text1, text2))\n",
    "print(text)\n",
    "tokenizer = SimpleTokenizerV2(vocab)\n",
    "print(tokenizer.encode(text))\n",
    "print(tokenizer.decode(tokenizer.encode(text)))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, do you like tea? <|endoftext|> In the sunlit terraces of the palace.\n",
      "[1130, 5, 355, 1126, 628, 975, 10, 1131, 55, 988, 956, 984, 722, 988, 1130, 7]\n",
      "<|unk|>, do you like tea? <|endoftext|> In the sunlit terraces of the <|unk|>.\n"
     ]
    }
   ],
   "execution_count": 58
  },
  {
   "cell_type": "code",
   "id": "e4e4738351ee6b9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-09T19:21:45.603369Z",
     "start_time": "2025-02-09T19:21:45.600387Z"
    }
   },
   "source": [
    "### Byte Pair Encoding \n",
    "from importlib.metadata import version\n",
    "import tiktoken\n",
    "print(\"Tiktoken version: \", version(\"tiktoken\"))\n",
    "#print(\"Tiktoken version: \", tiktoken.__version__)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tiktoken version:  0.8.0\n"
     ]
    }
   ],
   "execution_count": 59
  },
  {
   "cell_type": "code",
   "id": "ac57b0bafdc676f4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-09T19:21:58.516193Z",
     "start_time": "2025-02-09T19:21:58.513458Z"
    }
   },
   "source": [
    "#### This is the tokenizer using the GPT2 tokenization model\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "text = \"Hello, do you like tea? <|endoftext|> In the sunlit terraces of someunknownPlace.\"\n",
    "integers = tokenizer.encode(text, allowed_special={\"<|endoftext|>\"})\n",
    "print(f\"Encoded: {integers}\")\n",
    "strings = tokenizer.decode(integers)\n",
    "print(f\"Decoded: {strings}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded: [15496, 11, 466, 345, 588, 8887, 30, 220, 50256, 554, 262, 4252, 18250, 8812, 2114, 286, 617, 34680, 27271, 13]\n",
      "Decoded: Hello, do you like tea? <|endoftext|> In the sunlit terraces of someunknownPlace.\n"
     ]
    }
   ],
   "execution_count": 60
  },
  {
   "cell_type": "code",
   "id": "2a6a5225d4fab946",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-09T19:22:11.262028Z",
     "start_time": "2025-02-09T19:22:11.260094Z"
    }
   },
   "source": [
    "print(tokenizer.encode(\"Akwirw ier\"))\n",
    "print(tokenizer.decode(tokenizer.encode(\"Akwirw ier\")))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[33901, 86, 343, 86, 220, 959]\n",
      "Akwirw ier\n"
     ]
    }
   ],
   "execution_count": 61
  },
  {
   "cell_type": "code",
   "id": "1ab2edd9fb1ca03",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-09T19:22:36.370174Z",
     "start_time": "2025-02-09T19:22:36.366014Z"
    }
   },
   "source": [
    "# Let's now do Data Sampling with a sliding window\n",
    "# 1. Let's tokenize the entire story with BPE tokenizer first\n",
    "\n",
    "encoded_text = tokenizer.encode(raw_text)\n",
    "print(len(encoded_text))\n",
    "enc_sample = encoded_text[50:]\n",
    "\n",
    "# Now Let's start by defining x and y where x has input tokens and y the output tokens shifted by 1\n",
    "context_size = 4\n",
    "x = enc_sample[:context_size]\n",
    "y = enc_sample[1:context_size+1]\n",
    "# print(f\"x: {x}\")\n",
    "# print(f\"y:      {y}\")\n",
    "\n",
    "\n",
    "#####\n",
    "# Next word prediction tasks can now be created by \n",
    "for i in range(1, context_size + 1):\n",
    "    context = enc_sample[:i]\n",
    "    desired = enc_sample[i]\n",
    "    # print(f\"context input: {context} --> desired prediction: {desired}\")\n",
    "    # Now we create the input output target pairs\n",
    "    print(tokenizer.decode(context), \"---->\", tokenizer.decode([desired]))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5145\n",
      " and ---->  established\n",
      " and established ---->  himself\n",
      " and established himself ---->  in\n",
      " and established himself in ---->  a\n"
     ]
    }
   ],
   "execution_count": 62
  },
  {
   "cell_type": "code",
   "id": "62094a4513e01008",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-09T19:22:36.390972Z",
     "start_time": "2025-02-09T19:22:36.389226Z"
    }
   },
   "source": [
    "# from Dataloader import Dataloader\n",
    "# \n",
    "# dataloader = Dataloader(batch_size=8, max_length=4, stride=4, shuffle=False, drop_last=True, num_workers=0)\n",
    "# dataloader = dataloader.get_instance(file_path, text_enc='utf-8', mode='r')\n",
    "# if dataloader is not None:\n",
    "#     data_iter = iter(dataloader)\n",
    "#     inputs, targets = next(data_iter)\n",
    "#     print(\"Loaded text data...\\n\")\n",
    "#     print(\"Inputs: \\n\", inputs)\n",
    "#     print(\"\\nTargets: \\n\", targets)\n",
    "# else: \n",
    "#     print(\"Failed loading \", dataloader)\n"
   ],
   "outputs": [],
   "execution_count": 63
  },
  {
   "cell_type": "code",
   "id": "fa0c4b5f389cdc4e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-09T19:23:00.535280Z",
     "start_time": "2025-02-09T19:23:00.291552Z"
    }
   },
   "source": [
    "import torch.nn\n",
    "from src.chapter02.Dataloader import Dataloader\n",
    "file_path = \"../data/the-verdict.txt\"\n",
    "\n",
    "####\n",
    "# Finally we need to create the embeddings for the tokens\n",
    "# If we have a batch size of 8 with 4 tokens each it'll be an 8 x 4 x 256 tensor\n",
    "max_length = 4  \n",
    "\n",
    "mydataloader = Dataloader(batch_size=8, max_length=max_length, stride=4, shuffle=False, drop_last=True, num_workers=0)\n",
    "dataloader = mydataloader.create_dataloader_v1(txt=raw_text)\n",
    "data_iter = iter(dataloader)\n",
    "inputs, targets = next(data_iter)\n",
    "# print(\"Input Token IDs:\\n\", inputs)\n",
    "# print(\"Input tensor shape: \", inputs.shape) \n",
    "\n",
    "# Now since self-attentions are position agnostic, we should add some positional data.\n",
    "# Absolute and relative positional data can be added. So let's create embeddings with say 256 dimensions\n",
    "vocab_size = 50257\n",
    "output_dim = 256\n",
    "# context_length = 1024\n",
    "\n",
    "## Now lets embed the input tensors\n",
    "token_embedding_layer = torch.nn.Embedding(num_embeddings=vocab_size, embedding_dim=output_dim)\n",
    "token_embeddings = token_embedding_layer(inputs)\n",
    "print(\"Token embeddings shape: \", token_embeddings.shape) #8x4x256\n",
    "\n",
    "\n",
    "# For a GPT model’s absolute position embedding approach, we just need to create another embedding \n",
    "# layer that has the same embedding dimension as the token_embedding_ layer:\n",
    "context_length = max_length     #context is length of positions we care about for attention\n",
    "pos_embedding_layer = torch.nn.Embedding(context_length, output_dim)\n",
    "pos_embeddings = pos_embedding_layer(torch.arange(context_length))\n",
    "print(\"Positional Embeddings Shape: \", pos_embeddings.shape) # 4x256\n",
    "#\n",
    "# Add the positional embeddings to token embeddings\n",
    "input_embeddings = token_embeddings + pos_embeddings\n",
    "print(\"Position Merged Input Embeddings Shape: \", input_embeddings.shape)\n",
    "#\n",
    "# Now lets look at the dataloader\n",
    "for batch in dataloader:\n",
    "    inputs, targets = batch\n",
    "    token_embeddings = token_embedding_layer(inputs)\n",
    "    pos_embeddings = pos_embedding_layer(torch.arange(max_length))\n",
    "    input_embeddings = token_embeddings + pos_embeddings\n",
    "    break\n",
    "#\n",
    "print(\"Batch Embeddings Shape: \", input_embeddings.shape)\n",
    "    \n",
    "print(\"Input tensor \", x)\n",
    "print(\"Target tensor\", y)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token embeddings shape:  torch.Size([8, 4, 256])\n",
      "Positional Embeddings Shape:  torch.Size([4, 256])\n",
      "Position Merged Input Embeddings Shape:  torch.Size([8, 4, 256])\n",
      "Batch Embeddings Shape:  torch.Size([8, 4, 256])\n",
      "Input tensor  [290, 4920, 2241, 287]\n",
      "Target tensor [4920, 2241, 287, 257]\n"
     ]
    }
   ],
   "execution_count": 64
  },
  {
   "cell_type": "code",
   "id": "95c2c3a6eb3eea50",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-09T19:23:12.520234Z",
     "start_time": "2025-02-09T19:23:12.516764Z"
    }
   },
   "source": [
    "# Chapter 3 - Attention\n",
    "#\n",
    "import torch\n",
    "\n",
    "# In self-attention our goal is to calculate context vector z(i) for each \n",
    "# element x(i) of the input sequence. Consider the following input sequence \n",
    "#\n",
    "inputs = torch.tensor(\n",
    "  [[0.43, 0.15, 0.89], # Your     (x^1)\n",
    "   [0.55, 0.87, 0.66], # journey  (x^2)\n",
    "   [0.57, 0.85, 0.64], # starts   (x^3)\n",
    "   [0.22, 0.58, 0.33], # with     (x^4)\n",
    "   [0.77, 0.25, 0.10], # one      (x^5)\n",
    "   [0.05, 0.80, 0.55]] # step     (x^6)\n",
    ")\n",
    "# inputs.to(device)\n",
    "print(\"Input sequence shape: \", inputs.shape)\n",
    "# \n",
    "# Now calculate weights for attention\n",
    "# Assume query is the second word \"journey\" or inputs[1] \n",
    "#\n",
    "query = inputs[1]\n",
    "# query.to(device)\n",
    "print(f\"Query is the 2nd word 'journey': {query}\")\n",
    "#\n",
    "attention_scores_2 = torch.empty(inputs.shape[0])\n",
    "# attention_scores_2.to(device)\n",
    "for idx, x_i in enumerate(inputs):\n",
    "    attention_scores_2[idx] = torch.dot(x_i, query)\n",
    "#    print(f\"Sequence Element [{idx}], attention_score: {attention_scores_2}\")\n",
    "print(f\"Final value of attention_score_2: {attention_scores_2}\")\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input sequence shape:  torch.Size([6, 3])\n",
      "Query is the 2nd word 'journey': tensor([0.5500, 0.8700, 0.6600])\n",
      "Final value of attention_score_2: tensor([0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865])\n"
     ]
    }
   ],
   "execution_count": 65
  },
  {
   "cell_type": "code",
   "id": "53ede1deb7d5678e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-09T19:23:12.540612Z",
     "start_time": "2025-02-09T19:23:12.535106Z"
    }
   },
   "source": [
    "\n",
    "## Note: For all elements if we were to calculate attention it'd be a O(n^2) operation\n",
    "# NOW we normalize the attention weights, so they sum up to 1\n",
    "attention_weights_2_tmp = attention_scores_2 / attention_scores_2.sum()\n",
    "# attention_weights_2_tmp.to(device)\n",
    "print(\"Normalized attention weights:\", attention_weights_2_tmp)\n",
    "print(\"Sum of attention weights:\", attention_weights_2_tmp.sum())\n",
    "\n",
    "## Generally we normalize using the softmax to do the normalization\n",
    "\n",
    "# # define a softmax function\n",
    "def softmax_naive(tensor_x):\n",
    "    return torch.exp(tensor_x) / torch.exp(tensor_x).sum(dim=0, keepdim=True)\n",
    "# \n",
    "\n",
    "attention_scores_2_naive = softmax_naive(attention_scores_2)\n",
    "# attention_scores_2_naive.to(device)\n",
    "\n",
    "print(\"Attention weights naive:\", attention_scores_2_naive)\n",
    "print (\"Naive Sum: \", attention_scores_2_naive.sum())\n",
    "# \n",
    "# Generally we normalize using the torch.softmax() to do the normalization\n",
    "# Softmax ensures its always positive and always adds up to 1\n",
    "#\n",
    "attention_weights_2_torch_softmax = torch.softmax(attention_scores_2, dim=0)\n",
    "# attention_weights_2_torch_softmax.to(device)\n",
    "print(\"Attention weights torch softmax:\", attention_weights_2_torch_softmax)\n",
    "# print(\"Attention weights torch softmax Sum: \", attention_weights_2_torch_softmax.sum())\n",
    "\n",
    "# Now that we have calculated the normalized attention weights, we are ready for the final step.\n",
    "# Calculate the context vector z(2) by multiplying the embedded input tokens x(i), \n",
    "# with the corresponding normalized attention weights and then summing the resultant vectors\n",
    "#\n",
    "query = inputs[1]\n",
    "# query.to(device)\n",
    "#\n",
    "context_vec_2 = torch.zeros(query.shape)\n",
    "# context_vec_2.to(device)\n",
    "#\n",
    "for i, x_i in enumerate(inputs):\n",
    "    context_vec_2 += (attention_weights_2_torch_softmax[i] * x_i)\n",
    "print(\"Context vector z2: \", context_vec_2)\n",
    "\n",
    "#\n",
    "# Now in similar fashion lets calculate attention scores for all the input sequences \n",
    "attention_scores = torch.empty(inputs.shape[0],inputs.shape[0])\n",
    "# attention_scores.to(device)\n",
    "print(\"\\nAttention Scores matrix shape: \", attention_scores.shape)\n",
    "#\n",
    "# Using for loops\n",
    "#\n",
    "# for i, x_i in enumerate(inputs):\n",
    "#     for j, x_j in enumerate(inputs):\n",
    "#         attention_scores[i, j] = torch.dot(x_i, x_j)\n",
    "# #\n",
    "#print(attention_scores)\n",
    "#\n",
    "# Using matrix multiplication we can do it faster\n",
    "#\n",
    "attention_scores_m = inputs @ inputs.T\n",
    "# attention_scores_m.to(device)\n",
    "#print(\"Normalized attention scores \\n\", attention_scores_m)\n",
    "\n",
    "# Just as before lets normalize the rows, so they sum up to 1\n",
    "# NOTE: Here dim = -1 means we are applying the softmax along the last dimension of the attention_scores_m tensor\n",
    "#\n",
    "attention_weights = torch.softmax(attention_scores_m, dim=-1)\n",
    "# attention_weights.to(device)\n",
    "#print(\"Normalized ATTENTION weights \\n\", attention_weights)\n",
    "# print(\"Softmax Sums:\\n\", attention_weights.sum(dim=-1))\n",
    "\n",
    "# FINAL STEP\n",
    "# Now let's calculate the context vectors for all the input by multiplying the input with attention weights\n",
    "all_context_vectors = attention_weights @ inputs # Matrix multiplication\n",
    "# all_context_vectors.to(device)\n",
    "#print(\"Context vector for the entire sequence\\n\", all_context_vectors)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalized attention weights: tensor([0.1455, 0.2278, 0.2249, 0.1285, 0.1077, 0.1656])\n",
      "Sum of attention weights: tensor(1.0000)\n",
      "Attention weights naive: tensor([0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581])\n",
      "Naive Sum:  tensor(1.)\n",
      "Attention weights torch softmax: tensor([0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581])\n",
      "Context vector z2:  tensor([0.4419, 0.6515, 0.5683])\n",
      "\n",
      "Attention Scores matrix shape:  torch.Size([6, 6])\n"
     ]
    }
   ],
   "execution_count": 66
  },
  {
   "cell_type": "code",
   "id": "fa980b537b01e646",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-09T19:23:24.628704Z",
     "start_time": "2025-02-09T19:23:24.623507Z"
    }
   },
   "source": [
    "\n",
    "###\n",
    "### 3.4.1 Using weighted matrix\n",
    "###\n",
    "#\n",
    "# Computing the attention weights step by step\n",
    "#\n",
    "torch.manual_seed(123)\n",
    "x_2 = inputs[1]\n",
    "# x_2.to(device)\n",
    "d_in = inputs.shape[1]\n",
    "d_out = 2\n",
    "print(\"x_2: \", x_2)\n",
    "# Now let's initialize 3 weighted matrices Wq, Wk and Wv\n",
    "# Setting requires_grad = False, to reduce clutter, but for model training this should be set to True\n",
    "\n",
    "W_query = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)\n",
    "W_key   = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)\n",
    "W_value = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)\n",
    "#\n",
    "# Next we compute the query, key and value vectors\n",
    "# Note the output is a 2 dimenstional vector because we set dout to 2\n",
    "#\n",
    "# W_query.to(device)\n",
    "# W_key.to(device)\n",
    "# W_value.to(device)\n",
    "#\n",
    "# Now the dot product with the input\n",
    "#\n",
    "query_2 = x_2 @ W_query\n",
    "key_2   = x_2 @ W_key\n",
    "value_2 = x_2 @ W_value\n",
    "#\n",
    "# query_2.to(device)\n",
    "# key_2.to(device)\n",
    "# value_2.to(device)\n",
    "#\n",
    "print(\"Query 2: \", query_2)\n",
    "print(\"Key 2:   \", key_2)\n",
    "print(\"Value 2: \", value_2)\n",
    "#\n",
    "print(\"\\n\")\n",
    "#\n",
    "\n",
    "keys = inputs @ W_key \n",
    "values = inputs @ W_value\n",
    "# keys.to(device)\n",
    "# values.to(device)\n",
    "print(\"keys.shape:\", keys.shape)\n",
    "print(\"values.shape:\", values.shape)\n",
    "\n",
    "keys_2 = keys[1]\n",
    "\n",
    "attn_score_22 = query_2.dot(keys_2)\n",
    "# attn_score_22.to(device)\n",
    "# attn_score_22 = query_2 @ keys_2\n",
    "print(\"Attention (dot) score 22:\", attn_score_22)\n",
    "\n",
    "# Generalizing across all inputs\n",
    "attn_scores_2 = query_2 @ keys.T\n",
    "# attn_scores_2.to(device)\n",
    "print(\"Attention \\\\@ Scores 2: \", attn_scores_2)\n",
    "# Check the second element is same as previously calculated attention score\n",
    "#\n",
    "# We compute the attention weights by scaling the attention scores and using the softmax function. \n",
    "# However, now we scale the attention scores by dividing them by the square root of the embedding \n",
    "# dimension of the keys\n",
    "\n",
    "attn_weights_2 = torch.softmax(attn_scores_2 / keys.shape[-1]**0.5, dim=-1)\n",
    "# attn_weights_2.to(device)\n",
    "print(\"attn_weights_2: \", attn_weights_2)\n",
    "\n",
    "# The reason for the normalization by square root of embedding dimension size is to improve the training performance by avoiding small gradients. \n",
    "# For instance, when scaling up the embedding dimension, which is typically > 1,000 for GPT-like LLMs, large dot products can result in \n",
    "# very small gradients during backpropagation (due to the softmax function applied to them). \n",
    "# As dot products increase, the softmax function behaves more like a step function, resulting in gradients nearing zero. \n",
    "# These small gradients can drastically slow down learning or cause training to stagnate.\n",
    "#\n",
    "# The scaling by the square root of the embedding dimension is the reason why this self-attention mechanism is also called scaled-dot product attention.\n",
    "# Similar to when we computed the context vector as a weighted sum over the input vectors \n",
    "# we now compute the context vector as a weighted sum over the value vectors. \n",
    "# Here, the attention weights serve as a weighting factor that weighs the respective importance of each value vector\n",
    "context_vec_2 = attn_weights_2 @ values\n",
    "# context_vec_2.to(device)\n",
    "print(\"context_vec_2: \", context_vec_2)\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_2:  tensor([0.5500, 0.8700, 0.6600])\n",
      "Query 2:  tensor([0.4306, 1.4551])\n",
      "Key 2:    tensor([0.4433, 1.1419])\n",
      "Value 2:  tensor([0.3951, 1.0037])\n",
      "\n",
      "\n",
      "keys.shape: torch.Size([6, 2])\n",
      "values.shape: torch.Size([6, 2])\n",
      "Attention (dot) score 22: tensor(1.8524)\n",
      "Attention \\@ Scores 2:  tensor([1.2705, 1.8524, 1.8111, 1.0795, 0.5577, 1.5440])\n",
      "attn_weights_2:  tensor([0.1500, 0.2264, 0.2199, 0.1311, 0.0906, 0.1820])\n",
      "context_vec_2:  tensor([0.3061, 0.8210])\n"
     ]
    }
   ],
   "execution_count": 67
  },
  {
   "cell_type": "code",
   "id": "2ca52ea3a908fe20",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-09T19:23:36.748495Z",
     "start_time": "2025-02-09T19:23:36.739590Z"
    }
   },
   "source": [
    "from src.chapter03.SelfAttention_v2 import SelfAttention_v2\n",
    "\n",
    "torch.manual_seed(789)\n",
    "self_attn_v2 = SelfAttention_v2(d_in, d_out)\n",
    "#print(\"Context vectors from SelfAtten_v2: \\n\", self_attn_v2(inputs))\n",
    "\n",
    "# Note since the input contains 6 embedding vectors, the output also has 6 rows of context vectors\n",
    "\n",
    "\n",
    "# Causal Attention \n",
    "# First we apply softmax to the attention scores then mask with 0 above the diagonal and then normalize the rows to 1\n",
    "#\n",
    "queries = self_attn_v2.W_query(inputs)\n",
    "# queries.to(device)\n",
    "\n",
    "keys = self_attn_v2.W_key(inputs)\n",
    "# keys.to(device)\n",
    "\n",
    "attn_scores = queries @ keys.T\n",
    "# attn_scores.to(device)\n",
    "\n",
    "attn_weights = torch.softmax(attn_scores / keys.shape[-1] ** 0.5, dim=-1)\n",
    "# attn_weights.to(device)\n",
    "#print(\"Attention Wrights: \\n\",attn_weights)\n",
    "\n",
    "# Now mask the values above diagonal as 0 using the tril() function\n",
    "#\n",
    "context_length = attn_scores.shape[0]\n",
    "#\n",
    "mask_simple = torch.tril(torch.ones(context_length, context_length))\n",
    "#print(\"Mask Simple: \\n\", mask_simple)\n",
    "#\n",
    "# Now simply multiply them to prevent the look ahead \n",
    "#\n",
    "masked_attention_weights = attn_weights * mask_simple\n",
    "# masked_attention_weights.to(device)\n",
    "#print(\"Masked attention weights: \\n\", masked_attention_weights)\n",
    "\n",
    "#\n",
    "# Now re-normalize to make sure rows add up to 1. To do this we divide each element by sum of each row\n",
    "#\n",
    "row_sums = masked_attention_weights.sum(dim=-1, keepdim=True)\n",
    "#print(\"row_sums: \\n\", row_sums)\n",
    "masked_simple_norm = masked_attention_weights / row_sums\n",
    "print(\"Masked & re-normalized weights: \\n\", masked_simple_norm)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x = tensor([1.], device='mps:0') using 'mps:0' backend\n",
      "Masked & re-normalized weights: \n",
      " tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.5517, 0.4483, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.3800, 0.3097, 0.3103, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2758, 0.2460, 0.2462, 0.2319, 0.0000, 0.0000],\n",
      "        [0.2175, 0.1983, 0.1984, 0.1888, 0.1971, 0.0000],\n",
      "        [0.1935, 0.1663, 0.1666, 0.1542, 0.1666, 0.1529]],\n",
      "       grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "execution_count": 68
  },
  {
   "cell_type": "code",
   "id": "f3a442b3d9b7413a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-09T19:23:48.957512Z",
     "start_time": "2025-02-09T19:23:48.953101Z"
    }
   },
   "source": [
    "# A more efficient way to obtain masked attention weights is to mask the attention scores with \n",
    "# negative infinity before applying softmax function. (e^negative infinity -> 0)\n",
    "# We can implement this masking by replacing values above the diagonal with 1 and then replacing them \n",
    "# with negative infinity\n",
    "#\n",
    "mask = torch.triu(torch.ones(context_length, context_length), diagonal=1)\n",
    "# mask.to(device)\n",
    "masked = attn_scores.masked_fill(mask.bool(), -torch.inf)\n",
    "# masked.to(device)\n",
    "print(\"Masked attention weights: \\n\", masked)\n",
    "#\n",
    "# Now apply the softmax function \n",
    "#\n",
    "attn_weights = torch.softmax(masked / keys.shape[-1] ** 0.5, dim=-1)\n",
    "print(\"Softmax'd Attention weights: \\n\", attn_weights)\n",
    "\n",
    "# Now we can use these modified attention weights to calculate the context vector\n",
    "#\n",
    "context_vec = attn_weights @ values\n",
    "print(\"context_vec: \\n\", context_vec)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Masked attention weights: \n",
      " tensor([[0.2899,   -inf,   -inf,   -inf,   -inf,   -inf],\n",
      "        [0.4656, 0.1723,   -inf,   -inf,   -inf,   -inf],\n",
      "        [0.4594, 0.1703, 0.1731,   -inf,   -inf,   -inf],\n",
      "        [0.2642, 0.1024, 0.1036, 0.0186,   -inf,   -inf],\n",
      "        [0.2183, 0.0874, 0.0882, 0.0177, 0.0786,   -inf],\n",
      "        [0.3408, 0.1270, 0.1290, 0.0198, 0.1290, 0.0078]],\n",
      "       grad_fn=<MaskedFillBackward0>)\n",
      "Softmax'd Attention weights: \n",
      " tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.5517, 0.4483, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.3800, 0.3097, 0.3103, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2758, 0.2460, 0.2462, 0.2319, 0.0000, 0.0000],\n",
      "        [0.2175, 0.1983, 0.1984, 0.1888, 0.1971, 0.0000],\n",
      "        [0.1935, 0.1663, 0.1666, 0.1542, 0.1666, 0.1529]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "context_vec: \n",
      " tensor([[0.1855, 0.8812],\n",
      "        [0.2795, 0.9361],\n",
      "        [0.3133, 0.9508],\n",
      "        [0.2994, 0.8595],\n",
      "        [0.2702, 0.7554],\n",
      "        [0.2772, 0.7618]], grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "execution_count": 69
  },
  {
   "cell_type": "code",
   "id": "d6b38e5f420f6067",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-09T19:24:03.604480Z",
     "start_time": "2025-02-09T19:24:03.598354Z"
    }
   },
   "source": [
    "# Masking additional weights with dropout\n",
    "# Drop out in the attention mechanism is applied at 2 specific times: \n",
    "# 1. After calculating the attention weights\n",
    "# 2. After applying the attention weights to value vectors\n",
    "# Here we will apply the dropout mask after computing the attention weights\n",
    "#\n",
    "# Lets use a dropout rate of 50% meaning half the attention weights will be masked out. \n",
    "# Normally it's a much lower rate like 0.1 or 0.2\n",
    "torch.manual_seed(123)\n",
    "dropout = torch.nn.Dropout(0.5)\n",
    "example = torch.ones(6, 6)\n",
    "print(dropout(example))\n",
    "#\n",
    "# Since we are applying 50% dropout, to compensate for reduction in active elements\n",
    "# we are going to scale up the values of remaining elements by a factor of 1/0.5 = 2\n",
    "# This scaling is crucial to maintain the balance of the attention weights\n",
    "#\n",
    "torch.manual_seed(123)\n",
    "print(\"Dropped out attention weights: \\n\", dropout(attn_weights))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2., 2., 0., 2., 2., 0.],\n",
      "        [0., 0., 0., 2., 0., 2.],\n",
      "        [2., 2., 2., 2., 0., 2.],\n",
      "        [0., 2., 2., 0., 0., 2.],\n",
      "        [0., 2., 0., 2., 0., 2.],\n",
      "        [0., 2., 2., 2., 2., 0.]])\n",
      "Dropped out attention weights: \n",
      " tensor([[2.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.7599, 0.6194, 0.6206, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.4921, 0.4925, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.3966, 0.0000, 0.3775, 0.0000, 0.0000],\n",
      "        [0.0000, 0.3327, 0.3331, 0.3084, 0.3331, 0.0000]],\n",
      "       grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "execution_count": 70
  },
  {
   "cell_type": "code",
   "id": "50701dd69a456857",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-09T19:24:16.272396Z",
     "start_time": "2025-02-09T19:24:16.260226Z"
    }
   },
   "source": [
    "from src.chapter03.CausalAttention import CausalAttention\n",
    "\n",
    "# Let’s ensure that the code can handle batches consisting of more than one input so that \n",
    "# the CausalAttention class supports the batch outputs produced by the data loader\n",
    "# To simulate batch input lets duplicate the input text\n",
    "#\n",
    "batch = torch.stack((inputs, inputs), dim=0)\n",
    "# batch.to(device)\n",
    "print(\"batch: \\n\", batch.shape)\n",
    "# print(batch)\n",
    "#\n",
    "# We can now use the CausalAttention class as follows\n",
    "#\n",
    "torch.manual_seed(123)\n",
    "context_layer = batch.shape[1]\n",
    "causal_attn = CausalAttention(d_in, d_out, context_length, 0.0, False)\n",
    "context_vecs = causal_attn(batch)\n",
    "print(\"context_vecs: \\n\", context_vecs.shape)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch: \n",
      " torch.Size([2, 6, 3])\n",
      "context_vecs: \n",
      " torch.Size([2, 6, 2])\n"
     ]
    }
   ],
   "execution_count": 71
  },
  {
   "cell_type": "code",
   "id": "9d8058957dc253b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-09T19:24:28.434558Z",
     "start_time": "2025-02-09T19:24:28.423436Z"
    }
   },
   "source": [
    "from src.chapter03.MultiHeadAttentionWrapper import MultiHeadAttentionWrapper\n",
    "\n",
    "# Multi Head Attention\n",
    "# Now if we use the MultiHeadAttentionWrapper class with two attention heads, and CausalAttention \n",
    "# output dimension d_out = 2, we get a 4 dimensional context vector (d_out * num_heads = 4).\n",
    "#\n",
    "torch.manual_seed(123)\n",
    "context_length = batch.shape[1]\n",
    "d_in, d_out = 3, 2\n",
    "multi_head_attn = MultiHeadAttentionWrapper(d_in, d_out, context_length, dropout=0.0, num_heads=2, qkv_bias=False)\n",
    "context_vecs = multi_head_attn(batch)\n",
    "print(\"context_vecs: \\n\", context_vecs)\n",
    "print(\"context_vecs.shape: \\n\", context_vecs.shape)\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "context_vecs: \n",
      " tensor([[[-0.4519,  0.2216,  0.4772,  0.1063],\n",
      "         [-0.5874,  0.0058,  0.5891,  0.3257],\n",
      "         [-0.6300, -0.0632,  0.6202,  0.3860],\n",
      "         [-0.5675, -0.0843,  0.5478,  0.3589],\n",
      "         [-0.5526, -0.0981,  0.5321,  0.3428],\n",
      "         [-0.5299, -0.1081,  0.5077,  0.3493]],\n",
      "\n",
      "        [[-0.4519,  0.2216,  0.4772,  0.1063],\n",
      "         [-0.5874,  0.0058,  0.5891,  0.3257],\n",
      "         [-0.6300, -0.0632,  0.6202,  0.3860],\n",
      "         [-0.5675, -0.0843,  0.5478,  0.3589],\n",
      "         [-0.5526, -0.0981,  0.5321,  0.3428],\n",
      "         [-0.5299, -0.1081,  0.5077,  0.3493]]], grad_fn=<CatBackward0>)\n",
      "context_vecs.shape: \n",
      " torch.Size([2, 6, 4])\n"
     ]
    }
   ],
   "execution_count": 72
  },
  {
   "cell_type": "code",
   "id": "f20088921d8113c0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-09T19:24:41.271738Z",
     "start_time": "2025-02-09T19:24:41.265890Z"
    }
   },
   "source": [
    "a = torch.tensor([[[[0.2745, 0.6584, 0.2775, 0.8573],\n",
    "                    [0.8993, 0.0390, 0.9268, 0.7388],\n",
    "                    [0.7179, 0.7058, 0.9156, 0.4340]],\n",
    "\n",
    "                   [[0.0772, 0.3565, 0.1479, 0.5331],\n",
    "                    [0.4066, 0.2318, 0.4545, 0.9737],\n",
    "                    [0.4606, 0.5159, 0.4220, 0.5786]]]])\n",
    "# print(a.transpose(2, 3))\n",
    "a.to(device)\n",
    "\n",
    "first_head = a[0, 0, :, :]\n",
    "first_res = first_head @ first_head.T\n",
    "print(\"First head:\\n\", first_res)\n",
    "\n",
    "second_head = a[0, 1, :, :]\n",
    "second_res = second_head @ second_head.T\n",
    "print(\"\\nSecond head:\\n\", second_res)\n",
    "\n",
    "print(f\"Batched: \\n{a @ a.transpose(2, 3)}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First head:\n",
      " tensor([[1.3208, 1.1631, 1.2879],\n",
      "        [1.1631, 2.2150, 1.8424],\n",
      "        [1.2879, 1.8424, 2.0402]])\n",
      "\n",
      "Second head:\n",
      " tensor([[0.4391, 0.7003, 0.5903],\n",
      "        [0.7003, 1.3737, 1.0620],\n",
      "        [0.5903, 1.0620, 0.9912]])\n",
      "Batched: \n",
      "tensor([[[[1.3208, 1.1631, 1.2879],\n",
      "          [1.1631, 2.2150, 1.8424],\n",
      "          [1.2879, 1.8424, 2.0402]],\n",
      "\n",
      "         [[0.4391, 0.7003, 0.5903],\n",
      "          [0.7003, 1.3737, 1.0620],\n",
      "          [0.5903, 1.0620, 0.9912]]]])\n"
     ]
    }
   ],
   "execution_count": 73
  },
  {
   "cell_type": "code",
   "id": "df6a05dfc13ebbe4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-09T19:24:54.840247Z",
     "start_time": "2025-02-09T19:24:54.829506Z"
    }
   },
   "source": [
    "from src.chapter03.MultiHeadAttention import MultiHeadAttention\n",
    "\n",
    "torch.manual_seed(123)\n",
    "batch_size, context_length, d_in = batch.shape\n",
    "print(\"Batch Shape: \\n\", batch.shape)\n",
    "d_out = 2\n",
    "mha = MultiHeadAttention(d_in, d_out, context_length, 0.0, num_heads=2)\n",
    "context_vecs = mha(batch)\n",
    "print(context_vecs)\n",
    "print(\"Context vector shape: \\n\", context_vecs.shape)\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Shape: \n",
      " torch.Size([2, 6, 3])\n",
      "tensor([[[0.3190, 0.4858],\n",
      "         [0.2943, 0.3897],\n",
      "         [0.2856, 0.3593],\n",
      "         [0.2693, 0.3873],\n",
      "         [0.2639, 0.3928],\n",
      "         [0.2575, 0.4028]],\n",
      "\n",
      "        [[0.3190, 0.4858],\n",
      "         [0.2943, 0.3897],\n",
      "         [0.2856, 0.3593],\n",
      "         [0.2693, 0.3873],\n",
      "         [0.2639, 0.3928],\n",
      "         [0.2575, 0.4028]]], grad_fn=<ViewBackward0>)\n",
      "Context vector shape: \n",
      " torch.Size([2, 6, 2])\n"
     ]
    }
   ],
   "execution_count": 74
  },
  {
   "cell_type": "markdown",
   "id": "3f958161a85ca549",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "## Chapter 4: Implementing GPT from Scratch to generate text\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "id": "16f34621162871e9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-09T19:25:07.518732Z",
     "start_time": "2025-02-09T19:25:07.516755Z"
    }
   },
   "source": [
    "GPT_CONFIG_124M = {\n",
    "    \"vocab_size\": 50257,     # Vocabulary size\n",
    "    \"context_length\": 1024,  # Context length\n",
    "    \"emb_dim\": 768,          # Embedding dimension\n",
    "    \"n_heads\": 12,           # Number of attention heads\n",
    "    \"n_layers\": 12,          # Number of layers\n",
    "    \"drop_rate\": 0.1,        # Dropout rate\n",
    "    \"qkv_bias\": False,\n",
    "    \"model_name\": \"GPTModel\",\n",
    "    \"lr\": 5e-4,\n",
    "    \"weight_decay\": 0.1\n",
    "}"
   ],
   "outputs": [],
   "execution_count": 75
  },
  {
   "cell_type": "markdown",
   "id": "5ac1cbf145ad6daf",
   "metadata": {},
   "source": [
    "\n",
    "## Here is the proposed architecture and order of implementation\n",
    "![image](../data/4-3.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "75010152b67235a2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-09T19:25:20.693672Z",
     "start_time": "2025-02-09T19:25:20.062694Z"
    }
   },
   "source": [
    "from src.chapter04.DummyGPTModel import DummyGPTModel\n",
    "import tiktoken\n",
    "#\n",
    "torch.manual_seed(123)\n",
    "#\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "batch = []\n",
    "txt1 = \"Every effort moves you\"\n",
    "txt2 = \"Every day holds a\"\n",
    "batch.clear()\n",
    "batch.append(torch.tensor(tokenizer.encode(txt1)).to(device))\n",
    "batch.append(torch.tensor(tokenizer.encode(txt2)).to(device))\n",
    "batch = torch.stack(batch, dim=0).to(device)\n",
    "batch.to(device)\n",
    "print(\"Input Batch: \\n\", batch)\n",
    "print(\"Input batch shape: \\n\", batch.shape)\n",
    "#\n",
    "# Next, we initialize a new 124-million-parameter DummyGPTModel instance \n",
    "# and feed it the tokenized batch\n",
    "#\n",
    "model = DummyGPTModel(GPT_CONFIG_124M)\n",
    "model.to(device)\n",
    "logits = model(batch)\n",
    "print(\"Output shape: \\n\", logits.shape)\n",
    "#print(logits)\n",
    "#                      \n",
    "#"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Batch: \n",
      " tensor([[6109, 3626, 6100,  345],\n",
      "        [6109, 1110, 6622,  257]], device='mps:0')\n",
      "Input batch shape: \n",
      " torch.Size([2, 4])\n",
      "Output shape: \n",
      " torch.Size([2, 4, 50257])\n"
     ]
    }
   ],
   "execution_count": 76
  },
  {
   "cell_type": "code",
   "id": "c06060ecd0f8f8be",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-09T19:25:46.857314Z",
     "start_time": "2025-02-09T19:25:46.836468Z"
    }
   },
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "# Let’s now implement LAYER Normalization to improve the stability and efficiency of the training.\n",
    "# The main idea behind LAYER Normalization is to adjust the activations (outputs) of a deep\n",
    "# neural network layer to have a mean of 0 and a variance of 1\n",
    "#\n",
    "# This adjustment speeds up the convergence.\n",
    "#\n",
    "torch.manual_seed(123)\n",
    "torch.set_printoptions(sci_mode=False)\n",
    "#\n",
    "batch_example = torch.randn(2, 5)\n",
    "layer = nn.Sequential(nn.Linear(5,6), nn.ReLU())\n",
    "out = layer(batch_example)\n",
    "print(\"Layer: \\n\",out)\n",
    "#\n",
    "# The NN Layer contains the non-linear activation ReLU which 0's out the negative values\n",
    "# \n",
    "mean = out.mean(dim=-1, keepdim=True)\n",
    "var = out.var(dim=-1, keepdim=True)\n",
    "print(\"Mean: \", mean)\n",
    "print(\"Variance: \", var)\n",
    "print(\"\\n\")\n",
    "#\n",
    "# Next, let’s apply layer normalization to the layer outputs we obtained earlier. \n",
    "# The operation consists of subtracting the mean and dividing by the square root \n",
    "# of the variance (also known as the standard deviation):\n",
    "#\n",
    "eps = 1e-5\n",
    "out_norm = (out - mean) / torch.sqrt(var)\n",
    "# Dim = -1 indicates statistics along the last dimention\n",
    "mean = out_norm.mean(dim=-1, keepdim=True)\n",
    "var = out_norm.var(dim=-1, keepdim=True)\n",
    "torch.set_printoptions(sci_mode=True)\n",
    "print(\"Normalized Layer Outputs: \\n\", out_norm)\n",
    "print(\"Mean: \\n\", mean)\n",
    "print(\"Variance: \\n\", var)\n",
    "print(\"-------------------------\\n\")\n",
    "\n",
    "torch.set_printoptions(sci_mode=False)\n",
    "print(\"Mean:\\n\", mean)\n",
    "print(\"Variance:\\n\", var)\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: \n",
      " tensor([[0.2260, 0.3470, 0.0000, 0.2216, 0.0000, 0.0000],\n",
      "        [0.2133, 0.2394, 0.0000, 0.5198, 0.3297, 0.0000]],\n",
      "       grad_fn=<ReluBackward0>)\n",
      "Mean:  tensor([[0.1324],\n",
      "        [0.2170]], grad_fn=<MeanBackward1>)\n",
      "Variance:  tensor([[0.0231],\n",
      "        [0.0398]], grad_fn=<VarBackward0>)\n",
      "\n",
      "\n",
      "Normalized Layer Outputs: \n",
      " tensor([[6.1585e-01, 1.4126e+00, -8.7188e-01, 5.8723e-01, -8.7188e-01, -8.7188e-01],\n",
      "        [-1.8865e-02, 1.1211e-01, -1.0876e+00, 1.5173e+00, 5.6474e-01, -1.0876e+00]],\n",
      "       grad_fn=<DivBackward0>)\n",
      "Mean: \n",
      " tensor([[-5.9605e-08],\n",
      "        [ 1.9868e-08]], grad_fn=<MeanBackward1>)\n",
      "Variance: \n",
      " tensor([[1.0000e+00],\n",
      "        [1.0000e+00]], grad_fn=<VarBackward0>)\n",
      "-------------------------\n",
      "\n",
      "Mean:\n",
      " tensor([[    -0.0000],\n",
      "        [     0.0000]], grad_fn=<MeanBackward1>)\n",
      "Variance:\n",
      " tensor([[1.0000],\n",
      "        [1.0000]], grad_fn=<VarBackward0>)\n"
     ]
    }
   ],
   "execution_count": 77
  },
  {
   "cell_type": "code",
   "id": "a39e430a037896f0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-09T19:25:46.890687Z",
     "start_time": "2025-02-09T19:25:46.886705Z"
    }
   },
   "source": [
    "from src.chapter04.LayerNorm import LayerNorm\n",
    "\n",
    "# Previously we used unbiased = False in our variance calculation. This doesn't \n",
    "# apply Bessel's correction where divisor is n-1 instead of n. But this is \n",
    "# compatible with GPT-2\n",
    "\n",
    "ln = LayerNorm(emb_dim = 5)\n",
    "out_ln = ln(batch_example)\n",
    "mean = out_ln.mean(dim=-1, keepdim=True)\n",
    "var = out_ln.var(dim=-1, unbiased=False, keepdim=True)\n",
    "print(\"Mean: \\n\", mean)\n",
    "print(\"Variance: \\n\", var)\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean: \n",
      " tensor([[    -0.0000],\n",
      "        [     0.0000]], grad_fn=<MeanBackward1>)\n",
      "Variance: \n",
      " tensor([[1.0000],\n",
      "        [1.0000]], grad_fn=<VarBackward0>)\n"
     ]
    }
   ],
   "execution_count": 78
  },
  {
   "cell_type": "code",
   "id": "3130c2c83093ee01",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-09T19:26:00.250510Z",
     "start_time": "2025-02-09T19:26:00.160699Z"
    }
   },
   "source": [
    "# Let us see how the GELU (Gaussian Error Linear Unit) stacks up against \n",
    "# # RELU (REctified Linear Unit)\n",
    "from src.chapter04.GELU import GELU\n",
    "import matplotlib.pyplot as plt\n",
    "gelu, relu = GELU(), nn.ReLU()\n",
    "\n",
    "x = torch.linspace(-3, 3, 100)\n",
    "y_gelu, y_relu = gelu(x), relu(x)\n",
    "plt.figure(figsize=(8, 3))\n",
    "for i, (y, label) in enumerate(zip([y_gelu, y_relu], [\"GELU\", \"ReLU\"]), 1):\n",
    "    plt.subplot(1, 2, i)\n",
    "    plt.plot(x, y)\n",
    "    plt.title(f\"{label} activation function\")\n",
    "    plt.xlabel(\"x\")\n",
    "    plt.ylabel(f\"{label}(x)\")\n",
    "    plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 800x300 with 2 Axes>"
      ],
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 79
  },
  {
   "cell_type": "code",
   "id": "e496e641f9044a67",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-09T19:26:14.662483Z",
     "start_time": "2025-02-09T19:26:14.640001Z"
    }
   },
   "source": [
    "from src.chapter04.SimpleFeedForward import SimpleFeedForward\n",
    "\n",
    "# As we can see the smoothness of the GELU can lead to better optimization properties during training\n",
    "# as it allows more nuanced finer adjustments to models parameters. In contrast, RELU has a sharp corner\n",
    "# that can make adjustments difficult for very deep networks.\n",
    "#\n",
    "# Next we look at implementing a feed forward network with GELU activations\n",
    "# See SimpleFeedForward.py\n",
    "#\n",
    "sff = SimpleFeedForward(GPT_CONFIG_124M)\n",
    "x = torch.rand(2, 3, 768)\n",
    "out = sff(x)\n",
    "print(out.shape)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 768])\n"
     ]
    }
   ],
   "execution_count": 80
  },
  {
   "cell_type": "code",
   "id": "98a59bc5db854fa4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-09T19:26:29.354314Z",
     "start_time": "2025-02-09T19:26:29.342373Z"
    }
   },
   "source": [
    "from src.chapter04.ExampleDeepNeuralNetwork import ExampleDeepNeuralNetwork\n",
    "\n",
    "# Next we implement Shortcut Connections\n",
    "# Each layer will be initialized such that it accepts an example with three input \n",
    "# values and returns three output values.\n",
    "torch.manual_seed(123)\n",
    "layer_sizes = [3, 3, 3, 3, 3, 1]\n",
    "sample_input = torch.tensor([[1., 0., -1.]])\n",
    "model_without_shortcut = ExampleDeepNeuralNetwork(layer_sizes, use_shortcut=False)\n",
    "# model.to(device)\n",
    "\n",
    "# Next lets print the gradients\n",
    "def print_gradients(nnmodel, input_x):\n",
    "    output = nnmodel(input_x)\n",
    "    target = torch.tensor([[0.]])\n",
    "    loss = nn.MSELoss()\n",
    "    loss = loss(output, target)\n",
    "    torch.autograd.set_detect_anomaly(True)\n",
    "    loss.backward()\n",
    "    for name, param in nnmodel.named_parameters():\n",
    "        # print(name, \" = \", param)\n",
    "        if 'weight' in name:\n",
    "            print(f\"{name} has gradient mean of {param.grad.abs().mean().item()}\")\n",
    "\n",
    "#\n",
    "# Now Lets use this function to print the gradients calculated by loss.backward()\n",
    "print_gradients(model_without_shortcut, sample_input)\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layers.0.0.weight has gradient mean of 0.00020173587836325169\n",
      "layers.1.0.weight has gradient mean of 0.00012011159560643137\n",
      "layers.2.0.weight has gradient mean of 0.0007152039906941354\n",
      "layers.3.0.weight has gradient mean of 0.0013988736318424344\n",
      "layers.4.0.weight has gradient mean of 0.005049645435065031\n"
     ]
    }
   ],
   "execution_count": 81
  },
  {
   "cell_type": "code",
   "id": "4bfdfab7cb5bcc5f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-09T19:26:44.245461Z",
     "start_time": "2025-02-09T19:26:44.233754Z"
    }
   },
   "source": [
    "# As you can see above gradients become tiny aka Vanishing from Layer4 to Layer1\n",
    "# Let’s now instantiate a model with skip connections and see how it compares:\n",
    "torch.manual_seed(123)\n",
    "layer_sizes = [3, 3, 3, 3, 3, 1]\n",
    "sample_input = torch.tensor([[1., 0., -1.]])\n",
    "model_with_shortcut = ExampleDeepNeuralNetwork(layer_sizes, use_shortcut=True)\n",
    "# model.to(device)\n",
    "print_gradients(model_with_shortcut, sample_input)\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layers.0.0.weight has gradient mean of 0.22169792652130127\n",
      "layers.1.0.weight has gradient mean of 0.20694106817245483\n",
      "layers.2.0.weight has gradient mean of 0.32896995544433594\n",
      "layers.3.0.weight has gradient mean of 0.2665732204914093\n",
      "layers.4.0.weight has gradient mean of 1.3258540630340576\n"
     ]
    }
   ],
   "execution_count": 82
  },
  {
   "cell_type": "code",
   "id": "a229081ed60e29b5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-09T19:26:57.431081Z",
     "start_time": "2025-02-09T19:26:57.429321Z"
    }
   },
   "source": [
    "# Note here the gradient doesn't approach a vanishingly small value during backprop.\n",
    "# In conclusion, shortcut connections are important for overcoming the limitations posed \n",
    "# by the vanishing gradient problem in deep neural networks."
   ],
   "outputs": [],
   "execution_count": 83
  },
  {
   "cell_type": "markdown",
   "id": "2df6d339f0570629",
   "metadata": {},
   "source": [
    "#### Next, we’ll connect all the previously covered concepts (layer normalization, GELU activations, feed forward module, and shortcut connections) in a transformer  block, which is the final building block we need to code the GPT architecture.\n",
    "\n",
    "![image](../data/transformer_wiring.png)"
   ]
  },
  {
   "cell_type": "code",
   "id": "cd526dd3047ba477",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-09T19:27:21.628852Z",
     "start_time": "2025-02-09T19:27:21.587857Z"
    }
   },
   "source": [
    "# See TransformerBlock.py for the basic sequence and feedforward details\n",
    "from src.chapter04.TransformerBlock import TransformerBlock\n",
    "\n",
    "torch.manual_seed(123)\n",
    "x = torch.rand(2, 4, 768)\n",
    "tr_block = TransformerBlock(GPT_CONFIG_124M)\n",
    "out = tr_block(x)\n",
    "#\n",
    "print(\"Input Shape:  \", x.shape)\n",
    "print(\"Output Shape: \", out.shape)\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Shape:   torch.Size([2, 4, 768])\n",
      "Output Shape:  torch.Size([2, 4, 768])\n"
     ]
    }
   ],
   "execution_count": 84
  },
  {
   "cell_type": "code",
   "id": "8c3e76bf70259f86",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-09T19:27:22.663169Z",
     "start_time": "2025-02-09T19:27:21.637106Z"
    }
   },
   "source": [
    "from src.chapter04.GPTModel import GPTModel\n",
    "# Let us wire up the actual GPT Model we wrote now\n",
    "\n",
    "torch.manual_seed(123)\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "model.to(device)\n",
    "print(\"Input batch: \", batch)\n",
    "out = model(batch)\n",
    "print(\"Output shape: \", out.shape)\n",
    "# print(\"Out: \\n\", out)\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input batch:  tensor([[6109, 3626, 6100,  345],\n",
      "        [6109, 1110, 6622,  257]], device='mps:0')\n",
      "Output shape:  torch.Size([2, 4, 50257])\n"
     ]
    }
   ],
   "execution_count": 85
  },
  {
   "cell_type": "code",
   "id": "af55c75851ac7e89",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-09T19:27:34.796054Z",
     "start_time": "2025-02-09T19:27:34.793714Z"
    }
   },
   "source": [
    "# Note above the output tensor has the shape [2, 4, 50257], since we passed in two input texts (the two sentences) \n",
    "# with four tokens each. The last dimension, 50257, corresponds to the vocabulary size of the tokenizer.\n",
    "#\n",
    "# To capture the total number of Parameters for a model use numel parameter value\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Total number of parameters: {total_params:,}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of parameters: 163,009,536\n"
     ]
    }
   ],
   "execution_count": 86
  },
  {
   "cell_type": "markdown",
   "id": "ba624b8ce1a7e0a0",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "id": "1a31524b0cdc6ebf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-09T19:27:59.099206Z",
     "start_time": "2025-02-09T19:27:59.097318Z"
    }
   },
   "source": [
    "# Weight Tying: The model reuses weights from the token embedding layer in its output layer\n",
    "print(\"Token embedding layer shape:\", model.tok_emb.weight.shape)\n",
    "print(\"Output layer shape:\", model.out_head.weight.shape)\n",
    "# As we can see both shapes are same"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token embedding layer shape: torch.Size([50257, 768])\n",
      "Output layer shape: torch.Size([50257, 768])\n"
     ]
    }
   ],
   "execution_count": 87
  },
  {
   "cell_type": "code",
   "id": "197e0a5f1199aada",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-09T19:27:59.123640Z",
     "start_time": "2025-02-09T19:27:59.121233Z"
    }
   },
   "source": [
    "# The token embedding and output layers are very large due to the 50,257 rows in the tokenizer’s vocabulary. \n",
    "# If we remove the output layer parameter count from the total GPT-2 model count :\n",
    "total_params_gptmodel = (\n",
    "    total_params - sum(p.numel() for p in model.out_head.parameters())\n",
    ")\n",
    "print(f\"Total number of trainable parameters considering weight tying: \"\n",
    "      f\"{total_params_gptmodel:,}\")\n",
    "\n",
    "# Memory Requirement\n",
    "total_size_bytes = total_params * 4\n",
    "total_size_mb = total_size_bytes / (1024 * 1024)\n",
    "print(f\"Total size of the model: {total_size_mb:.2f} MB\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of trainable parameters considering weight tying: 124,412,160\n",
      "Total size of the model: 621.83 MB\n"
     ]
    }
   ],
   "execution_count": 88
  },
  {
   "cell_type": "markdown",
   "id": "82b6b3cc8d05feb4",
   "metadata": {},
   "source": [
    "# Generating text \n",
    "#### We will now write code to generate text from the predicted tensors by the GPTModel\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "7da3f91bddda842d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-09T19:28:11.423657Z",
     "start_time": "2025-02-09T19:28:11.376506Z"
    }
   },
   "source": [
    "def generate_text_simple(input_model, tokenids, max_new_tokens, context_size):\n",
    "    for _ in range(max_new_tokens):\n",
    "        token_cond = tokenids[:, -context_size:]\n",
    "        with torch.no_grad():\n",
    "            logits = input_model(token_cond)\n",
    "\n",
    "        logits = logits[:, -1, :]\n",
    "        probabs = torch.softmax(logits, dim=-1)\n",
    "        next_token = torch.argmax(probabs, dim=-1, keepdim=True)\n",
    "        tokenids = torch.cat((tokenids, next_token), dim=1)\n",
    "\n",
    "        return tokenids\n",
    "#    \n",
    "#  Try it with a sample sentence  \n",
    "#    \n",
    "start_context = \"Hello, I am \"\n",
    "encoded = tokenizer.encode(start_context)\n",
    "print(\"encoded: \", encoded)\n",
    "#\n",
    "encoded_tensor = torch.tensor(encoded).to(device).unsqueeze(0)\n",
    "print(\"encoded_tensor.shape: \", encoded_tensor.shape)\n",
    "#\n",
    "model.eval()\n",
    "out = generate_text_simple(\n",
    "    input_model=model,\n",
    "    tokenids=encoded_tensor, \n",
    "    max_new_tokens=6, \n",
    "    context_size=GPT_CONFIG_124M[\"context_length\"]\n",
    ")\n",
    "print(\"Output:\", out)\n",
    "print(\"Output length:\", len(out[0]))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoded:  [15496, 11, 314, 716, 220]\n",
      "encoded_tensor.shape:  torch.Size([1, 5])\n",
      "Output: tensor([[15496,    11,   314,   716,   220, 24464]], device='mps:0')\n",
      "Output length: 6\n"
     ]
    }
   ],
   "execution_count": 89
  },
  {
   "cell_type": "code",
   "id": "ab7699859ab25da6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-09T19:29:54.443825Z",
     "start_time": "2025-02-09T19:29:54.440648Z"
    }
   },
   "source": [
    "decoded_text = tokenizer.decode(out.squeeze(0).tolist())\n",
    "print(decoded_text)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, I am opia\n"
     ]
    }
   ],
   "execution_count": 90
  },
  {
   "cell_type": "markdown",
   "id": "7397b4b87caf10c6",
   "metadata": {},
   "source": [
    "# Chapter 5: Pretraining on unlabeled data\n",
    "#### Using GPT to generate text"
   ]
  },
  {
   "cell_type": "code",
   "id": "21d8324b6eace0d0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-09T19:30:08.819790Z",
     "start_time": "2025-02-09T19:30:08.755253Z"
    }
   },
   "source": [
    "# How to calculate loss and relatively randomize next word prediction\n",
    "#\n",
    "inputs = torch.tensor([[16833, 3626, 6100],   # [\"every effort moves\",\n",
    "                       [40,    1107, 588]]).to(device)   #  \"I really like\"\n",
    "#\n",
    "targets = torch.tensor([[3626, 6100, 345  ],  # [\" effort moves you\",\n",
    "                        [1107, 588, 11311]]).to(device)  #  \" really like chocolate\"\n",
    "\n",
    "with torch.no_grad():\n",
    "    logits = model(inputs)\n",
    "probas = torch.softmax(logits, dim=-1)\n",
    "print(f\"probas: {probas.shape}\")\n",
    "\n",
    "token_ids = torch.argmax(probas, dim=-1, keepdim=True)\n",
    "print(f\"Token IDs: {token_ids}\")\n",
    "\n",
    "# Utility function\n",
    "def text_to_token_ids(txt, tokenizr):\n",
    "    encoded_txt = tokenizr.encode(txt, allowed_special={'<|endoftext|>'})\n",
    "    encoded_tnsr = torch.tensor(encoded_txt).to(device).unsqueeze(0)\n",
    "    return encoded_tnsr\n",
    "\n",
    "# Utility function\n",
    "def token_ids_to_text(tokenids, tokenizr):\n",
    "    flat = tokenids.squeeze(0)\n",
    "    return tokenizr.decode(flat.tolist())\n",
    "\n",
    "\n",
    "print(f\"Targets batch 1: {token_ids_to_text(targets[0], tokenizer)}\")\n",
    "print(f\"Outputs batch 1: {token_ids_to_text(token_ids[0].flatten(), tokenizer)}\")\n",
    "\n",
    "torch.set_printoptions(sci_mode=True)\n",
    "text_idx = 0\n",
    "target_probas_1 = probas[text_idx, [0, 1, 2], targets[text_idx]]\n",
    "print(\"Softmax scores for Text 1:\", target_probas_1)\n",
    "\n",
    "text_idx = 1\n",
    "target_probas_2 = probas[text_idx, [0, 1, 2], targets[text_idx]]\n",
    "print(\"Softmax scores for Text 2: \", target_probas_2)\n",
    "\n",
    "# The goal of training an LLM is to maximize the likelihood of the correct token, \n",
    "# which involves increasing its probability relative to other tokens. \n",
    "\n",
    "# Loss of probabilities for the two batches are\n",
    "log_probas = torch.log(torch.cat((target_probas_1, target_probas_2)))\n",
    "print(\"Log probabilities for both batches are: \\n\", log_probas)\n",
    "\n",
    "# Next, we combine the log probabilities into a single score by computing \n",
    "# the average\n",
    "avg_log_probas = torch.mean(log_probas)\n",
    "print(f\"Average log probability: {avg_log_probas}\")\n",
    "\n",
    "# However, in deep learning, the common practice isn’t to push the average log probability \n",
    "# up to 0 but rather to bring the negative average log probability down to 0. The negative \n",
    "# average log probability is simply the average log probability multiplied by –1\n",
    "neg_avg_log_probabs = avg_log_probas * -1\n",
    "print(f\"Negative of average log probability: {neg_avg_log_probabs}\")\n",
    "\n",
    "# In deep learning, the term for turning this negative value, –10.7940, into 10.7940, \n",
    "# is known as the cross entropy loss.\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "probas: torch.Size([2, 3, 50257])\n",
      "Token IDs: tensor([[[36397],\n",
      "         [39619],\n",
      "         [20610]],\n",
      "\n",
      "        [[ 8615],\n",
      "         [49289],\n",
      "         [47105]]], device='mps:0')\n",
      "Targets batch 1:  effort moves you\n",
      "Outputs batch 1:  Gathering SerbianFriday\n",
      "Softmax scores for Text 1: tensor([2.3466e-05, 2.0531e-05, 1.1733e-05], device='mps:0')\n",
      "Softmax scores for Text 2:  tensor([4.2794e-05, 1.6248e-05, 1.1586e-05], device='mps:0')\n",
      "Log probabilities for both batches are: \n",
      " tensor([-1.0660e+01, -1.0794e+01, -1.1353e+01, -1.0059e+01, -1.1028e+01, -1.1366e+01],\n",
      "       device='mps:0')\n",
      "Average log probability: -10.876513481140137\n",
      "Negative of average log probability: 10.876513481140137\n"
     ]
    }
   ],
   "execution_count": 91
  },
  {
   "cell_type": "markdown",
   "id": "2e39970739c5f593",
   "metadata": {},
   "source": [
    "#### NOTE: Our goal during training is to get the average log probability as close to 0 as possible by updating the model’s weights"
   ]
  },
  {
   "cell_type": "code",
   "id": "62d971c360d51b5b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-09T19:30:22.869680Z",
     "start_time": "2025-02-09T19:30:21.955126Z"
    }
   },
   "source": [
    "GPT_CONFIG_124M_2 = {\n",
    "    \"vocab_size\": 50257,\n",
    "    \"context_length\": 256,\n",
    "    \"emb_dim\": 768,\n",
    "    \"n_heads\": 12,\n",
    "    \"n_layers\": 12, \n",
    "    \"drop_rate\": 0.1,\n",
    "    \"qkv_bias\": False,\n",
    "    \"model_name\": \"GPTModel\",\n",
    "    \"lr\": 5e-4,\n",
    "    \"weight_decay\": 0.1\n",
    "}\n",
    "torch.manual_seed(123)\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "model.to(torch.device(\"mps\"))\n",
    "model.eval()\n",
    "\n",
    "\n",
    "\n",
    "start_context = \"Every effort moves you\"\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "token_ids = generate_text_simple(\n",
    "    input_model=model,\n",
    "    tokenids=text_to_token_ids(start_context, tokenizer),\n",
    "    max_new_tokens=10,\n",
    "    context_size=GPT_CONFIG_124M[\"context_length\"]\n",
    ")\n",
    "print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output text:\n",
      " Every effort moves you Ae\n"
     ]
    }
   ],
   "execution_count": 92
  },
  {
   "cell_type": "code",
   "id": "d0465f554d8c18b6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-09T19:30:35.059654Z",
     "start_time": "2025-02-09T19:30:35.055727Z"
    }
   },
   "source": [
    "print(\"Logits shape:\", logits.shape) # batch size, num of tokens, vocab size\n",
    "print(\"Targets shape:\", targets.shape) # batch size, num of tokens\n",
    "\n",
    "# For the cross_entropy loss function in PyTorch, we want to flatten these \n",
    "# tensors by combining them over the batch dimension:\n",
    "logits_flat = logits.flatten(0, 1)\n",
    "targets_flat = targets.flatten()\n",
    "print(\"Flattened logits:\", logits_flat.shape)\n",
    "print(\"Flattened targets:\", targets_flat.shape)\n",
    "\n",
    "# Now we can call CE from torch to calculate the loss\n",
    "celoss = torch.nn.functional.cross_entropy(logits_flat, targets_flat)\n",
    "print(f\"Cross Entropy loss: {celoss}\")\n",
    "\n",
    "# Perplexity measures how well the probability distribution predicted by the model \n",
    "# matches the actual distribution of the words in the dataset. Similar to the loss, \n",
    "# a lower perplexity means the model predictions are closer to the actual distribution.\n",
    "perplexity = torch.exp(celoss)\n",
    "print(f\"Perplexity: {perplexity}\")\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logits shape: torch.Size([2, 3, 50257])\n",
      "Targets shape: torch.Size([2, 3])\n",
      "Flattened logits: torch.Size([6, 50257])\n",
      "Flattened targets: torch.Size([6])\n",
      "Cross Entropy loss: 10.876513481140137\n",
      "Perplexity: 52918.7734375\n"
     ]
    }
   ],
   "execution_count": 93
  },
  {
   "cell_type": "code",
   "id": "9d0758479b082989",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-09T19:30:48.231005Z",
     "start_time": "2025-02-09T19:30:47.769293Z"
    }
   },
   "source": [
    "from src.chapter02.Dataloader import Dataloader\n",
    "# To implement the data splitting and loading, we first define a train_ratio\n",
    "# to use 90% of the data for training and the remaining 10% as validation data \n",
    "# for model evaluation during training\n",
    "torch.manual_seed(123)\n",
    "# \n",
    "train_ratio = 0.9\n",
    "split_idx = int(train_ratio * len(raw_text))\n",
    "train_data = raw_text[:split_idx]\n",
    "val_data = raw_text[split_idx:]\n",
    "# \n",
    "train_loader = Dataloader(\n",
    "    batch_size=2,\n",
    "    max_length=GPT_CONFIG_124M_2[\"context_length\"],\n",
    "    stride=GPT_CONFIG_124M_2[\"context_length\"],\n",
    "    drop_last=True,\n",
    "    shuffle=True,\n",
    "    num_workers=0\n",
    ").create_dataloader_v1(train_data)\n",
    "# \n",
    "val_loader = Dataloader(\n",
    "    batch_size=2,\n",
    "    max_length=GPT_CONFIG_124M_2[\"context_length\"],\n",
    "    stride=GPT_CONFIG_124M_2[\"context_length\"],\n",
    "    drop_last=False,\n",
    "    shuffle=False,\n",
    "    num_workers=0\n",
    ").create_dataloader_v1(val_data)\n",
    "# \n",
    "print(\"Train loader:\")\n",
    "for x, y in train_loader:\n",
    "    print(x.shape, y.shape)\n",
    "# \n",
    "print(\"\\nValidation loader:\")\n",
    "for x, y in val_loader:\n",
    "    print(x.shape, y.shape)\n",
    "#     \n",
    "# Calculate per batch loss \n",
    "# Use CrossEntropy\n",
    "#\n",
    "def calculate_batch_loss(input_batch, target_batch, model, device):\n",
    "    input_batch = input_batch.to(device)\n",
    "    target_batch = target_batch.to(device)      \n",
    "    batch_logits = model(input_batch)\n",
    "    # print(f\"Batch logits shape: {batch_logits.shape}, targets shape: {target_batch.shape}\")\n",
    "    batch_loss = F.cross_entropy(batch_logits.flatten(0, 1), target_batch.flatten())\n",
    "    # batch_loss = F.nll_loss(\n",
    "    #     batch_logits.flatten(0, 1), \n",
    "    #     target_batch.flatten()\n",
    "    # )\n",
    "    return batch_loss\n",
    "\n",
    "# \n",
    "# Calculate loss across batches\n",
    "# \n",
    "def calculate_loss_loader(data_loader, model, device, num_batches=None):\n",
    "    total_loss = 0.\n",
    "    if len(data_loader) == 0:\n",
    "        return float(\"nan\")\n",
    "    elif num_batches is None:\n",
    "        num_batches = len(data_loader)\n",
    "    else:\n",
    "        num_batches = min(num_batches, len(data_loader))\n",
    "    for n, (input_batch, target_batch) in enumerate(data_loader):\n",
    "        if n < num_batches:\n",
    "            loss = calculate_batch_loss(input_batch, target_batch, model, device)\n",
    "            total_loss += loss.item()\n",
    "        else:\n",
    "            break\n",
    "    return total_loss / num_batches\n",
    "# \n",
    "model.to(device)\n",
    "with torch.no_grad():\n",
    "    train_loss = calculate_loss_loader(train_loader, model, device)\n",
    "    val_loss = calculate_loss_loader(val_loader, model, device)\n",
    "print(\"Default Training loss:\", train_loss)\n",
    "print(\"Default Validation loss:\", val_loss)\n",
    "# \n",
    "# Model Evaluation\n",
    "#   Calculate both training and validation losses and return\n",
    "#\n",
    "def evaluate_model(eval_model, training_loader, validn_loader, eval_device, eval_iter):\n",
    "    eval_model.eval()\n",
    "    with torch.no_grad():\n",
    "        training_loss = calculate_loss_loader(\n",
    "            training_loader, eval_model, eval_device, num_batches=eval_iter\n",
    "        )\n",
    "        validn_loss = calculate_loss_loader(\n",
    "            validn_loader, eval_model, eval_device, num_batches=eval_iter\n",
    "        )\n",
    "    eval_model.train()\n",
    "    return training_loss, validn_loss\n",
    "\n",
    "# A convenience function that we use to track whether the model improves during the training. \n",
    "# The generate_and_print_sample() function takes a text snippet (start_context) as input, \n",
    "# converts it into token IDs, and feeds it to the LLM to generate a text sample \n",
    "# using the generate_text_simple function\n",
    "def generate_and_print_sample(model, tokenizer, device, start_context):\n",
    "    model.eval()\n",
    "    context_size = model.pos_emb.weight.shape[0]\n",
    "    encoded = text_to_token_ids(start_context, tokenizer).to(device)\n",
    "    with torch.no_grad():\n",
    "        token_ids = generate_text_simple(\n",
    "            input_model=model, \n",
    "            tokenids=encoded,\n",
    "            max_new_tokens=50, \n",
    "            context_size=context_size\n",
    "        )\n",
    "    decoded_text = token_ids_to_text(token_ids, tokenizer)\n",
    "    print(decoded_text.replace(\"\\n\", \" \"))\n",
    "    model.train()\n",
    "\n",
    "\n",
    "# \n",
    "# Now we implement the model training flow \n",
    "#\n",
    "def train_model_simple(model, train_loader, val_loader,\n",
    "                       optimizer, device, num_epochs,\n",
    "                       eval_freq, eval_iter, start_context, tokenizer):\n",
    "    # \n",
    "    train_losses, val_losses, track_tokens_seen = [], [], []\n",
    "    tokens_seen, global_step = 0, -1\n",
    "    # \n",
    "    model.train()\n",
    "    model.to(device)\n",
    "    # \n",
    "    for epoch in range(num_epochs):\n",
    "        for input_batch, target_batch in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            input_batch = input_batch.to(device)\n",
    "            target_batch = target_batch.to(device)\n",
    "            #\n",
    "            loss = calculate_batch_loss(input_batch, target_batch, model, device)\n",
    "            loss.backward() # Back propagation\n",
    "            # Optimizer step function with no closure supplied is below.\n",
    "            # A closure should clear the gradients, compute the loss, and return it.\n",
    "            # For example if it was a Conjugate Grad. optimization method \n",
    "            # We had to pass a closure. A closure is nothing but a function\n",
    "            # for input, target in dataset:\n",
    "            #    def closure():\n",
    "            #        optimizer.zero_grad()\n",
    "            #        output = model(input)\n",
    "            #        loss = loss_fn(output, target)\n",
    "            #        loss.backward()\n",
    "            #        return loss\n",
    "            #    optimizer.step(closure) \n",
    "            # \n",
    "            optimizer.step() \n",
    "            \n",
    "            tokens_seen += input_batch.numel()\n",
    "            global_step += 1\n",
    "\n",
    "            if global_step % eval_freq == 0:\n",
    "                trn_loss, validn_loss = evaluate_model(\n",
    "                    model, train_loader, val_loader, device, eval_iter\n",
    "                )\n",
    "                train_losses.append(trn_loss)\n",
    "                val_losses.append(validn_loss)\n",
    "                track_tokens_seen.append(tokens_seen)\n",
    "                print(f\"Ep {epoch+1} (Step {global_step:06d}): \"\n",
    "                      f\"Train loss {trn_loss:.3f}, \"\n",
    "                      f\"Val loss {validn_loss:.3f}\"\n",
    "                )\n",
    "        # End inner for\n",
    "        generate_and_print_sample(model, tokenizer, device, start_context) # After each epoch\n",
    "    # End outer for\n",
    "    return train_losses, val_losses, track_tokens_seen\n",
    "\n",
    "\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loader:\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "\n",
      "Validation loader:\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "Default Training loss: 10.988501654730904\n",
      "Default Validation loss: 10.99034309387207\n"
     ]
    }
   ],
   "execution_count": 94
  },
  {
   "cell_type": "code",
   "id": "9f203bec3d737b0a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-09T19:31:01.938505Z",
     "start_time": "2025-02-09T19:31:01.082460Z"
    }
   },
   "source": [
    "#\n",
    "# Training Loop\n",
    "#\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "\n",
    "def plot_losses(epochs_seen, tokens_seen, train_losses, val_losses):\n",
    "    fig, ax1 = plt.subplots(figsize=(6, 4))\n",
    "    ax1.plot(epochs_seen, train_losses, label=\"Training loss\")\n",
    "    ax1.plot(\n",
    "        epochs_seen, val_losses, linestyle=\"-.\", label=\"Validation loss\"\n",
    "    )\n",
    "    ax1.set_xlabel(\"Epochs\")\n",
    "    ax1.set_ylabel(\"Loss\")\n",
    "    ax1.legend(loc=\"upper right\")\n",
    "    ax1.xaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "    ax2 = ax1.twiny()\n",
    "    ax2.plot(tokens_seen, train_losses, alpha=0)\n",
    "    ax2.set_xlabel(\"Tokens seen\")\n",
    "    fig.tight_layout()\n",
    "    plt.show()\n",
    "#\n",
    "#\n",
    "#\n",
    "torch.manual_seed(123)\n",
    "model_file_path= \"../models/GPTModel.pth\"\n",
    "num_epochs = 10\n",
    "train_losses, val_losses, tokens_seen = [], [], []\n",
    "epochs_tensor = None\n",
    "\n",
    "model = GPTModel(GPT_CONFIG_124M_2)\n",
    "model.to(device)\n",
    "optimizer = torch.optim.AdamW(\n",
    "     model.parameters(),\n",
    "    lr=0.0004,\n",
    "    weight_decay=0.1\n",
    ")\n",
    "\n",
    "if not os.path.exists(model_file_path):\n",
    "    train_losses, val_losses, tokens_seen = (\n",
    "        train_model_simple(model, train_loader, val_loader, optimizer, device,\n",
    "                           num_epochs=num_epochs, eval_freq=5, eval_iter=5,\n",
    "                           start_context=\"Every effort moves you\", tokenizer=tokenizer\n",
    "                           )\n",
    "    )\n",
    "    if epochs_tensor is None:\n",
    "        epochs_tensor = torch.linspace(0, num_epochs, len(train_losses))\n",
    "        plot_losses(epochs_tensor, tokens_seen, train_losses, val_losses)\n",
    "else:\n",
    "    print(f\"Model already exists\")\n",
    "    pass"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model already exists\n"
     ]
    }
   ],
   "execution_count": 95
  },
  {
   "cell_type": "code",
   "id": "acfc02f2040cd8ed",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-09T19:31:17.419811Z",
     "start_time": "2025-02-09T19:31:15.894917Z"
    }
   },
   "source": [
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "token_ids = generate_text_simple(\n",
    "    input_model=model,\n",
    "    tokenids=text_to_token_ids(\"Every effort moves you\", tokenizer),\n",
    "    max_new_tokens=25,\n",
    "    context_size=GPT_CONFIG_124M[\"context_length\"]\n",
    ")\n",
    "print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))\n",
    "\n",
    "# Now lets look at Temperature Scaling\n",
    "vocab = { \n",
    "    \"closer\": 0,\n",
    "    \"every\": 1, \n",
    "    \"effort\": 2, \n",
    "    \"forward\": 3,\n",
    "    \"inches\": 4,\n",
    "    \"moves\": 5, \n",
    "    \"pizza\": 6,\n",
    "    \"toward\": 7,\n",
    "    \"you\": 8,\n",
    "} \n",
    "inverse_vocab = {v: k for k, v in vocab.items()}\n",
    "# Assume the model generates the following logits \n",
    "next_token_logits = torch.tensor(\n",
    "    [4.51, 0.89, -1.90, 6.75, 1.63, -1.62, -1.89, 6.28, 1.79]\n",
    ").to(device)\n",
    "\n",
    "# We now generally do an argmax of the probabilities \n",
    "probas = torch.softmax(next_token_logits, dim=0).to(device)\n",
    "next_token_id = torch.argmax(probas).item()\n",
    "print(f\"Next token (argmax): {inverse_vocab[next_token_id]}\")\n",
    "#\n",
    "# But we can try replacing argmax with multinomial and get a sampling\n",
    "#\n",
    "torch.manual_seed(123) \n",
    "next_token_id = torch.multinomial(probas, num_samples=1).item()\n",
    "print(f\"Next token (multinomial): {inverse_vocab[next_token_id]}\")\n",
    "# \n",
    "# Let's see if multinomial can produce any other probabilites\n",
    "def print_sampled_tokens(probabs):\n",
    "    torch.manual_seed(123)\n",
    "    # Multinomial Sampling of probabilities instead of max\n",
    "    samples = [torch.multinomial(probabs, num_samples=1).item() for _ in range(1_000)]\n",
    "    sampled_ids = torch.bincount(torch.tensor(samples).to(device))\n",
    "    for cnt, freq in enumerate(sampled_ids):\n",
    "        print(f\"Sampled Freq. {freq} : {inverse_vocab[cnt]}\")\n",
    "\n",
    "print_sampled_tokens(probas)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output text:\n",
      " Every effort moves you renting\n",
      "Next token (argmax): forward\n",
      "Next token (multinomial): forward\n",
      "Sampled Freq. 72 : closer\n",
      "Sampled Freq. 2 : every\n",
      "Sampled Freq. 0 : effort\n",
      "Sampled Freq. 575 : forward\n",
      "Sampled Freq. 2 : inches\n",
      "Sampled Freq. 0 : moves\n",
      "Sampled Freq. 0 : pizza\n",
      "Sampled Freq. 343 : toward\n",
      "Sampled Freq. 6 : you\n"
     ]
    }
   ],
   "execution_count": 96
  },
  {
   "cell_type": "code",
   "id": "2edb9cac7f426800",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-09T19:31:29.666805Z",
     "start_time": "2025-02-09T19:31:29.599855Z"
    }
   },
   "source": [
    "# We can further control the distribution by a technique called temperature scaling\n",
    "# meaning dividing the logits with a >0 number\n",
    "def softmax_with_temperature(logits, temperature, dim):\n",
    "    scaled_logits = logits / temperature\n",
    "    scaled_probs =  torch.softmax(scaled_logits, dim=dim)\n",
    "    return scaled_probs\n",
    "\n",
    "# Let's check that out\n",
    "temperatures = [1, .01, 5]\n",
    "next_token_logits: Tensor = Tensor.cpu(next_token_logits)\n",
    "scaled_probas = [softmax_with_temperature(next_token_logits, float(T), 0) for T in temperatures]\n",
    "x = torch.arange(len(vocab))\n",
    "bar_width = 0.5\n",
    "fig, ax = plt.subplots(figsize=(6, 4))\n",
    "for i, T in enumerate(temperatures):\n",
    "    rects = ax.bar(x + i * bar_width, \n",
    "                   scaled_probas[i], \n",
    "                   bar_width, \n",
    "                   label=f'Temperature = {T}'\n",
    "    )\n",
    "ax.set_ylabel('Probability')\n",
    "ax.set_xlabel('Words')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(vocab.keys(), rotation=90)\n",
    "ax.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 600x400 with 1 Axes>"
      ],
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 97
  },
  {
   "cell_type": "markdown",
   "id": "48c5d9214db5c7e3",
   "metadata": {},
   "source": [
    "## Top K sampling"
   ]
  },
  {
   "cell_type": "code",
   "id": "af408748892d4b7f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-09T19:31:42.287351Z",
     "start_time": "2025-02-09T19:31:41.785526Z"
    }
   },
   "source": [
    "# Previously we implemented a probabilistic sampling approach coupled with \n",
    "# temperature scaling to increase the diversity of the outputs.  This method \n",
    "# allows for the exploring of less likely but potentially more interesting and \n",
    "# creative paths in the generation process.\n",
    "#\n",
    "# Top-k sampling, when combined with probabilistic sampling and temperature \n",
    "# scaling, can improve the text generation results.\n",
    "#\n",
    "# Here we can restrict the sampled tokens to the top-k most likely tokens \n",
    "# and exclude all other tokens from the selection process by masking their \n",
    "# probability scores\n",
    "# \n",
    "top_k = 3\n",
    "torch.set_printoptions(sci_mode=False)\n",
    "# \n",
    "top_logits, top_pos = torch.topk(next_token_logits, top_k)\n",
    "print(\"\\nTop Logits: \", top_logits)\n",
    "print(\"Top Positions: \", top_pos)\n",
    "print(f\"Next token logits: {next_token_logits}\\n\")\n",
    "\n",
    "# Pytorch WHERE function to set the logit values of tokens that are below the lowest \n",
    "# logit value within our top-three selection to negative infinity (-inf)\n",
    "#\n",
    "new_logits = torch.where(\n",
    "    condition=next_token_logits < top_logits[-1],\n",
    "    input=torch.tensor(float('-inf')),\n",
    "    other=next_token_logits\n",
    ")\n",
    "print(\"New Logits: \", new_logits)\n",
    "\n",
    "# Now apply the softmax\n",
    "topk_probas = torch.softmax(new_logits, dim=0)\n",
    "print(f\"top k probabilities: {topk_probas}\")\n",
    "\n",
    "\n",
    "# \n",
    "# We can now apply the temperature scaling and multinomial function for probabilistic \n",
    "# sampling to select the next token among these three non-zero probability scores to \n",
    "# GENERATE THE NEXT TOKEN with more diversity.\n",
    "# \n",
    "def generate(model, idx, max_new_tokens, context_size, temperature=0.0, top_k: int=None, eos_id=None):\n",
    "    # print(\"Entering generate()..\")\n",
    "    # print(idx.shape)\n",
    "    for i in range(max_new_tokens):\n",
    "        # print(f\"idx: [{i}]: {idx}\")\n",
    "        idx_cond = idx[:, -context_size:]\n",
    "        with torch.no_grad():\n",
    "            logits = model(idx_cond)\n",
    "        #     \n",
    "        logits = logits[:, -1, :] # ([1, 50257])\n",
    "        # print(logits.shape)\n",
    "        if top_k is not None:\n",
    "            top_logits, _ = torch.topk(logits, top_k)\n",
    "            min_val: Tensor = top_logits[:, -1] # Less than the lowest value of top k\n",
    "            # Now mark the minvals with -inf, so softmax becomes 0\n",
    "            logits = torch.where(\n",
    "                logits < min_val,\n",
    "                torch.tensor(float('-inf')).to(logits.device),\n",
    "                logits\n",
    "            )\n",
    "        if temperature > 0.0:\n",
    "            probs = softmax_with_temperature(logits, temperature, dim=-1)\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)\n",
    "        else: \n",
    "            idx_next = torch.argmax(logits, dim=-1, keepdim=True)\n",
    "\n",
    "        if idx_next == eos_id:\n",
    "            break\n",
    "\n",
    "        # Next word\n",
    "        idx = torch.cat((idx, idx_next), dim=1)\n",
    "\n",
    "    return idx\n",
    "\n",
    "token_ids = generate(\n",
    "    model=model,\n",
    "    idx=text_to_token_ids(\"Every effort moves you\", tokenizer),\n",
    "    max_new_tokens=15,\n",
    "    context_size=GPT_CONFIG_124M_2[\"context_length\"],\n",
    "    top_k=25,\n",
    "    temperature=1.4\n",
    ")\n",
    "print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))\n",
    "\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top Logits:  tensor([6.7500, 6.2800, 4.5100])\n",
      "Top Positions:  tensor([3, 7, 0])\n",
      "Next token logits: tensor([ 4.5100,  0.8900, -1.9000,  6.7500,  1.6300, -1.6200, -1.8900,  6.2800,\n",
      "         1.7900])\n",
      "\n",
      "New Logits:  tensor([4.5100,   -inf,   -inf, 6.7500,   -inf,   -inf,   -inf, 6.2800,   -inf])\n",
      "top k probabilities: tensor([0.0615, 0.0000, 0.0000, 0.5775, 0.0000, 0.0000, 0.0000, 0.3610, 0.0000])\n",
      "Output text:\n",
      " Every effort moves you Demon helicopter�RLhess bindings Lancadobementedurnal45 Livingston resembleeca Butter\n"
     ]
    }
   ],
   "execution_count": 98
  },
  {
   "cell_type": "code",
   "id": "7e164cfeef1f9707",
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "source": [
    "# torch.save(model.state_dict(), f\"/Users/amlanchatterjee/Documents/ws/python/PycharmProjects/SimpleLLMProject/models/{GPT_CONFIG_124M_2['model_name']}.pth\")\n",
    "MODEL_PATH = f\"../models/{GPT_CONFIG_124M_2['model_name']}.pth\"\n",
    "#\n",
    "if not os.path.exists(model_file_path):\n",
    "    try:\n",
    "        torch.save({\n",
    "            \"model_state_dict\": model.state_dict(),\n",
    "            \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "            },\n",
    "            MODEL_PATH\n",
    "        )\n",
    "        print(f\"Model saved at {MODEL_PATH}\\n\")\n",
    "    except Exception as e:\n",
    "        print(f\"Encountered exception : {e}\")\n",
    "else:\n",
    "    print(f\"Model exists at {MODEL_PATH}, not overwritten\")\n",
    "#"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "d0a3f72c8cebc09d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-09T19:19:50.721276Z",
     "start_time": "2025-02-09T19:19:49.241875Z"
    }
   },
   "source": [
    "# Load the model back\n",
    "if os.path.exists(MODEL_PATH):\n",
    "    checkpoint = torch.load(MODEL_PATH, map_location=device, weights_only=True)\n",
    "    loaded_model = GPTModel(GPT_CONFIG_124M_2).to(device)\n",
    "    try:\n",
    "        loaded_model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "        optimizer = torch.optim.AdamW(loaded_model.parameters(),\n",
    "                                     lr=GPT_CONFIG_124M_2[\"lr\"],\n",
    "                                     weight_decay=GPT_CONFIG_124M_2[\"weight_decay\"])\n",
    "        optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n",
    "        print(f\"Model and Optimizer successfully loaded from \\n{MODEL_PATH}\\n\")\n",
    "    except Exception as ex:\n",
    "        print(f\"Encountered exception : {ex}\")\n",
    "\n",
    "    loaded_model.train()\n",
    "    print(\"Model set to train mode\")\n",
    "else:\n",
    "    print(f\"Model not found at {MODEL_PATH}. Nothing to load\\n\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model and Optimizer successfully loaded from \n",
      "../models/GPTModel.pth\n",
      "\n",
      "Model set to train mode\n"
     ]
    }
   ],
   "execution_count": 49
  },
  {
   "cell_type": "markdown",
   "id": "d38b09e853311dec",
   "metadata": {},
   "source": [
    "### OpenAI also shares the weights of larger models: 355M, 774M, and 1558M\n",
    "![image](../data/model_arch_stack.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "68e160927f07d23a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-09T19:20:19.731466Z",
     "start_time": "2025-02-09T19:20:16.193643Z"
    }
   },
   "source": [
    "# Load the downloaded GPT Data\n",
    "import os, sys\n",
    "import urllib.request\n",
    "from src.chapter05.gpt_download import download_and_load_gpt2\n",
    "\n",
    "url = (\n",
    "    \"https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/ch05/01_main-chapter-code/gpt_download.py\"\n",
    ")\n",
    "filename = url.split('/')[-1]\n",
    "# print(filename)\n",
    "filename = \"./chapter05/\"+filename\n",
    "# print(filename)\n",
    "if not os.path.exists(filename):\n",
    "    try:\n",
    "        urllib.request.urlretrieve(url, filename)\n",
    "        print(f\"Downloaded {filename}\")\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        sys.exit()\n",
    "#\n",
    "settings, gpt_params = download_and_load_gpt2(model_size=\"124M\", models_dir=\"../data/gpt2\")\n",
    "print(f\"\\nParams: {gpt_params.keys()}\")\n",
    "print(f\"Settings: {settings}\")\n",
    "print(f\"Token embedding layer weight tensor dimensions: {gpt_params[\"wte\"].shape}\")\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model directory: ../data/gpt2/124M\n",
      "File already exists and is up-to-date: ../data/gpt2/124M/checkpoint\n",
      "File already exists and is up-to-date: ../data/gpt2/124M/encoder.json\n",
      "File already exists and is up-to-date: ../data/gpt2/124M/hparams.json\n",
      "File already exists and is up-to-date: ../data/gpt2/124M/model.ckpt.data-00000-of-00001\n",
      "File already exists and is up-to-date: ../data/gpt2/124M/model.ckpt.index\n",
      "File already exists and is up-to-date: ../data/gpt2/124M/model.ckpt.meta\n",
      "File already exists and is up-to-date: ../data/gpt2/124M/vocab.bpe\n",
      "\n",
      "Params: dict_keys(['blocks', 'b', 'g', 'wpe', 'wte'])\n",
      "Settings: {'n_vocab': 50257, 'n_ctx': 1024, 'n_embd': 768, 'n_head': 12, 'n_layer': 12}\n",
      "Token embedding layer weight tensor dimensions: (50257, 768)\n"
     ]
    }
   ],
   "execution_count": 51
  },
  {
   "cell_type": "code",
   "id": "ba48ffe795182755",
   "metadata": {},
   "source": [
    "\n",
    "# After loading the GPT-2 model weights into Python, we still need to transfer \n",
    "# them from the settings and params dictionaries into our GPTModel instance. \n",
    "# First, we create a dictionary that lists the differences between the \n",
    "# different GPT model sizes\n",
    "model_configs = {\n",
    "    \"gpt2-small (124M)\": {\"emb_dim\": 768, \"n_layers\": 12, \"n_heads\": 12},\n",
    "    \"gpt2-medium (355M)\": {\"emb_dim\": 1024, \"n_layers\": 24, \"n_heads\": 16},\n",
    "    \"gpt2-large (774M)\": {\"emb_dim\": 1280, \"n_layers\": 36, \"n_heads\": 20},\n",
    "    \"gpt2-xl (1558M)\": {\"emb_dim\": 1600, \"n_layers\": 48, \"n_heads\": 25},\n",
    "}\n",
    "model_name=\"gpt2-small (124M)\"\n",
    "#\n",
    "NEW_GPT_CONFIG = GPT_CONFIG_124M.copy()\n",
    "NEW_GPT_CONFIG.update({\"model_name\": model_name})\n",
    "# Update the value ex. {\"emb_dim\": 768, \"n_layers\": 12, \"n_heads\": 12},\n",
    "NEW_GPT_CONFIG.update(model_configs[model_name])\n",
    "NEW_GPT_CONFIG.update({\"context_length\": 1024})\n",
    "NEW_GPT_CONFIG.update({\"qkv_bias\": True})\n",
    "#\n",
    "print(f\"{model_name}: {model_configs[model_name]}\")\n",
    "print(\"NEW_GPT_CONFIG:\\n\"+\"\".join(f\"\\t{k}: {v}\\n\" for k, v in sorted(NEW_GPT_CONFIG.items())))\n",
    "#\n",
    "newgpt = GPTModel(NEW_GPT_CONFIG).to(device)\n",
    "newgpt.eval()\n",
    "# \n",
    "# Before we assign the loaded openai weights into the model, we will first define \n",
    "# a small assign utility function that checks whether two tensors or arrays \n",
    "# (left and right) have the same dimensions or shape and returns the right tensor \n",
    "# as trainable PyTorch parameters\n",
    "#\n",
    "def assign(left: Tensor, right: Tensor) -> Tensor:\n",
    "    if left.shape != right.shape:\n",
    "        raise ValueError(f\"Shape mismatch Left shape: {left.shape} Right shape: {right.shape}\")\n",
    "    return torch.nn.Parameter(torch.tensor(right).to(device))\n",
    "# \n",
    "print(f\"Params: {gpt_params.keys()}\")\n",
    "# print(f\"Params: {gpt_params}\")\n",
    "print(f\"GPT Parameter Blocks Count: {len(gpt_params[\"blocks\"])}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "dbc4d3fb2c383826",
   "metadata": {},
   "source": [
    "#\n",
    "# Load OpenAI GPT2 Weights into our GPTModel code\n",
    "#\n",
    "import numpy as np\n",
    "\n",
    "def load_weights_into_gpt(gpt, params):\n",
    "    gpt.tok_emb.weight = assign(gpt.tok_emb.weight, params['wte'])\n",
    "    gpt.pos_emb.weight = assign(gpt.pos_emb.weight, params['wpe'])\n",
    "\n",
    "    for b in range(len(params[\"blocks\"])):\n",
    "        q_w, k_w, v_w = np.split(\n",
    "            (params[\"blocks\"][b][\"attn\"][\"c_attn\"])[\"w\"], 3, axis=-1)\n",
    "        gpt.trf_blocks[b].att.W_query.weight = assign(\n",
    "            gpt.trf_blocks[b].att.W_query.weight, q_w.T)\n",
    "        gpt.trf_blocks[b].att.W_key.weight = assign(\n",
    "            gpt.trf_blocks[b].att.W_key.weight, k_w.T)\n",
    "        gpt.trf_blocks[b].att.W_value.weight = assign(\n",
    "            gpt.trf_blocks[b].att.W_value.weight, v_w.T)\n",
    "\n",
    "        q_b, k_b, v_b = np.split(\n",
    "            (params[\"blocks\"][b][\"attn\"][\"c_attn\"])[\"b\"], 3, axis=-1)\n",
    "        gpt.trf_blocks[b].att.W_query.bias = assign(\n",
    "            gpt.trf_blocks[b].att.W_query.bias, q_b)\n",
    "        gpt.trf_blocks[b].att.W_key.bias = assign(\n",
    "            gpt.trf_blocks[b].att.W_key.bias, k_b)\n",
    "        gpt.trf_blocks[b].att.W_value.bias = assign(\n",
    "            gpt.trf_blocks[b].att.W_value.bias, v_b)\n",
    "\n",
    "        gpt.trf_blocks[b].att.out_proj.weight = assign(\n",
    "            gpt.trf_blocks[b].att.out_proj.weight, \n",
    "            params[\"blocks\"][b][\"attn\"][\"c_proj\"][\"w\"].T)\n",
    "        gpt.trf_blocks[b].att.out_proj.bias = assign(\n",
    "            gpt.trf_blocks[b].att.out_proj.bias, \n",
    "            params[\"blocks\"][b][\"attn\"][\"c_proj\"][\"b\"])\n",
    "\n",
    "        gpt.trf_blocks[b].sff.layers[0].weight = assign(\n",
    "            gpt.trf_blocks[b].sff.layers[0].weight, \n",
    "            params[\"blocks\"][b][\"mlp\"][\"c_fc\"][\"w\"].T)\n",
    "        gpt.trf_blocks[b].sff.layers[0].bias = assign(\n",
    "            gpt.trf_blocks[b].sff.layers[0].bias, \n",
    "            params[\"blocks\"][b][\"mlp\"][\"c_fc\"][\"b\"])\n",
    "        gpt.trf_blocks[b].sff.layers[2].weight = assign(\n",
    "            gpt.trf_blocks[b].sff.layers[2].weight, \n",
    "            params[\"blocks\"][b][\"mlp\"][\"c_proj\"][\"w\"].T)\n",
    "        gpt.trf_blocks[b].sff.layers[2].bias = assign(\n",
    "            gpt.trf_blocks[b].sff.layers[2].bias, \n",
    "            params[\"blocks\"][b][\"mlp\"][\"c_proj\"][\"b\"])\n",
    "\n",
    "        gpt.trf_blocks[b].norm1.scale = assign(\n",
    "            gpt.trf_blocks[b].norm1.scale, \n",
    "            params[\"blocks\"][b][\"ln_1\"][\"g\"])\n",
    "        gpt.trf_blocks[b].norm1.shift = assign(\n",
    "            gpt.trf_blocks[b].norm1.shift, \n",
    "            params[\"blocks\"][b][\"ln_1\"][\"b\"])\n",
    "        gpt.trf_blocks[b].norm2.scale = assign(\n",
    "            gpt.trf_blocks[b].norm2.scale, \n",
    "            params[\"blocks\"][b][\"ln_2\"][\"g\"])\n",
    "        gpt.trf_blocks[b].norm2.shift = assign(\n",
    "            gpt.trf_blocks[b].norm2.shift, \n",
    "            params[\"blocks\"][b][\"ln_2\"][\"b\"])\n",
    "\n",
    "    gpt.final_norm.scale = assign(gpt.final_norm.scale, params[\"g\"])\n",
    "    gpt.final_norm.shift = assign(gpt.final_norm.shift, params[\"b\"])\n",
    "    gpt.out_head.weight = assign(gpt.out_head.weight, params[\"wte\"])\n",
    "    "
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "c0eeef9e5cadf037",
   "metadata": {},
   "source": [
    "# Now lets try to load the weights and see\n",
    "newgpt_loaded = False\n",
    "if not newgpt_loaded:\n",
    "    load_weights_into_gpt(newgpt, gpt_params)\n",
    "    newgpt.to(device)\n",
    "    print(\"Loaded weights into newgpt GPTModel..\")\n",
    "    newgpt_loaded = True\n",
    "else:\n",
    "    print(\"GPTModel is already loaded..\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "53993f0b2ef4021a",
   "metadata": {},
   "source": [
    "# Now let's generate using the actual GPT trained weights\n",
    "torch.manual_seed(123)\n",
    "token_ids = generate(\n",
    "    model=newgpt,\n",
    "    idx=text_to_token_ids(\"Every effort moves you\", tokenizer).to(device),\n",
    "    max_new_tokens=25,\n",
    "    context_size=NEW_GPT_CONFIG[\"context_length\"],\n",
    "    top_k=50,\n",
    "    temperature=1.5\n",
    ")\n",
    "\n",
    "print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "850187dca8e7693f",
   "metadata": {},
   "source": [
    "# Fine-tuning for Classification"
   ]
  },
  {
   "cell_type": "code",
   "id": "533a90b85f4e1468",
   "metadata": {},
   "source": [
    "from pathlib import Path\n",
    "from src.chapter06 import DownloadDataset\n",
    "# \n",
    "# Download ehtSPAM Dataset\n",
    "# \n",
    "url = \"https://archive.ics.uci.edu/static/public/228/sms+spam+collection.zip\"\n",
    "zip_path = \"../data/sms_spam_collection.zip\"\n",
    "extracted_path = \"../data/sms_spam_collection\"\n",
    "data_file_path = Path(extracted_path) / \"SMSSpamCollection.tsv\"\n",
    "\n",
    "# \n",
    "DownloadDataset.download_and_unzip_spam_data(url, \n",
    "                                             zip_path, \n",
    "                                             extracted_path, \n",
    "                                             data_file_path)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "bcc8d30ddb5d1c8",
   "metadata": {},
   "source": [
    "from pandas import DataFrame\n",
    "import pandas as pd\n",
    "\n",
    "# At this point the spam dataset should have been downloaded at {data_file_path}\n",
    "df: DataFrame = None\n",
    "\n",
    "if data_file_path.exists() and data_file_path.is_file():\n",
    "    df = pd.read_csv(data_file_path, sep=\"\\t\", header=None, names=[\"Label\", \"Text\"])\n",
    "   \n",
    "# df.head(10)\n",
    "#Let's take a look at class distributions\n",
    "print(f\"Class counts: {df[\"Label\"].value_counts()}\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "eb7fba4d33fca94f",
   "metadata": {},
   "source": [
    "# Considering there are so many more hams than spams we need to create a somewhat balanced dataset\n",
    "import os\n",
    "def create_balanced_dataset(df: DataFrame) -> DataFrame:\n",
    "    # print(df.shape)\n",
    "    bal_df = df\n",
    "    num_spam = df[df[\"Label\"] == \"spam\"].shape[0]\n",
    "    num_ham = df[df[\"Label\"] == \"ham\"].shape[0]\n",
    "    print(f\"Spam and Ham counts: {num_spam}, {num_ham} \\n\")\n",
    "\n",
    "    # If num_spam is a lot less than num_ham\n",
    "    if num_spam < num_ham or num_spam == 0:\n",
    "        ham_subset = df[df[\"Label\"] == \"ham\"].sample(num_spam, random_state=123)\n",
    "\n",
    "    if ham_subset is not None:\n",
    "        bal_df = pd.concat([ham_subset, df[df[\"Label\"] == \"spam\"]])\n",
    "\n",
    "    return bal_df\n",
    "\n",
    "balanced_df = create_balanced_dataset(df)\n",
    "# print(f\"Rebalanced dataset \\n {balanced_df[\"Label\"].value_counts()}\")\n",
    "\n",
    "# Now we are going to change the string class labels to ints\n",
    "balanced_df[\"Label\"] = balanced_df[\"Label\"].map({\"ham\": 0, \"spam\": 1})\n",
    "\n",
    "# This process is similar to converting text into token IDs. However, instead \n",
    "# of using the GPT vocabulary, which consists of more than 50,000 words, we \n",
    "# are dealing with just two token IDs: 0 and 1.\n",
    "# \n",
    "def random_split(df, train_frac, validation_frac):\n",
    "    df = df.sample(frac=1, random_state=123).reset_index(drop=True)\n",
    "    train_end = int(len(df) * train_frac) # * 0.7\n",
    "    validation_end = train_end + int(len(df) * validation_frac)  # * 0.1\n",
    "    \n",
    "    train_df = df[:train_end] # 70%\n",
    "    valid_df = df[train_end:validation_end] # 10%\n",
    "    test_df = df[validation_end:]   # 20%\n",
    "    \n",
    "    return train_df, valid_df, test_df\n",
    "\n",
    "# Next, we create a random_split function to split the dataset into three parts: \n",
    "# 70% for training, 10% for validation, and 20% for testing\n",
    "training_df, validation_df, testing_df = random_split(balanced_df, 0.7, 0.1)\n",
    "# \n",
    "print(f\"Training dataset \\n {training_df['Label'].value_counts()}\")\n",
    "print(f\"Validation dataset \\n {validation_df['Label'].value_counts()}\")\n",
    "print(f\"Training dataset \\n {testing_df['Label'].value_counts()}\")\n",
    "# \n",
    "# Save the files\n",
    "if not os.path.exists(\"../data/train.csv\"):\n",
    "    training_df.to_csv(\"../data/train.csv\", index=None)\n",
    "    \n",
    "if not os.path.exists(\"../data/validation.csv\"):   \n",
    "    validation_df.to_csv(\"../data/validate.csv\", index=None)\n",
    "    \n",
    "if not os.path.exists(\"../data/test.csv\"):    \n",
    "    testing_df.to_csv(\"../data/test.csv\", index=None)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "75ffdb033fba532f",
   "metadata": {},
   "source": [
    "### 6.3 Setting up PyTorch Data Loaders"
   ]
  },
  {
   "cell_type": "code",
   "id": "a13c23eaebccca8a",
   "metadata": {},
   "source": [
    "import tiktoken\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "print(tokenizer.encode(\"<|endoftext|>\", allowed_special={\"<|endoftext|>\"}))\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "dfb2d42b723f5113",
   "metadata": {},
   "source": [
    "from src.chapter06.SpamDataset import SpamDataset\n",
    "\n",
    "# Since each row of training data has varying length, we are going to padd all \n",
    "# rows to the size of the max length of the longest row using \"<|endoftext|>\" or rather \n",
    "# its token equivalent i.e. 50256\n",
    "train_dataset = SpamDataset(\n",
    "    csv_file=\"../data/train.csv\",\n",
    "    max_length=None,\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "print(f\"Max length of training set : {train_dataset.max_length}\\n\")\n",
    "\n",
    "val_dataset = SpamDataset(\n",
    "    csv_file=\"../data/validate.csv\",\n",
    "    max_length=train_dataset.max_length,\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "print(f\"Max length of validate set : {val_dataset.max_length}\\n\")\n",
    "\n",
    "test_dataset = SpamDataset(\n",
    "    csv_file=\"../data/test.csv\",\n",
    "    max_length=train_dataset.max_length,\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "print(f\"Max length of test set : {test_dataset.max_length}\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "828481d7cb91412c",
   "metadata": {},
   "source": [
    "#### NOTE: The difference with text prediction here is that for each sample we have a class label\n",
    "#### associated with it using the datasets as inputs, we can now instantiate the data loaders\n",
    "#### similarly to when we were working with text data. However, in this case,  the targets\n",
    "#### represent class labels rather than the next tokens in the text. For instance, if we \n",
    "#### choose a batch size of 8, each batch will consist of eight training examples of length \n",
    "#### '120' and the corresponding class label of each example"
   ]
  },
  {
   "cell_type": "code",
   "id": "8a0f692e09c92e85",
   "metadata": {},
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "torch.manual_seed(123)\n",
    "num_workers = 0\n",
    "batch_size = 8\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    dataset=train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=num_workers,\n",
    "    drop_last=True,\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    dataset=val_dataset,\n",
    "    batch_size=batch_size,\n",
    "    num_workers=num_workers,\n",
    "    drop_last=False,\n",
    ")\n",
    "test_loader = DataLoader(\n",
    "    dataset=test_dataset,\n",
    "    batch_size=batch_size,\n",
    "    num_workers=num_workers,\n",
    "    drop_last=False,\n",
    ")\n",
    "\n",
    "for input_batch, target_batch in train_loader:\n",
    "    pass\n",
    "print(\"[training] Input batch dimensions:\", input_batch.shape)\n",
    "print(\"[training] Label batch dimensions\", target_batch.shape)\n",
    "\n",
    "# Number of batches in each dataset\n",
    "print(f\"{len(train_loader)} training batches\")\n",
    "print(f\"{len(val_loader)} validation batches\")\n",
    "print(f\"{len(test_loader)} test batches\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "177359b3787f5cf7",
   "metadata": {},
   "source": [
    "from src.chapter05.gpt_download import download_and_load_gpt2\n",
    "#\n",
    "# Now we start initializing our model and load the pretrained weights\n",
    "#\n",
    "from chapter04.GPTModel import GPTModel\n",
    "#\n",
    "#\n",
    "model_name = \"gpt2-small (124M)\"\n",
    "INPUT_PROMPT = \"Every effort moves\"\n",
    "BASE_CONFIG = {\n",
    "    \"vocab_size\" : 50257,\n",
    "    \"context_length\" : 1024,\n",
    "    \"drop_rate\" : 0.0,\n",
    "    \"qkv_bias\" : True,\n",
    "    \"lr\": 5e-4,\n",
    "    \"weight_decay\": 0.1\n",
    "}\n",
    "model_configs = {\n",
    "    \"gpt2-small (124M)\": {\"emb_dim\": 768, \"n_layers\": 12, \"n_heads\": 12},\n",
    "    \"gpt2-medium (355M)\": {\"emb_dim\": 1024, \"n_layers\": 24, \"n_heads\": 16},\n",
    "    \"gpt2-large (774M)\": {\"emb_dim\": 1280, \"n_layers\": 36, \"n_heads\": 20},\n",
    "    \"gpt2-xl (1558M)\": {\"emb_dim\": 1600, \"n_layers\": 48, \"n_heads\": 25},\n",
    "}\n",
    "model_name=\"gpt2-small (124M)\"\n",
    "#\n",
    "BASE_CONFIG.update({\"model_name\": model_name})\n",
    "BASE_CONFIG.update(model_configs[model_name])\n",
    "# print(\"BASE_CONFIG:\\n\"+\"\".join(f\"\\t{k}: {v}\\n\" for k, v in sorted(BASE_CONFIG.items())))\n",
    "#\n",
    "model_size = model_name.split(\" \")[-1].lstrip(\"(\").rstrip(\")\")\n",
    "small_gpt_loaded = False\n",
    "\n",
    "if not small_gpt_loaded:\n",
    "    small_gpt_model = GPTModel(BASE_CONFIG).to(device)\n",
    "    settings, params = download_and_load_gpt2(\n",
    "        model_size=model_size,\n",
    "        models_dir=\"../data/gpt2\"\n",
    "    )\n",
    "    load_weights_into_gpt(small_gpt_model, params)\n",
    "    # small_gpt_model.eval()\n",
    "    print(\"Loaded weights on to small_gpt_model\")\n",
    "else:\n",
    "    print(\"small_gpt_model is already loaded\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "b49206bdd6a269b6",
   "metadata": {},
   "source": [
    "# To test after loading the model weights into the GPTModel, we reuse the text generation utility\n",
    "# to generate coherent text\n",
    "#\n",
    "text_1 = \"Every effort moves you\"\n",
    "token_ids = generate_text_simple(\n",
    "    input_model=small_gpt_model,\n",
    "    tokenids=text_to_token_ids(text_1, tokenizer),\n",
    "    max_new_tokens=15,\n",
    "    context_size=BASE_CONFIG[\"context_length\"]\n",
    ")\n",
    "\n",
    "print(token_ids_to_text(token_ids, tokenizer))\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "7e2d6f94b699b652",
   "metadata": {},
   "source": [
    "# Before we train the classifier lets try the model as is\n",
    "text_2 = (\n",
    "    \"Is the following text 'spam'? Answer with 'yes' or 'no':\"\n",
    "    \" 'You are a winner you have been specially\"\n",
    "    \" selected to receive $1000 cash or a $2000 award.'\"\n",
    ")\n",
    "#\n",
    "token_ids = generate_text_simple(\n",
    "    input_model=small_gpt_model,\n",
    "    tokenids=text_to_token_ids(text_2, tokenizer),\n",
    "    max_new_tokens=15,\n",
    "    context_size=BASE_CONFIG[\"context_length\"]\n",
    ")\n",
    "#\n",
    "print(token_ids_to_text(token_ids, tokenizer))\n",
    "\n",
    "# Clearly the model is unable to answer"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "72d040c09f0a6a96",
   "metadata": {},
   "source": [
    "# Classification finetuning - Adding a Classification Head\n",
    "![Classification Fine Tuning Image](../data/classification_tuning.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "defa84126df77d59",
   "metadata": {},
   "source": [
    "#\n",
    "# Essentially we will replace the last Sequential layer or Head which mapped \n",
    "# from 768 dimentions to 50257 vocabulary dimensions with a layer that maps \n",
    "# the 768 dimensions to just 2 i.e. 1 and 0\n",
    "#\n",
    "# print(small_gpt_model)\n",
    "# First freeze the model\n",
    "for param in small_gpt_model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Now replace the output layer i.e. out_head with new one\n",
    "torch.manual_seed(123)\n",
    "num_classes = 2 # Spam or Ham\n",
    "\n",
    "# Remember each layer must be on GPU\n",
    "small_gpt_model.out_head = torch.nn.Linear(\n",
    "    in_features = BASE_CONFIG[\"emb_dim\"],\n",
    "    out_features = num_classes\n",
    ").to(device)\n",
    "#\n",
    "# NOTE: This new layer will have the requires_grad set to True by default\n",
    "# that means if we train this model only this layer will be trained.\n",
    "#\n",
    "# While this is sufficient, as per Sebastian the accuracy improves if we\n",
    "# also train the last transformer block and the last LayerNorm module.\n",
    "#\n",
    "# So Set the requires_grad for the last transformer block\n",
    "for param in small_gpt_model.trf_blocks[-1].parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "# Set the requires_grad for the last LayerNorm\n",
    "for param in small_gpt_model.final_norm.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "# To test it we can feed it an example text\n",
    "inputs = tokenizer.encode(\"Do you have time\")\n",
    "inputs = torch.tensor(inputs).unsqueeze(0).to(device)\n",
    "#\n",
    "print(\"Inputs: \", inputs)\n",
    "print(\"Inputs dimensions: \", inputs.shape)\n",
    "\n",
    "with torch.no_grad():\n",
    "     outputs = small_gpt_model(inputs)\n",
    "#\n",
    "print(\"Outputs dimensions: \", outputs.shape)\n",
    "assert(outputs.shape[-1] == num_classes) # Number of output classes\n",
    "print(\"Outputs: \\n\", outputs)\n",
    "# Number of Output rows now correspond to input token count which is 4 in this case\n",
    "# but the embeddings dimension is only 2 instead of 50257 because of the new \n",
    "# output head\n",
    "\n",
    "# We don’t need to fine-tune all four output rows; instead, we can focus on a \n",
    "# single output token. In particular, we will focus on the last row corresponding \n",
    "# to the last output token BECAUSE LAST TOKEN IS THE ONLY ONE WITH ALL THE ATTENTION OF \n",
    "# ALL OF ITS PREVIOUS TOKENS\n",
    "print(\"Last output token:\", outputs[:, -1, :])\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "f2aafc8bf0654f7f",
   "metadata": {},
   "source": [
    "# Before we finetune the model we need to implement the model evaluation functions\n",
    "# Similar to our previous approach we take the next token id generated, calculate \n",
    "# probabilities and use argmax to get the highest probability. Only here its in 2\n",
    "# instead of 50257 dimensions\n",
    "# print(f\"Last output token: {outputs[:, -1, :]}\")\n",
    "\n",
    "# Now we can get the class label\n",
    "probas = torch.softmax(outputs[:, -1, :], dim=-1)\n",
    "label = torch.argmax(probas)\n",
    "print(f\"Class Label: {label.item()} \\n\")\n",
    "\n",
    "# This concept can be used to design an accuracy loader\n",
    "def calc_accuracy_loader(data_loader, small_gpt_model, device, num_batches=None):\n",
    "    small_gpt_model.eval()\n",
    "    correct_predictions, num_examples = 0, 0\n",
    "    if num_batches is None:\n",
    "        num_batches = len(data_loader)\n",
    "    else:\n",
    "        num_batches = min(num_batches, len(data_loader))\n",
    "        \n",
    "    for i, (input_batch, target_batch) in enumerate(data_loader):\n",
    "        if i < num_batches:\n",
    "            input_batch = input_batch.to(device)\n",
    "            target_batch = target_batch.to(device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                logits = small_gpt_model(input_batch)[:, -1, :]\n",
    "\n",
    "            predicted_labels = torch.argmax(logits, dim=-1)\n",
    "            num_examples += predicted_labels.shape[0]\n",
    "            correct_predictions += ((predicted_labels == target_batch).sum().item())\n",
    "\n",
    "        else:\n",
    "            break\n",
    "    return correct_predictions / num_examples\n",
    "#\n",
    "#\n",
    "torch.manual_seed(123)\n",
    "small_gpt_model.to(device)\n",
    "#\n",
    "train_accuracy = calc_accuracy_loader(\n",
    "    train_loader, small_gpt_model, device, num_batches=10\n",
    ")\n",
    "val_accuracy = calc_accuracy_loader(\n",
    "    val_loader, small_gpt_model, device, num_batches=10\n",
    ")\n",
    "test_accuracy = calc_accuracy_loader(\n",
    "    test_loader, small_gpt_model, device, num_batches=10\n",
    ")\n",
    "#\n",
    "print(f\"Training accuracy: {train_accuracy*100:.2f}%\")\n",
    "print(f\"Validation accuracy: {val_accuracy*100:.2f}%\")\n",
    "print(f\"Test accuracy: {test_accuracy*100:.2f}%\")\n",
    "#"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "89c11a1d6d0f813c",
   "metadata": {},
   "source": [
    "# As we can see the prediction accuracy is almost random i.e. 50%\n",
    "# \n",
    "# Before we fine tune the model we need to describe the loss function\n",
    "# Our objective is to maximize the spam classification accuracy of the \n",
    "# model, which means that the preceding code should output the correct \n",
    "# class labels: 0 for non-spam and 1 for spam.\n",
    "# \n",
    "# Because classification accuracy is not a differentiable function, we \n",
    "# can use cross-entropy loss as a proxy to maximize accuracy\n",
    "\n",
    "def calc_batch_loss(input_batch, target_batch, small_gpt_model, device):\n",
    "    input_batch = input_batch.to(device)\n",
    "    target_batch = target_batch.to(device)\n",
    "    logits = small_gpt_model(input_batch)\n",
    "    logits = logits[:, -1, :] # Get the last token\n",
    "    loss = torch.nn.functional.cross_entropy(logits, target_batch)\n",
    "    return loss\n",
    "\n",
    "# Now to calculate loss for all the batches using the above function\n",
    "def calc_all_batch_loss(data_loader, small_gpt_model, device, num_batches=None) -> float:\n",
    "    total_loss: float = 0.\n",
    "    if len(data_loader) == 0:\n",
    "        return float(\"nan\")\n",
    "    elif num_batches is None:\n",
    "        num_batches = len(data_loader)\n",
    "    else:\n",
    "        num_batches = min(num_batches, len(data_loader))\n",
    "        \n",
    "    for i, (input_batch, target_batch) in enumerate(data_loader):\n",
    "        # print(f\"Input shape {input_batch.shape}, Target shape {target_batch.shape}\")\n",
    "        if i < num_batches:\n",
    "            loss = calc_batch_loss(\n",
    "                input_batch, target_batch, small_gpt_model, device\n",
    "            )\n",
    "            total_loss += loss.item()\n",
    "        else:\n",
    "            break\n",
    "            \n",
    "    return total_loss / num_batches\n",
    "\n",
    "# Similar to calculating the training accuracy, we now compute \n",
    "# the initial loss for each data set\n",
    "with torch.no_grad():\n",
    "    train_loss = calc_all_batch_loss(\n",
    "        train_loader, small_gpt_model, device, num_batches=5\n",
    "    )\n",
    "    val_loss = calc_all_batch_loss(val_loader, small_gpt_model, device, num_batches=5)\n",
    "    test_loss = calc_all_batch_loss(test_loader, small_gpt_model, device, num_batches=5)\n",
    "#\n",
    "print(f\"Training loss: {train_loss:.3f}\")\n",
    "print(f\"Validation loss: {val_loss:.3f}\")\n",
    "print(f\"Test loss: {test_loss:.3f}\")\n",
    "# "
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "63bdb536dc7a9dcb",
   "metadata": {},
   "source": [
    "# Now we implement a training function to fine-tune the model\n",
    "# which means adjusting the model to minimize the training set loss\n",
    "def train_classifier_simple(small_gpt_model, \n",
    "                            train_loader, \n",
    "                            val_loader, \n",
    "                            optimizer, \n",
    "                            device, \n",
    "                            num_epochs, \n",
    "                            eval_freq, \n",
    "                            eval_iter):\n",
    "    train_losses, val_losses, train_accs, val_accs = [], [], [], []\n",
    "    examples_seen, global_step = 0, -1\n",
    "    for epoch in range(num_epochs):\n",
    "        small_gpt_model.train()               # Sets model to training mode\n",
    "        for input_batch, target_batch in train_loader:\n",
    "            optimizer.zero_grad()       # reset the loss gradients\n",
    "            loss = calc_batch_loss(\n",
    "                input_batch, target_batch, small_gpt_model, device\n",
    "            )\n",
    "            loss.backward()     # calculate loss gradients\n",
    "            optimizer.step()    # backprop updates model weights\n",
    "            examples_seen += input_batch.shape[0] # Tracks examples instead of tokens\n",
    "            global_step += 1\n",
    "            # Optional Evaluation Step\n",
    "            if global_step % eval_freq == 0:\n",
    "                train_loss, val_loss = evaluate_model(\n",
    "                    small_gpt_model, train_loader, val_loader, device, eval_iter)\n",
    "                train_losses.append(train_loss)\n",
    "                val_losses.append(val_loss)\n",
    "                print(f\"Ep {epoch+1} (Step {global_step:06d}): \"\n",
    "                      f\"Train loss {train_loss:.3f}, \"\n",
    "                      f\"Val loss {val_loss:.3f}\"\n",
    "                )\n",
    "\n",
    "        train_accuracy = calc_accuracy_loader(\n",
    "            train_loader, small_gpt_model, device, num_batches=eval_iter\n",
    "        )\n",
    "        val_accuracy = calc_accuracy_loader(\n",
    "            val_loader, small_gpt_model, device, num_batches=eval_iter\n",
    "        )\n",
    "        print(f\"Training accuracy: {train_accuracy*100:.2f}% | \", end=\"\")\n",
    "        print(f\"Validation accuracy: {val_accuracy*100:.2f}%\")\n",
    "        train_accs.append(train_accuracy)\n",
    "        val_accs.append(val_accuracy)\n",
    "\n",
    "    return train_losses, val_losses, train_accs, val_accs, examples_seen\n",
    "\n",
    "\n",
    "def evaluate_model(small_gpt_model, train_loader, val_loader, device, eval_iter):\n",
    "    small_gpt_model.eval()\n",
    "    with torch.no_grad():\n",
    "        train_loss = calc_all_batch_loss(\n",
    "            train_loader, small_gpt_model, device, num_batches=eval_iter\n",
    "        )\n",
    "        val_loss = calc_all_batch_loss(\n",
    "            val_loader, small_gpt_model, device, num_batches=eval_iter\n",
    "        )\n",
    "    small_gpt_model.train()\n",
    "    return train_loss, val_loss\n",
    "\n",
    "# Now we initialize the optimizer \n",
    "import time\n",
    "#\n",
    "torch.manual_seed(123)\n",
    "start_time = time.time()\n",
    "optimizer = torch.optim.AdamW(small_gpt_model.parameters())\n",
    "num_epochs = 5\n",
    "examples_seen = 0\n",
    "training_done:bool = False\n",
    "train_losses, val_losses = [], []\n",
    "train_accs, val_accs = [float], [float]\n",
    "#\n",
    "if not os.path.exists(\"../models/review_classifier.pth\") and not training_done:\n",
    "    train_losses, val_losses, train_accs, val_accs, examples_seen = train_classifier_simple(\n",
    "        small_gpt_model,\n",
    "        train_loader,\n",
    "        val_loader,\n",
    "        optimizer,\n",
    "        device,\n",
    "        num_epochs=num_epochs,\n",
    "        eval_freq=50,\n",
    "        eval_iter=5\n",
    "    )\n",
    "    #\n",
    "    end_time = time.time()\n",
    "    execution_time_minutes = (end_time - start_time) / 60\n",
    "    print(f\"Training completed in {execution_time_minutes:.2f} minutes.\")\n",
    "    training_done = True\n",
    "else:\n",
    "    print(\"Classifier Model exists\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "2d461d605119d1de",
   "metadata": {},
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# Plot the loss function during training\n",
    "def plot_values(epochs_seen, examples_seen, train_values, val_values, label=\"loss\"):\n",
    "    fig, ax1 = plt.subplots(figsize=(5, 3))\n",
    "    ax1.plot(epochs_seen, train_values, label=f\"Training {label}\")\n",
    "    ax1.plot(\n",
    "        epochs_seen, val_values, linestyle=\"-.\",\n",
    "        label=f\"Validation {label}\"\n",
    "    )\n",
    "    ax1.set_xlabel(\"Epochs\")\n",
    "    ax1.set_ylabel(label.capitalize())\n",
    "    ax1.legend()\n",
    "\n",
    "    ax2 = ax1.twiny()\n",
    "    ax2.plot(examples_seen, train_values, alpha=0)\n",
    "    ax2.set_xlabel(\"Examples seen\")\n",
    "\n",
    "    fig.tight_layout()\n",
    "    plt.savefig(f\"{label}-plot.pdf\")\n",
    "    plt.show()\n",
    "\n",
    "if training_done:\n",
    "    epochs_tensor = torch.linspace(0, num_epochs, len(train_losses))\n",
    "    examples_seen_tensor = torch.linspace(0, examples_seen, len(train_losses))\n",
    "    plot_values(epochs_tensor, examples_seen_tensor, train_losses, val_losses)\n",
    "else:\n",
    "    print(\"Training already done. No need to plot again, moving on...\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "968fde4292b581cc",
   "metadata": {},
   "source": [
    "# Now lets plot the resulting accuracy\n",
    "if training_done:\n",
    "    epochs_tensor = torch.linspace(0, num_epochs, len(train_accs))\n",
    "    examples_seen_tensor = torch.linspace(0, examples_seen, len(train_accs))\n",
    "\n",
    "    plot_values(\n",
    "        epochs_tensor, examples_seen_tensor, train_accs, val_accs,\n",
    "        label=\"accuracy\"\n",
    "    )\n",
    "\n",
    "    # Now we must calculate performance metrics for the training\n",
    "    train_accuracy = calc_accuracy_loader(train_loader, small_gpt_model, device)\n",
    "    val_accuracy = calc_accuracy_loader(val_loader, small_gpt_model, device)\n",
    "    test_accuracy = calc_accuracy_loader(test_loader, small_gpt_model, device)\n",
    "\n",
    "    print(f\"Training accuracy: {train_accuracy*100:.2f} %\")\n",
    "    print(f\"Validation accuracy: {val_accuracy*100:.2f} %\")\n",
    "    print(f\"Test accuracy: {test_accuracy*100:.2f} %\")\n",
    "else:\n",
    "    print(\"Training already done. No need to calculate accuracy...\\n\")\n",
    "    pass"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "a3138646c2deb599",
   "metadata": {},
   "source": [
    "# Now let's classify spam messages\n",
    "def classify_review(text, small_gpt_model, tokenizer, device, max_length=None, pad_token_id=50256):\n",
    "    small_gpt_model.eval()\n",
    "    input_ids = tokenizer.encode(text)\n",
    "    supported_context_length = small_gpt_model.pos_emb.weight.shape[1]\n",
    "    input_ids = input_ids[:min(max_length, supported_context_length)]\n",
    "    input_ids += [pad_token_id] * (max_length - len(input_ids))\n",
    "    input_tensor = torch.tensor(input_ids, device=device).unsqueeze(0)\n",
    "    with torch.no_grad():\n",
    "        logits = small_gpt_model(input_tensor)[:, -1, :]\n",
    "    predicted_label = torch.argmax(logits, dim=-1).item()\n",
    "\n",
    "    return \"Spam\" if predicted_label == 1 else \"Not Spam\"\n",
    "\n",
    "text_1 = (\n",
    "    \"You are a winner you have been specially\"\n",
    "    \" selected to receive $1000 cash or a $2000 award.\"\n",
    ")\n",
    "#\n",
    "print(f\"{classify_review(\n",
    "    text_1, small_gpt_model, tokenizer, device, max_length=train_dataset.max_length\n",
    ")} : {text_1}\")\n",
    "\n",
    "text_2 = (\n",
    "    \"Hey, just wanted to check if we're still on\"\n",
    "    \" for dinner tonight? Let me know!\"\n",
    ")\n",
    "\n",
    "print(f\"{classify_review(\n",
    "    text_2, small_gpt_model, tokenizer, device, max_length=train_dataset.max_length\n",
    ")}: {text_2}\")\n",
    "# Now let's try classifying all the test dataset\n",
    "# df = test_dataset.data[\"Text\"]\n",
    "# i = 0\n",
    "# for text in df.values:\n",
    "#     i+=1\n",
    "#     print(f\"{i}: \"\n",
    "#           f\"{classify_review(text, small_gpt_model, tokenizer, device, max_length=test_dataset.max_length)} \"\n",
    "#           f\": {text}\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "99208e2db11a0271",
   "metadata": {},
   "source": [
    "# Save the model\n",
    "cls_model_path=\"../models/review_classifier.pth\"\n",
    "# cls_model_path=\"/Users/amlanchatterjee/Documents/ws/python/PycharmProjects/SimpleLLMProject/models/review_classifier.pth\"\n",
    "if not os.path.exists(cls_model_path):\n",
    "    torch.save(small_gpt_model.state_dict(), cls_model_path)\n",
    "    print(f\"Saved model at {cls_model_path}\")\n",
    "else:\n",
    "    print(f\"Model exists at {cls_model_path}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "117ef11dd9ff2827",
   "metadata": {},
   "source": [
    "# We can load this model again as well\n",
    "print(cls_model_path)\n",
    "model_loaded = False\n",
    "if os.path.exists(cls_model_path):\n",
    "    model_state_dict = torch.load(cls_model_path, map_location=device)\n",
    "    if not model_loaded:\n",
    "        small_gpt_model.load_state_dict(model_state_dict)\n",
    "        model_loaded = True\n",
    "        print(f\"Model successfully loaded from {cls_model_path} ...\")\n",
    "    else:\n",
    "        print(\"Model already loaded\")\n",
    "else:\n",
    "    print(f\"No model exists at {cls_model_path} can not load\\n\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "c9641da3f77713a3",
   "metadata": {},
   "source": [
    "# Fine-tuning to follow instructions  "
   ]
  },
  {
   "cell_type": "code",
   "id": "80b77746a0024957",
   "metadata": {},
   "source": [
    "from src.chapter07.GetInstructionDataset import download_and_load_file\n",
    "from src.chapter07.InstructionDataset import InstructionDataset\n",
    "# 1. Download the dataset (See chapter07/GetInstructionDataset.py)\n",
    "ins_filepath = \"../data/instruction-data.json\"\n",
    "insdata = download_and_load_file(ins_filepath)\n",
    "print(\"Example instruction: \\n\",insdata[50],\"\\n\")\n",
    "print(len(insdata[50])) # 3\n",
    "\n",
    "# 2. Implement the input prompt formatting function ALPACA style\n",
    "def format_input(entry):          # Accepts a dictionary entry\n",
    "    instruction_text = (\n",
    "        f\"Below is an instruction that describes a task. \"\n",
    "        f\"Write a response that appropriately completes the request.\"\n",
    "        f\"\\n\\n### Instruction:\\n{entry['instruction']}\"\n",
    "    )\n",
    "    input_text = (\n",
    "        f\"\\n\\n### Input:\\n{entry['input']}\" if entry[\"input\"] else \"\"\n",
    "    )\n",
    "    return instruction_text + input_text\n",
    "\n",
    "model_input = InstructionDataset.format_input(insdata[50])\n",
    "desired_response = f\"\\n\\n### Response:\\n{insdata[50]['output']}\"\n",
    "print(model_input + desired_response)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "3676e3e06f6cf1de",
   "metadata": {},
   "source": [
    "# Note we skip the ###Input if input is empty\n",
    "model_input = InstructionDataset.format_input(insdata[999])\n",
    "desired_response = f\"\\n\\n### Response:\\n{insdata[999]['output']}\"\n",
    "print(model_input + desired_response)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "58e45ae8d8348fd3",
   "metadata": {},
   "source": [
    "# Partitioning the dataset\n",
    "train_portion = int(len(insdata) * 0.80)\n",
    "test_portion = int(len(insdata) * 0.05)\n",
    "val_portion = len(insdata) - train_portion - test_portion\n",
    "\n",
    "ins_train_data = insdata[:train_portion]\n",
    "ins_test_data = insdata[train_portion:train_portion + test_portion]\n",
    "ins_val_data = insdata[train_portion + test_portion:]\n",
    "\n",
    "print(ins_val_data[0])\n",
    "print(\"Training set length:\", len(ins_train_data))\n",
    "print(\"Validation set length:\", len(ins_val_data))\n",
    "print(\"Test set length:\", len(ins_test_data))\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "7d86901c0c16c3ce",
   "metadata": {},
   "source": [
    "# Similar to the approach used for classification fine-tuning, we want to accelerate \n",
    "# training by collecting multiple training examples in a batch, which necessitates \n",
    "# padding all inputs to a similar length. As with classification fine-tuning, we \n",
    "# use the <|endoftext|> token as a padding token\n",
    "\n",
    "# Instead of appending the <|endoftext|> tokens to the text inputs, we can append \n",
    "# the Token ID corresponding to <|endoftext|> to the pretokenized inputs directly.\n",
    "\n",
    "import tiktoken\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "print(tokenizer.encode(\"<|endoftext|>\", allowed_special={\"<|endoftext|>\"}))\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "4bf32b1bbf58a8ee",
   "metadata": {},
   "source": [
    "# Now we develop a custom collate function that we can pass to the data loader.\n",
    "# This custom collate function pads the training examples in each batch to the\n",
    "# same length while allowing different batches to have different lengths\n",
    "# def custom_collate_draft_1(batch, pad_token_id=50256, device=\"cpu\"):\n",
    "#     batch_max_length = max(len(item)+1 for item in batch)\n",
    "#     inputs_list = []\n",
    "#     for item in batch:\n",
    "#         new_item = item.copy()\n",
    "#         new_item += [pad_token_id]\n",
    "#\n",
    "#         padded = (new_item + [pad_token_id] * (batch_max_length - len(new_item)))\n",
    "#         input_item = torch.tensor(padded[:-1])\n",
    "#         inputs_list.append(input_item)\n",
    "#\n",
    "#     inputs_tensor = torch.stack(inputs_list).to(device) # Stack the tensors across dim=0\n",
    "#     return inputs_tensor\n",
    "#\n",
    "inputs_1 = [0, 1, 2, 3, 4]\n",
    "inputs_2 = [5, 6]\n",
    "inputs_3 = [7, 8, 9]\n",
    "#\n",
    "batch = (inputs_1, inputs_2, inputs_3)\n",
    "print(\"Batch: \", batch)\n",
    "# Test for the cusrom_collate function\n",
    "# print(custom_collate_draft_1(batch))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "e46b93a48cd4f5a9",
   "metadata": {},
   "source": [
    "# Above output shows that all inputs have been padded to the length of\n",
    "# the longest input list i.e. inputs_1, containing five token IDs\n",
    "#\n",
    "# However, as we learned earlier, we also need to create batches\n",
    "# with target token IDs corresponding to each batch of input IDs\n",
    "# The target token IDs match the input token IDs but are shifted\n",
    "# one position to the right\n",
    "\n",
    "def custom_collate_draft_2(batch, pad_token_id=50256, device=\"cpu\"):\n",
    "    batch_max_length = max(len(item)+1 for item in batch)\n",
    "    inputs_list, targets_list = [], []\n",
    "\n",
    "    for item in batch:\n",
    "        new_item = item.copy()\n",
    "        new_item += [pad_token_id]\n",
    "\n",
    "        padded = (new_item + [pad_token_id] * (batch_max_length - len(new_item)))\n",
    "\n",
    "        inputs = torch.tensor(padded[:-1]) # Take the array except the last token\n",
    "        targets = torch.tensor(padded[1:]) # Shifted one position right from beginning to the last token\n",
    "        inputs_list.append(inputs)\n",
    "        targets_list.append(targets)\n",
    "\n",
    "    inputs_tensor = torch.stack(inputs_list).to(device)\n",
    "    targets_tensor = torch.stack(targets_list).to(device)\n",
    "    return inputs_tensor, targets_tensor\n",
    "\n",
    "inputs, targets = custom_collate_draft_2(batch)\n",
    "print(\"Inputs: \\n\", inputs)\n",
    "print(\"Targets: \\n\", targets)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "bbb6315c11a51b93",
   "metadata": {},
   "source": [
    "# Now we assign a -100 placeholder value to all padding tokens. This\n",
    "# special value allows us to exclude these padding tokens from contributing\n",
    "# to the training loss calculation, ensuring that only meaningful data\n",
    "# influences model learning.\n",
    "#\n",
    "# But we always retain at least one <|end-of-text|>\n",
    "# token to make sure the model knows where to generate the EOT token.\n",
    "# So the final collate function looks like this\n",
    "def custom_collate_fn(batch,\n",
    "                      pad_token_id=50256,\n",
    "                      ignore_index=-100,\n",
    "                      allowed_max_length=None,\n",
    "                      device=\"cpu\"):\n",
    "    batch_max_length = max(len(item)+1 for item in batch)\n",
    "    inputs_list, targets_list = [], []\n",
    "    for item in batch:\n",
    "        new_item = item.copy()\n",
    "        new_item += [pad_token_id]\n",
    "\n",
    "        padded = (new_item + [pad_token_id] * (batch_max_length - len(new_item)))\n",
    "\n",
    "        inputs = torch.tensor(padded[:-1])\n",
    "        targets = torch.tensor(padded[1:])\n",
    "\n",
    "        # Create a boolean mask for padding tokens like [False, False, False, False, False, True]\n",
    "        mask = (targets == pad_token_id)\n",
    "        # print(\"Mask: \\n\", mask)\n",
    "\n",
    "        # First we mark the non-zero tokens, then remove all dimensions of size 1\n",
    "        # so it returns the indices of the padding tokens in a list like format\n",
    "        # like tensor(4) or tensor([1, 2, 3, 4])\n",
    "        indices = torch.nonzero(mask).squeeze()\n",
    "        # print(\"Indices: \\n\", indices)\n",
    "\n",
    "        # If there are more than one padding tokens sequentially, keep first one\n",
    "        # and replace rest of the non-zero ones with ignore_index i.e. -100 on\n",
    "        # TARGETS TENSOR to remove from Loss calculation during training\n",
    "        if indices.numel() > 1:\n",
    "            targets[indices[1:]] = ignore_index # ignore_index is -100\n",
    "\n",
    "        # Optionally truncate to max sequence length\n",
    "        if allowed_max_length is not None:\n",
    "            inputs = inputs[:allowed_max_length]\n",
    "            targets = targets[:allowed_max_length]\n",
    "\n",
    "        inputs_list.append(inputs)\n",
    "        targets_list.append(targets)\n",
    "\n",
    "    inputs_tensor = torch.stack(inputs_list).to(device)\n",
    "    targets_tensor = torch.stack(targets_list).to(device)\n",
    "    return inputs_tensor, targets_tensor\n",
    "\n",
    "# Test custom collate function\n",
    "inputs, targets = custom_collate_fn(batch)\n",
    "\n",
    "print(\"Collated Inputs Tensor: \\n\", inputs)\n",
    "print(\"\\nCollated Targets Tensor: \\n\", targets)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "5d7c946885fce7b8",
   "metadata": {},
   "source": [
    "# For demonstration purposes, consider the following example\n",
    "# where each output logit corresponds to a potential token\n",
    "# from our model’s vocabulary. Here’s how we might calculate\n",
    "# the cross entropy loss during training\n",
    "logits_1 = torch.tensor(\n",
    "    [[-1.0, 1.0],   # Prediction for first token\n",
    "     [-0.5, 1.5]]   # Prediction for second token\n",
    ")\n",
    "targets_1 = torch.tensor([0, 1]) # Correct token indices to generate\n",
    "loss_1 = F.cross_entropy(logits_1, targets_1)\n",
    "print(\"Loss_1: \", loss_1) # tensor(1.1269e+00) or 0.11269 or 0.1127\n",
    "\n",
    "# We can now see if adding any extra tokens will affect\n",
    "# the loss calculation\n",
    "logits_2 = torch.tensor(\n",
    "    [[-1.0, 1.0],\n",
    "     [-0.5, 1.5],\n",
    "     [-0.5, 1.5]]\n",
    ")\n",
    "targets_2 = torch.tensor([0, 1, 1])\n",
    "loss_2 = F.cross_entropy(logits_2, targets_2)\n",
    "print(\"Loss_2: \", loss_2)\n",
    "#\n",
    "# Now the loss becomes tensor(7.9359e-01) or 0.79359 or 0.7936\n",
    "# NOTE: So far, we have carried out example calculations using the\n",
    "# cross-entropy loss function in PyTorch, the same loss function we used in\n",
    "# training functions for pre-training and fine-tuning for classification.\n",
    "# However, now if we replace the 3rd token with -100 the loss will be\n",
    "targets_3 = torch.tensor([0, 1, -100])\n",
    "loss_3 = F.cross_entropy(logits_2, targets_3)\n",
    "print(\"Loss_3: \", loss_3)\n",
    "print(\"\\nloss_1 == loss_3:\", loss_1 == loss_3)\n",
    "\n",
    "# NOTE: This is specific to pytorch where the default setting of the cross entropy\n",
    "# function in PyTorch is cross_entropy(..., ignore_index=-100)\n",
    "# which means that the loss function ignores the padding tokens if they are set to -100\n",
    "# However, we would like to keep one 50256 (end-of-text) token ID in the targets tensor\n",
    "# because it helps the LLM learn where to generate the end-of-text tokens"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "d5e44c030b5c3c9d",
   "metadata": {},
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from functools import partial\n",
    "\n",
    "# In addition to masking out padding tokens, it is also common to\n",
    "# mask out the TARGET TOKEN IDs that correspond to the instructions.\n",
    "#\n",
    "# By masking out the LLM’s TARGET TOKEN IDs corresponding to the instruction,\n",
    "# the cross entropy loss is only computed for the generated response target IDs.\n",
    "# Thus, the model is trained to focus on generating accurate responses rather\n",
    "# than memorizing instructions, which could help reduce overfitting.\n",
    "# We can try and use the same -100 mask to mask out the TARGET TOKEN IDs\n",
    "#\n",
    "# Next, to reuse the chosen device setting in custom_collate_fn when we plug\n",
    "# it into the PyTorch DataLoader class, we use the partial function from Python’s\n",
    "# functools standard library to create a new version of the function with the device prefilled.\n",
    "\n",
    "customized_collate_fn = partial(\n",
    "    custom_collate_fn,\n",
    "    device=device,              # Set the device to GPU i.e. \"cuda:0\" or \"mps:0\" or \"cpu\"\n",
    "    allowed_max_length=1024     # Truncate to 1024 tokens [default]\n",
    ")\n",
    "#\n",
    "# Now we can use the \"customized_collate_fn\" in the DataLoader\n",
    "#\n",
    "num_workers = 0\n",
    "batch_size = 8\n",
    "#\n",
    "torch.manual_seed(123)\n",
    "#\n",
    "ins_train_dataset = InstructionDataset(ins_train_data, tokenizer)\n",
    "ins_train_loader = DataLoader(\n",
    "    ins_train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    collate_fn=customized_collate_fn,\n",
    "    shuffle=True,\n",
    "    drop_last=True,\n",
    "    num_workers=num_workers\n",
    ")\n",
    "#\n",
    "ins_val_dataset = InstructionDataset(ins_val_data, tokenizer)\n",
    "ins_val_loader = DataLoader(\n",
    "    ins_val_dataset,\n",
    "    batch_size=batch_size,\n",
    "    collate_fn=customized_collate_fn,\n",
    "    shuffle=False,\n",
    "    drop_last=False,\n",
    "    num_workers=num_workers\n",
    ")\n",
    "#\n",
    "ins_test_dataset = InstructionDataset(ins_test_data, tokenizer)\n",
    "ins_test_loader = DataLoader(\n",
    "    ins_test_dataset,\n",
    "    batch_size=batch_size,\n",
    "    collate_fn=customized_collate_fn,\n",
    "    shuffle=False,\n",
    "    drop_last=False,\n",
    "    num_workers=num_workers\n",
    ")\n",
    "#\n",
    "# print(\"Example from Train loader: \\n\")\n",
    "# i: int = 1\n",
    "# for inputs, targets in train_loader:\n",
    "#     i += 1\n",
    "#     print(f\"[{i}] Inputs Shape: {inputs.shape}, Targets Shape: {targets.shape}\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Executing this code will initiate the download of the medium-sized GPT model,\n",
    "# which has a storage requirement of approximately 1.42 gigabytes\n",
    "# from chapter05.gpt_download import download_and_load_gpt2\n",
    "\n",
    "BASE_CONFIG = {\n",
    "    \"vocab_size\": 50257,     # Vocabulary size\n",
    "    \"context_length\": 1024,  # Context length\n",
    "    \"drop_rate\": 0.0,        # Dropout rate\n",
    "    \"qkv_bias\": True,         # Query-key-value bias\n",
    "    \"lr\": 5e-4,\n",
    "    \"weight_decay\": 0.1\n",
    "}\n",
    "\n",
    "model_configs = {\n",
    "    \"gpt2-small (124M)\": {\"emb_dim\": 768, \"n_layers\": 12, \"n_heads\": 12},\n",
    "    \"gpt2-medium (355M)\": {\"emb_dim\": 1024, \"n_layers\": 24, \"n_heads\": 16},\n",
    "    \"gpt2-large (774M)\": {\"emb_dim\": 1280, \"n_layers\": 36, \"n_heads\": 20},\n",
    "    \"gpt2-xl (1558M)\": {\"emb_dim\": 1600, \"n_layers\": 48, \"n_heads\": 25},\n",
    "}\n",
    "\n",
    "CHOOSE_MODEL = \"gpt2-medium (355M)\"\n",
    "BASE_CONFIG.update(model_configs[CHOOSE_MODEL])\n",
    "BASE_CONFIG.update({\"model_name\" : CHOOSE_MODEL.split(\" \")[0]})\n",
    "\n",
    "medium_model_size = CHOOSE_MODEL.split(\" \")[-1].lstrip(\"(\").rstrip(\")\")\n",
    "settings, params = download_and_load_gpt2(\n",
    "    model_size=medium_model_size,\n",
    "    models_dir=\"../data/gpt2\"\n",
    ")\n",
    "\n",
    "instr_model = GPTModel(BASE_CONFIG)\n",
    "load_weights_into_gpt(instr_model, params)\n",
    "instr_model.eval();\n"
   ],
   "id": "e8ba03bdc57e1be4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from src.chapter07.InstructionDataset import InstructionDataset\n",
    "\n",
    "# Next we generate the model’s response using the same generate function\n",
    "torch.manual_seed(123)\n",
    "input_text = InstructionDataset.format_input(ins_val_data[0])\n",
    "print(\"Formatted Input: \\n\", input_text, \"\\n==================\\n\")\n",
    "\n",
    "#\n",
    "token_ids = generate(\n",
    "    model=instr_model,\n",
    "    idx=text_to_token_ids(input_text, tokenizer),\n",
    "    max_new_tokens=35,\n",
    "    context_size=BASE_CONFIG[\"context_length\"],\n",
    "    eos_id=50256\n",
    ")\n",
    "generated_text = token_ids_to_text(token_ids, tokenizer)\n",
    "response_text = generated_text[len(input_text):].strip()\n",
    "print(\"Generated Response Text: \\n\", response_text)\n"
   ],
   "id": "a8bd1ccfd91f7479",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#  Fine-tuning the LLM on instruction data",
   "id": "25d1c5dc245bdb6e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "torch.manual_seed(123)\n",
    "\n",
    "instr_model.to(device)\n",
    "with torch.no_grad():\n",
    "    ins_train_loss = calculate_loss_loader(ins_train_loader, instr_model, device, num_batches=5)\n",
    "    ins_val_loss = calculate_loss_loader(ins_val_loader, instr_model, device, num_batches=5)\n",
    "\n",
    "print(\"Training loss:\", ins_train_loss)\n",
    "print(\"Validation loss:\", ins_val_loss)"
   ],
   "id": "cb213296699f454",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import time\n",
    "#\n",
    "start_time = time.time()\n",
    "torch.manual_seed(123)\n",
    "ins_optimizer = torch.optim.AdamW(\n",
    "    instr_model.parameters(),\n",
    "    lr=0.00005,\n",
    "    weight_decay=0.1\n",
    ")\n",
    "num_epochs = 2\n",
    "ins_training_done:bool = False\n",
    "#\n",
    "# Train the model now\n",
    "#\n",
    "if not ins_training_done:\n",
    "    ins_train_losses, ins_val_losses, ins_tokens_seen = train_model_simple(\n",
    "        instr_model,\n",
    "        ins_train_loader,\n",
    "        ins_val_loader,\n",
    "        ins_optimizer,\n",
    "        device,\n",
    "        num_epochs=num_epochs,\n",
    "        eval_freq=5,\n",
    "        eval_iter=5,\n",
    "        start_context=InstructionDataset.format_input(val_data[0]),\n",
    "        tokenizer=tokenizer\n",
    "    )\n",
    "    end_time = time.time()\n",
    "    execution_time_minutes = (end_time - start_time) / 60\n",
    "    print(f\"Training completed in {execution_time_minutes:.2f} minutes.\")\n",
    "    ins_training_done = True\n",
    "    # Plot the losses\n",
    "    epochs_tensor = torch.linspace(0, num_epochs, len(ins_train_losses))\n",
    "    plot_losses(epochs_tensor, ins_tokens_seen, ins_train_losses, ins_val_losses)\n",
    "else:\n",
    "    print(f\"Training has already been completed for {num_epochs} epochs.\")"
   ],
   "id": "ea8355d274d2912e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "torch.manual_seed(123)\n",
    "\n",
    "for entry in ins_test_data[:3]:\n",
    "    input_text = InstructionDataset.format_input(entry)\n",
    "    token_ids = generate(\n",
    "        model=instr_model,\n",
    "        idx=text_to_token_ids(input_text, tokenizer).to(device),\n",
    "        max_new_tokens=256,\n",
    "        context_size=BASE_CONFIG[\"context_length\"],\n",
    "        eos_id=50256\n",
    "    )\n",
    "    generated_text = token_ids_to_text(token_ids, tokenizer)\n",
    "    model_response_text = generated_text[len(input_text):].replace(\"### Response:\", \"\").strip()\n",
    "\n",
    "    print(\"Input Text: \", input_text)\n",
    "    print(f\"\\nCorrect response:\\n>> {entry['output']}\")\n",
    "    print(f\"\\nModel response:\\n>> {model_response_text.strip()}\")\n",
    "\n",
    "    print(\"-------------------------------------\")"
   ],
   "id": "3230573ca955c154",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import json\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Generating test set responses\n",
    "for i, entry in tqdm(enumerate(ins_test_data), total=len(ins_test_data)):\n",
    "    input_text = InstructionDataset.format_input(entry)\n",
    "\n",
    "    token_ids = generate(\n",
    "        model=instr_model,\n",
    "        idx=text_to_token_ids(input_text, tokenizer).to(device),\n",
    "        max_new_tokens=256,\n",
    "        context_size=BASE_CONFIG[\"context_length\"],\n",
    "        eos_id=50256\n",
    "    )\n",
    "    generated_text = token_ids_to_text(token_ids, tokenizer)\n",
    "\n",
    "    response_text = (\n",
    "        generated_text[len(input_text):]\n",
    "        .replace(\"### Response:\", \"\")\n",
    "        .strip()\n",
    "    )\n",
    "    ins_test_data[i][\"model_response\"] = response_text\n",
    "\n",
    "with open(\"../data/instruction-data-with-response.json\", \"w\") as file:\n",
    "    json.dump(ins_test_data, file, indent=4)\n",
    "    file.close()\n",
    "\n",
    "print(\"Instruction test data [0]: \\n\", ins_test_data[0])\n",
    "\n",
    "# Save the model\n",
    "file_name = f\"../models/{re.sub(r'[ ()]', '', CHOOSE_MODEL) }-sft.pth\"\n",
    "if not os.path.exists(file_name):\n",
    "    torch.save(model.state_dict(), file_name)\n",
    "    print(f\"Model saved as {file_name}\")\n",
    "else:\n",
    "    print(f\"Model exists at {file_name}\")"
   ],
   "id": "2b9190d81f029c7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "88285654099ba3c5",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
