{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# This is an attempt to learn by building and training an LLM from Scratch  ",
   "id": "8a77807f92f26ee"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-17T20:48:05.187325Z",
     "start_time": "2025-01-17T20:48:05.183890Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import torch\n",
    "import urllib.request\n",
    "\n",
    "url = \"https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/ch02/01_main-chapter-code/the-verdict.txt\" # Download a text (book)\n",
    "file_path = \"../data/the-verdict.txt\"\n",
    "if not os.path.exists(file_path):\n",
    "    urllib.request.urlretrieve(url, file_path)\n",
    "    \n",
    "if torch.backends.mps.is_available():\n",
    "    mps_device = torch.device('mps')\n",
    "    # x = torch.ones(1, device=mps_device)\n",
    "    # print(x)\n",
    "else:\n",
    "    print(\"mps not available\")\n",
    "    \n",
    "    \n",
    "# Now lets load the text data\n",
    "with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    raw_text = f.read() \n",
    "\n",
    "print(\"Total characters in the story: \", len(raw_text))\n",
    "print(\"Total Lines in raw text: \", raw_text.count(\"\\n\"))\n",
    "print(raw_text[:99])\n"
   ],
   "id": "fbc121e30a2defb3",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total characters in the story:  20479\n",
      "Total Lines in raw text:  164\n",
      "I HAD always thought Jack Gisburn rather a cheap genius--though a good fellow enough--so it was no \n"
     ]
    }
   ],
   "execution_count": 107
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-17T20:48:05.272930Z",
     "start_time": "2025-01-17T20:48:05.269659Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Now we have to tokenize the text. The best way to do that is to use a pre-build tokennizer, but first we will try some \n",
    "# basic python regular expressions to do the same things\n",
    "import re\n",
    "preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', raw_text)\n",
    "preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
    "print(len(preprocessed))\n",
    "print(preprocessed[:30])"
   ],
   "id": "2c7227e79afbcad7",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4690\n",
      "['I', 'HAD', 'always', 'thought', 'Jack', 'Gisburn', 'rather', 'a', 'cheap', 'genius', '--', 'though', 'a', 'good', 'fellow', 'enough', '--', 'so', 'it', 'was', 'no', 'great', 'surprise', 'to', 'me', 'to', 'hear', 'that', ',', 'in']\n"
     ]
    }
   ],
   "execution_count": 108
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-17T20:48:05.328373Z",
     "start_time": "2025-01-17T20:48:05.325585Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Now we need to generate token IDs\n",
    "# Now let us create a list of all unique tokens and sort them alphabetically to determine the vocabulary size\n",
    "all_uniq_words = sorted(set(preprocessed))\n",
    "vocab_size = len(all_uniq_words)\n",
    "print(\"Vocab size: \", vocab_size)\n",
    "\n",
    "# Now that we know the vocabulary size, lets enumerate and assign some numbers to them\n",
    "vocab = {token:integer for integer,token in enumerate(all_uniq_words)}\n",
    "for i, item in enumerate(vocab.items()):\n",
    "    print(item)\n",
    "    if i >= 20:\n",
    "        break\n"
   ],
   "id": "b9dc98b584bc6d61",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size:  1130\n",
      "('!', 0)\n",
      "('\"', 1)\n",
      "(\"'\", 2)\n",
      "('(', 3)\n",
      "(')', 4)\n",
      "(',', 5)\n",
      "('--', 6)\n",
      "('.', 7)\n",
      "(':', 8)\n",
      "(';', 9)\n",
      "('?', 10)\n",
      "('A', 11)\n",
      "('Ah', 12)\n",
      "('Among', 13)\n",
      "('And', 14)\n",
      "('Are', 15)\n",
      "('Arrt', 16)\n",
      "('As', 17)\n",
      "('At', 18)\n",
      "('Be', 19)\n",
      "('Begin', 20)\n"
     ]
    }
   ],
   "execution_count": 109
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-17T20:48:05.359023Z",
     "start_time": "2025-01-17T20:48:05.356922Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from src.SimpleTokenizerV1 import SimpleTokenizerV1\n",
    "\n",
    "# Now we want to apply this vocabulary to convert new text to generate token id\n",
    "# When we want to convert the outputs of an LLM from numbers back into text, we need a way to turn token IDs into text. \n",
    "# For this, we can create an inverse version of the vocabulary that maps token IDs back to the corresponding text tokens.\n",
    "\n",
    "tokenizer = SimpleTokenizerV1(vocab)\n",
    "text = \"\"\"\"It's the last he painted you know,\" \n",
    "        Mrs Gisburn said with pardonable pride.\"\"\"\n",
    "\n",
    "ids = tokenizer.encode(text)\n",
    "print(ids)\n",
    "print(tokenizer.decode(ids))"
   ],
   "id": "b49e9060996c9f10",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 56, 2, 850, 988, 602, 533, 746, 1126, 596, 5, 1, 67, 38, 851, 1108, 754, 793, 7]\n",
      "\" It' s the last he painted you know,\" Mrs Gisburn said with pardonable pride.\n"
     ]
    }
   ],
   "execution_count": 110
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-17T20:48:05.373369Z",
     "start_time": "2025-01-17T20:48:05.371009Z"
    }
   },
   "cell_type": "code",
   "source": [
    "all_tokens = sorted(list(set(preprocessed))) # Make preprocessed a list so we can extend it\n",
    "all_tokens.extend([\"<|unk|>\", \"<|endoftext|>\"])\n",
    "# redo the vocab population\n",
    "vocab = {token:integer for integer, token in enumerate(all_tokens)}\n",
    "print(len(vocab.items()))\n"
   ],
   "id": "92b6519fe1741db8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1132\n"
     ]
    }
   ],
   "execution_count": 111
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-17T20:48:05.387230Z",
     "start_time": "2025-01-17T20:48:05.385144Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Print the last 5 vocab items\n",
    "for i, item in enumerate(list(vocab.items())[-5:]):\n",
    "    print(item)"
   ],
   "id": "d03ae607c2d20268",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('younger', 1127)\n",
      "('your', 1128)\n",
      "('yourself', 1129)\n",
      "('<|unk|>', 1130)\n",
      "('<|endoftext|>', 1131)\n"
     ]
    }
   ],
   "execution_count": 112
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-17T20:48:05.415248Z",
     "start_time": "2025-01-17T20:48:05.412747Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from src.SimpleTokenizerV2 import SimpleTokenizerV2\n",
    "\n",
    "text1 = \"Hello, do you like tea?\"\n",
    "text2 = \"In the sunlit terraces of the palace.\"\n",
    "text = \" <|endoftext|> \".join((text1, text2))\n",
    "print(text)\n",
    "tokenizer = SimpleTokenizerV2(vocab)\n",
    "print(tokenizer.encode(text))\n",
    "print(tokenizer.decode(tokenizer.encode(text)))"
   ],
   "id": "dcdf224cd056d26f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, do you like tea? <|endoftext|> In the sunlit terraces of the palace.\n",
      "[1130, 5, 355, 1126, 628, 975, 10, 1131, 55, 988, 956, 984, 722, 988, 1130, 7]\n",
      "<|unk|>, do you like tea? <|endoftext|> In the sunlit terraces of the <|unk|>.\n"
     ]
    }
   ],
   "execution_count": 113
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-17T20:48:05.429416Z",
     "start_time": "2025-01-17T20:48:05.426356Z"
    }
   },
   "cell_type": "code",
   "source": [
    "### Byte Pair Encoding \n",
    "from importlib.metadata import version\n",
    "import tiktoken\n",
    "print(\"Tiktoken version: \", version(\"tiktoken\"))\n",
    "#print(\"Tiktoken version: \", tiktoken.__version__)"
   ],
   "id": "e4e4738351ee6b9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tiktoken version:  0.8.0\n"
     ]
    }
   ],
   "execution_count": 114
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-17T20:48:05.441816Z",
     "start_time": "2025-01-17T20:48:05.439706Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#### This is the tokenizer using the GPT2 tokenization model\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "text = \"Hello, do you like tea? <|endoftext|> In the sunlit terraces of someunknownPlace.\"\n",
    "integers = tokenizer.encode(text, allowed_special={\"<|endoftext|>\"})\n",
    "print(integers)\n",
    "strings = tokenizer.decode(integers)\n",
    "print(strings)"
   ],
   "id": "ac57b0bafdc676f4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15496, 11, 466, 345, 588, 8887, 30, 220, 50256, 554, 262, 4252, 18250, 8812, 2114, 286, 617, 34680, 27271, 13]\n",
      "Hello, do you like tea? <|endoftext|> In the sunlit terraces of someunknownPlace.\n"
     ]
    }
   ],
   "execution_count": 115
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-17T20:48:05.465666Z",
     "start_time": "2025-01-17T20:48:05.463436Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(tokenizer.encode(\"Akwirw ier\"))\n",
    "print(tokenizer.decode(tokenizer.encode(\"Akwirw ier\")))"
   ],
   "id": "2a6a5225d4fab946",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[33901, 86, 343, 86, 220, 959]\n",
      "Akwirw ier\n"
     ]
    }
   ],
   "execution_count": 116
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-17T20:48:05.480764Z",
     "start_time": "2025-01-17T20:48:05.476212Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Let's now do Data Sampling with a sliding window\n",
    "# 1. Let's tokenize the entire story with BPE tokenizer first\n",
    "with (open(file_path, \"r\", encoding=\"utf-8\")) as f:\n",
    "    raw_text = f.read()\n",
    "    \n",
    "encoded_text = tokenizer.encode(raw_text)\n",
    "print(len(encoded_text))\n",
    "enc_sample = encoded_text[50:]\n",
    "\n",
    "# Now Let's start by defining x and y where x has input tokens and y the output tokens shifted by 1\n",
    "context_size = 4\n",
    "x = enc_sample[:context_size]\n",
    "y = enc_sample[1:context_size+1]\n",
    "# print(f\"x: {x}\")\n",
    "# print(f\"y:      {y}\")\n",
    "\n",
    "\n",
    "#####\n",
    "# Next word prediction tasks can now be created by \n",
    "for i in range(1, context_size + 1):\n",
    "    context = enc_sample[:i]\n",
    "    desired = enc_sample[i]\n",
    "    # print(f\"context input: {context} --> desired prediction: {desired}\")\n",
    "    # Now we create the input output target pairs\n",
    "    print(tokenizer.decode(context), \"---->\", tokenizer.decode([desired]))"
   ],
   "id": "1ab2edd9fb1ca03",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5145\n",
      " and ---->  established\n",
      " and established ---->  himself\n",
      " and established himself ---->  in\n",
      " and established himself in ---->  a\n"
     ]
    }
   ],
   "execution_count": 117
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-17T20:48:05.498279Z",
     "start_time": "2025-01-17T20:48:05.496564Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# from Dataloader import Dataloader\n",
    "# \n",
    "# dataloader = Dataloader(batch_size=8, max_length=4, stride=4, shuffle=False, drop_last=True, num_workers=0)\n",
    "# dataloader = dataloader.get_instance(file_path, text_enc='utf-8', mode='r')\n",
    "# if dataloader is not None:\n",
    "#     data_iter = iter(dataloader)\n",
    "#     inputs, targets = next(data_iter)\n",
    "#     print(\"Loaded text data...\\n\")\n",
    "#     print(\"Inputs: \\n\", inputs)\n",
    "#     print(\"\\nTargets: \\n\", targets)\n",
    "# else: \n",
    "#     print(\"Failed loading \", dataloader)\n"
   ],
   "id": "62094a4513e01008",
   "outputs": [],
   "execution_count": 118
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-17T20:48:05.586616Z",
     "start_time": "2025-01-17T20:48:05.518072Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch.nn\n",
    "from src.Dataloader import Dataloader\n",
    "file_path = \"data/the-verdict.txt\"\n",
    "\n",
    "####\n",
    "# Finally we need to create the embeddings for the tokens\n",
    "# If we have a batch size of 8 with 4 tokens each it'll be an 8 x 4 x 256 tensor\n",
    "max_length = 4\n",
    "\n",
    "with open(\"data/the-verdict.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    raw_text = f.read()\n",
    "    \n",
    "\n",
    "mydataloader = Dataloader(batch_size=8, max_length=max_length, stride=4, shuffle=False, drop_last=True, num_workers=0)\n",
    "dataloader = mydataloader.create_dataloader_v1(txt=raw_text)\n",
    "data_iter = iter(dataloader)\n",
    "inputs, targets = next(data_iter)\n",
    "# print(\"Input Token IDs:\\n\", inputs)\n",
    "# print(\"Input tensor shape: \", inputs.shape) \n",
    "\n",
    "# Now since self-attentions are position agnostic, we should add some positional data.\n",
    "# Absolute and relative positional data can be added. So let's create embeddings with say 256 dimensions\n",
    "vocab_size = 50257\n",
    "output_dim = 256\n",
    "# context_length = 1024\n",
    "\n",
    "## Now lets embed the input tensors\n",
    "token_embedding_layer = torch.nn.Embedding(vocab_size, output_dim)\n",
    "token_embeddings = token_embedding_layer(inputs)\n",
    "print(\"Token embeddings shape: \", token_embeddings.shape) #8x4x256\n",
    "\n",
    "\n",
    "# For a GPT model’s absolute position embedding approach, we just need to create another embedding \n",
    "# layer that has the same embedding dimension as the token_embedding_ layer:\n",
    "context_length = max_length     #context is length of positions we care about for attention\n",
    "pos_embedding_layer = torch.nn.Embedding(context_length, output_dim)\n",
    "pos_embeddings = pos_embedding_layer(torch.arange(context_length))\n",
    "print(\"Positional Embeddings Shape: \", pos_embeddings.shape) # 4x256\n",
    "#\n",
    "# Add the positional embeddings to token embeddings\n",
    "input_embeddings = token_embeddings + pos_embeddings\n",
    "print(\"Position Merged Input Embeddings Shape: \", input_embeddings.shape)\n",
    "#\n",
    "# Now lets look at the dataloader\n",
    "for batch in dataloader:\n",
    "    inputs, targets = batch\n",
    "    token_embeddings = token_embedding_layer(inputs)\n",
    "    pos_embeddings = pos_embedding_layer(torch.arange(max_length))\n",
    "    input_embeddings = token_embeddings + pos_embeddings\n",
    "    break\n",
    "#\n",
    "print(\"Batch Embeddings Shape: \", input_embeddings.shape)\n",
    "    \n",
    "print(\"Input tensor \", x)\n",
    "print(\"Target tensor\", y)"
   ],
   "id": "fa0c4b5f389cdc4e",
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data/the-verdict.txt'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mFileNotFoundError\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[119], line 10\u001B[0m\n\u001B[1;32m      5\u001B[0m \u001B[38;5;66;03m####\u001B[39;00m\n\u001B[1;32m      6\u001B[0m \u001B[38;5;66;03m# Finally we need to create the embeddings for the tokens\u001B[39;00m\n\u001B[1;32m      7\u001B[0m \u001B[38;5;66;03m# If we have a batch size of 8 with 4 tokens each it'll be an 8 x 4 x 256 tensor\u001B[39;00m\n\u001B[1;32m      8\u001B[0m max_length \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m4\u001B[39m\n\u001B[0;32m---> 10\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28;43mopen\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mdata/the-verdict.txt\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mr\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mencoding\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mutf-8\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m \u001B[38;5;28;01mas\u001B[39;00m f:\n\u001B[1;32m     11\u001B[0m     raw_text \u001B[38;5;241m=\u001B[39m f\u001B[38;5;241m.\u001B[39mread()\n\u001B[1;32m     14\u001B[0m mydataloader \u001B[38;5;241m=\u001B[39m Dataloader(batch_size\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m8\u001B[39m, max_length\u001B[38;5;241m=\u001B[39mmax_length, stride\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m4\u001B[39m, shuffle\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m, drop_last\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m, num_workers\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0\u001B[39m)\n",
      "File \u001B[0;32m~/Documents/ws/python/PycharmProjects/SimpleLLMProject/.venv/lib/python3.12/site-packages/IPython/core/interactiveshell.py:324\u001B[0m, in \u001B[0;36m_modified_open\u001B[0;34m(file, *args, **kwargs)\u001B[0m\n\u001B[1;32m    317\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m file \u001B[38;5;129;01min\u001B[39;00m {\u001B[38;5;241m0\u001B[39m, \u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m2\u001B[39m}:\n\u001B[1;32m    318\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[1;32m    319\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mIPython won\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mt let you open fd=\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mfile\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m by default \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    320\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    321\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124myou can use builtins\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m open.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    322\u001B[0m     )\n\u001B[0;32m--> 324\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mio_open\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfile\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[0;31mFileNotFoundError\u001B[0m: [Errno 2] No such file or directory: 'data/the-verdict.txt'"
     ]
    }
   ],
   "execution_count": 119
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Chapter 3 - Attention\n",
    "#\n",
    "import torch\n",
    "\n",
    "# In self-attention our goal is to calculate context vector z(i) for each \n",
    "# element x(i) of the input sequence. Consider the following input sequence \n",
    "#\n",
    "inputs = torch.tensor(\n",
    "  [[0.43, 0.15, 0.89], # Your     (x^1)\n",
    "   [0.55, 0.87, 0.66], # journey  (x^2)\n",
    "   [0.57, 0.85, 0.64], # starts   (x^3)\n",
    "   [0.22, 0.58, 0.33], # with     (x^4)\n",
    "   [0.77, 0.25, 0.10], # one      (x^5)\n",
    "   [0.05, 0.80, 0.55]] # step     (x^6)\n",
    ")\n",
    "print(\"Input sequence shape: \", inputs.shape)\n",
    "# \n",
    "# Now calculate weights for attention\n",
    "# Assume query is the second word \"journey\" or inputs[1] \n",
    "#\n",
    "query = inputs[1]\n",
    "print(f\"Query is the 2nd word 'journey': {query}\")\n",
    "#\n",
    "attention_scores_2 = torch.empty(inputs.shape[0])\n",
    "for idx, x_i in enumerate(inputs):\n",
    "    attention_scores_2[idx] = torch.dot(x_i, query)\n",
    "#    print(f\"Sequence Element [{idx}], attention_score: {attention_scores_2}\")\n",
    "print(f\"Final value of attention_score_2: {attention_scores_2}\")\n"
   ],
   "id": "95c2c3a6eb3eea50",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "## Note: For all elements if we were to calculate attention it'd be a O(n^2) operation\n",
    "# NOW we normalize the attention weights, so they sum up to 1\n",
    "attention_weights_2_tmp = attention_scores_2 / attention_scores_2.sum()\n",
    "print(\"Normalized attention weights:\", attention_weights_2_tmp)\n",
    "print(\"Sum of attention weights:\", attention_weights_2_tmp.sum())\n",
    "\n",
    "## Generally we normalize using the softmax to do the normalization\n",
    "\n",
    "# # define a softmax function\n",
    "def softmax_naive(tensor_x):\n",
    "    return torch.exp(tensor_x) / torch.exp(tensor_x).sum(dim=0, keepdim=True)\n",
    "# \n",
    "attention_scores_2_naive = softmax_naive(attention_scores_2)\n",
    "print(\"Attention weights naive:\", attention_scores_2_naive)\n",
    "print (\"Naive Sum: \", attention_scores_2_naive.sum())\n",
    "# \n",
    "# Generally we normalize using the torch.softmax() to do the normalization\n",
    "# Softmax ensures its always positive and always adds up to 1\n",
    "#\n",
    "attention_weights_2_torch_softmax = torch.softmax(attention_scores_2, dim=0)\n",
    "print(\"Attention weights torch softmax:\", attention_weights_2_torch_softmax)\n",
    "#print(\"Attention weights torch softmax Sum: \", attention_weights_2_torch_softmax.sum())\n",
    "\n",
    "# Now that we have calculated the normalized attention weights, we are ready for the final step.\n",
    "# Calculate the context vector z(2) by multiplying the embedded input tokens x(i), \n",
    "# with the corresponding normalized attention weights and then summing the resultant vectors\n",
    "#\n",
    "query = inputs[1]\n",
    "context_vec_2 = torch.zeros(query.shape)\n",
    "#\n",
    "for i, x_i in enumerate(inputs):\n",
    "     context_vec_2 += (attention_weights_2_torch_softmax[i] * x_i)\n",
    "print(\"Context vector z2: \", context_vec_2)\n",
    "#\n",
    "# Now in similar fashion lets calculate attention scores for all the input sequences \n",
    "attention_scores = torch.empty(inputs.shape[0],inputs.shape[0])\n",
    "print(\"Attention Scores matrix shape: \", attention_scores.shape)\n",
    "#\n",
    "# Using for loops\n",
    "#\n",
    "# for i, x_i in enumerate(inputs):\n",
    "#     for j, x_j in enumerate(inputs):\n",
    "#         attention_scores[i, j] = torch.dot(x_i, x_j)\n",
    "# #\n",
    "#print(attention_scores)\n",
    "#\n",
    "# Using matrix multiplication we can do it faster\n",
    "#\n",
    "attention_scores_m = inputs @ inputs.T\n",
    "#print(\"Normalized attention scores \\n\", attention_scores_m)\n",
    "\n",
    "# Just as before lets normalize the rows, so they sum up to 1\n",
    "# NOTE: Here dim = -1 means we are applying the softmax along the last dimension of the attention_scores_m tensor\n",
    "#\n",
    "attention_weights = torch.softmax(attention_scores_m, dim=-1)\n",
    "#print(\"Normalized ATTENTION weights \\n\", attention_weights)\n",
    "# print(\"Softmax Sums:\\n\", attention_weights.sum(dim=-1))\n",
    "\n",
    "# FINAL STEP\n",
    "# Now let's calculate the context vectors for all the input by multiplying the input with attention weights\n",
    "all_context_vectors = attention_weights @ inputs # Matrix multiplication\n",
    "#print(\"Context vector for the entire sequence\\n\", all_context_vectors)"
   ],
   "id": "53ede1deb7d5678e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "###\n",
    "### 3.4.1 Using weighted matrix\n",
    "###\n",
    "#\n",
    "# Computing the attention weights step by step\n",
    "#\n",
    "x_2 = inputs[1]\n",
    "d_in = inputs.shape[1]\n",
    "d_out = 2\n",
    "print(\"x_2: \", x_2)\n",
    "# Now let's initialize 3 weighted matrices Wq, Wk and Wv\n",
    "# Setting requires_grad = False, to reduce clutter, but for model training this should be set to True\n",
    "torch.manual_seed(123)\n",
    "W_query = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)\n",
    "W_key   = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)\n",
    "W_value = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)\n",
    "#\n",
    "# Next we compute the query, key and value vectors\n",
    "# Note the output is a 2 dimenstional vector because we set dout to 2\n",
    "#\n",
    "query_2 = x_2 @ W_query\n",
    "key_2   = x_2 @ W_key\n",
    "value_2 = x_2 @ W_value\n",
    "print(\"Query 2: \", query_2)\n",
    "print(\"Key 2:   \", key_2)\n",
    "print(\"Value 2: \", value_2)\n",
    "#\n",
    "# print(\"\\n\")\n",
    "#\n",
    "\n",
    "keys = inputs @ W_key \n",
    "values = inputs @ W_value\n",
    "print(\"keys.shape:\", keys.shape)\n",
    "print(\"values.shape:\", values.shape)\n",
    "\n",
    "keys_2 = keys[1]\n",
    "attn_score_22 = query_2.dot(keys_2)\n",
    "# attn_score_22 = query_2 @ keys_2\n",
    "print(\"Attention score 22:\", attn_score_22)\n",
    "\n",
    "# Generalizing across all inputs\n",
    "attn_scores_2 = query_2 @ keys.T\n",
    "print(\"Attention Scores 2: \", attn_scores_2)\n",
    "# Check the second element is same as previously calculated attention score\n",
    "#\n",
    "# We compute the attention weights by scaling the attention scores and using the softmax function. \n",
    "# However, now we scale the attention scores by dividing them by the square root of the embedding \n",
    "# dimension of the keys\n",
    "\n",
    "attn_weights_2 = torch.softmax(attn_scores_2 / keys.shape[-1]**0.5, dim=-1)\n",
    "print(\"attn_weights_2: \", attn_weights_2)\n",
    "\n",
    "# The reason for the normalization by square root of embedding dimension size is to improve the training performance by avoiding small gradients. \n",
    "# For instance, when scaling up the embedding dimension, which is typically > 1,000 for GPT-like LLMs, large dot products can result in \n",
    "# very small gradients during backpropagation (due to the softmax function applied to them). \n",
    "# As dot products increase, the softmax function behaves more like a step function, resulting in gradients nearing zero. \n",
    "# These small gradients can drastically slow down learning or cause training to stagnate.\n",
    "#\n",
    "# The scaling by the square root of the embedding dimension is the reason why this self-attention mechanism is also called scaled-dot product attention.\n",
    "# Similar to when we computed the context vector as a weighted sum over the input vectors \n",
    "# we now compute the context vector as a weighted sum over the value vectors. \n",
    "# Here, the attention weights serve as a weighting factor that weighs the respective importance of each value vector\n",
    "context_vec_2 = attn_weights_2 @ values\n",
    "print(\"context_vec_2: \", context_vec_2)"
   ],
   "id": "fa980b537b01e646",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from src.SelfAttention_v2 import SelfAttention_v2\n",
    "\n",
    "torch.manual_seed(789)\n",
    "self_attn_v2 = SelfAttention_v2(d_in, d_out)\n",
    "#print(\"Context vectors from SelfAtten_v2: \\n\", self_attn_v2(inputs))\n",
    "\n",
    "# Note since the input contains 6 embedding vectors, the output also has 6 rows of context vectors\n",
    "\n",
    "\n",
    "# Causal Attention \n",
    "# First we apply softmax to the attention scores then mask with 0 above the diagonal and then normalize the rows to 1\n",
    "#\n",
    "queries = self_attn_v2.W_query(inputs)\n",
    "keys = self_attn_v2.W_key(inputs)\n",
    "attn_scores = queries @ keys.T\n",
    "attn_weights = torch.softmax(attn_scores / keys.shape[-1] ** 0.5, dim=-1)\n",
    "#print(\"Attention Wrights: \\n\",attn_weights)\n",
    "\n",
    "# Now mask the values above diagonal as 0 using the tril() function\n",
    "#\n",
    "context_length = attn_scores.shape[0]\n",
    "mask_simple = torch.tril(torch.ones(context_length, context_length))\n",
    "#print(\"Mask Simple: \\n\", mask_simple)\n",
    "#\n",
    "# Now simply multiply them to prevent the look ahead \n",
    "#\n",
    "masked_attention_weights = attn_weights * mask_simple\n",
    "#print(\"Masked attention weights: \\n\", masked_attention_weights)\n",
    "\n",
    "#\n",
    "# Now re-normalize to make sure rows add up to 1. To do this we divide each element by sum of each row\n",
    "#\n",
    "row_sums = masked_attention_weights.sum(dim=-1, keepdim=True)\n",
    "#print(\"row_sums: \\n\", row_sums)\n",
    "masked_simple_norm = masked_attention_weights / row_sums\n",
    "print(\"Masked & re-normalized weights: \\n\", masked_simple_norm)"
   ],
   "id": "2ca52ea3a908fe20",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# A more efficient way to obtain masked attention weights is to mask the attention scores with \n",
    "# negative infinity before applying softmax function. (e^negative infinity -> 0)\n",
    "# We can implement this masking by replacing values above the diagonal with 1 and then replacing them \n",
    "# with negative infinity\n",
    "#\n",
    "mask = torch.triu(torch.ones(context_length, context_length), diagonal=1)\n",
    "masked = attn_scores.masked_fill(mask.bool(), -torch.inf)\n",
    "print(\"Masked attention weights: \\n\", masked)\n",
    "#\n",
    "# Now apply the softmax function \n",
    "#\n",
    "attn_weights = torch.softmax(masked / keys.shape[-1] ** 0.5, dim=-1)\n",
    "print(\"Softmax'd Attention weights: \\n\", attn_weights)\n",
    "\n",
    "# Now we can use these modified attention weights to calculate the context vector\n",
    "#\n",
    "context_vec = attn_weights @ values\n",
    "print(\"context_vec: \\n\", context_vec)"
   ],
   "id": "f3a442b3d9b7413a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Masking additional weights with dropout\n",
    "# Drop out in the attention mechanism is applied at 2 specific times: \n",
    "# 1. After calculating the attention weights\n",
    "# 2. After applying the attention weights to value vectors\n",
    "# Here we will apply the dropout mask after computing the attention weights\n",
    "#\n",
    "# Lets use a dropout rate of 50% meaning half the attention weights will be masked out. \n",
    "# Normally it's a much lower rate like 0.1 or 0.2\n",
    "torch.manual_seed(123)\n",
    "dropout = torch.nn.Dropout(0.5)\n",
    "example = torch.ones(6, 6)\n",
    "print(dropout(example))\n",
    "#\n",
    "# Since we are applying 50% dropout, to compensate for reduction in active elements\n",
    "# we are going to scale up the values of remaining elements by a factor of 1/0.5 = 2\n",
    "# This scaling is crucial to maintain the balance of the attention weights\n",
    "#\n",
    "torch.manual_seed(123)\n",
    "print(\"Dropped out attention weights: \\n\", dropout(attn_weights))"
   ],
   "id": "d6b38e5f420f6067",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from src.CausalAttention import CausalAttention\n",
    "\n",
    "# Let’s ensure that the code can handle batches consisting of more than one input so that \n",
    "# the CausalAttention class supports the batch outputs produced by the data loader\n",
    "# To simulate batch input lets duplicate the input text\n",
    "#\n",
    "batch = torch.stack((inputs, inputs), dim=0)\n",
    "print(\"batch: \\n\", batch.shape)\n",
    "# print(batch)\n",
    "#\n",
    "# We can now use the CausalAttention class as follows\n",
    "#\n",
    "torch.manual_seed(123)\n",
    "context_layer = batch.shape[1]\n",
    "causal_attn = CausalAttention(d_in, d_out, context_length, 0.0, False)\n",
    "context_vecs = causal_attn(batch)\n",
    "print(\"context_vecs: \\n\", context_vecs.shape)"
   ],
   "id": "50701dd69a456857",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-17T20:48:05.671434Z",
     "start_time": "2025-01-17T20:48:05.592451Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from src.MultiHeadAttentionWrapper import MultiHeadAttentionWrapper\n",
    "\n",
    "# Multi Head Attention\n",
    "# Now if we use the MultiHeadAttentionWrapper class with two attention heads, and CausalAttention \n",
    "# output dimension d_out = 2, we get a 4 dimensional context vector (d_out * num_heads = 4).\n",
    "#\n",
    "torch.manual_seed(123)\n",
    "context_length = batch.shape[1]\n",
    "d_in, d_out = 3, 2\n",
    "multi_head_attn = MultiHeadAttentionWrapper(d_in, d_out, context_length, dropout=0.0, num_heads=2, qkv_bias=False)\n",
    "context_vecs = multi_head_attn(batch)\n",
    "print(\"context_vecs: \\n\", context_vecs)\n",
    "print(\"context_vecs.shape: \\n\", context_vecs.shape)\n"
   ],
   "id": "9d8058957dc253b",
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 3, got 2)",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[120], line 11\u001B[0m\n\u001B[1;32m      9\u001B[0m d_in, d_out \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m3\u001B[39m, \u001B[38;5;241m2\u001B[39m\n\u001B[1;32m     10\u001B[0m multi_head_attn \u001B[38;5;241m=\u001B[39m MultiHeadAttentionWrapper(d_in, d_out, context_length, dropout\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0.0\u001B[39m, num_heads\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m2\u001B[39m, qkv_bias\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m)\n\u001B[0;32m---> 11\u001B[0m context_vecs \u001B[38;5;241m=\u001B[39m \u001B[43mmulti_head_attn\u001B[49m\u001B[43m(\u001B[49m\u001B[43mbatch\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     12\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcontext_vecs: \u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m, context_vecs)\n\u001B[1;32m     13\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcontext_vecs.shape: \u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m, context_vecs\u001B[38;5;241m.\u001B[39mshape)\n",
      "File \u001B[0;32m~/Documents/ws/python/PycharmProjects/SimpleLLMProject/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1734\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1735\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1736\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Documents/ws/python/PycharmProjects/SimpleLLMProject/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1742\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1743\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1744\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1745\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1746\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1747\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1749\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m   1750\u001B[0m called_always_called_hooks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m()\n",
      "File \u001B[0;32m~/Documents/ws/python/PycharmProjects/SimpleLLMProject/src/MultiHeadAttentionWrapper.py:15\u001B[0m, in \u001B[0;36mMultiHeadAttentionWrapper.forward\u001B[0;34m(self, x)\u001B[0m\n\u001B[1;32m     14\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, x):\n\u001B[0;32m---> 15\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mcat([\u001B[43mhead\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m \u001B[38;5;28;01mfor\u001B[39;00m head \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mheads], dim\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m)\n",
      "File \u001B[0;32m~/Documents/ws/python/PycharmProjects/SimpleLLMProject/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1734\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1735\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1736\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Documents/ws/python/PycharmProjects/SimpleLLMProject/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1742\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1743\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1744\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1745\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1746\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1747\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1749\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m   1750\u001B[0m called_always_called_hooks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m()\n",
      "File \u001B[0;32m~/Documents/ws/python/PycharmProjects/SimpleLLMProject/src/CausalAttention.py:18\u001B[0m, in \u001B[0;36mCausalAttention.forward\u001B[0;34m(self, x)\u001B[0m\n\u001B[1;32m     17\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, x):\n\u001B[0;32m---> 18\u001B[0m     b, num_tokens, d_in \u001B[38;5;241m=\u001B[39m x\u001B[38;5;241m.\u001B[39mshape\n\u001B[1;32m     19\u001B[0m     queries \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mW_query(x)\n\u001B[1;32m     20\u001B[0m     keys \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mW_key(x)\n",
      "\u001B[0;31mValueError\u001B[0m: not enough values to unpack (expected 3, got 2)"
     ]
    }
   ],
   "execution_count": 120
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# import torch\n",
    "# if torch.backends.mps.is_available():\n",
    "#     mps_device = torch.device('mps')\n",
    "#     x = torch.ones(1, device=mps_device)\n",
    "#     print(x)\n",
    "# else:\n",
    "#     print(\"mps not available\")"
   ],
   "id": "8dfa9b39ec1db8c2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-17T20:48:05.673759Z",
     "start_time": "2025-01-17T20:48:05.673693Z"
    }
   },
   "cell_type": "code",
   "source": [
    "a = torch.tensor([[[[0.2745, 0.6584, 0.2775, 0.8573],\n",
    "                    [0.8993, 0.0390, 0.9268, 0.7388],\n",
    "                    [0.7179, 0.7058, 0.9156, 0.4340]],\n",
    "\n",
    "                   [[0.0772, 0.3565, 0.1479, 0.5331],\n",
    "                    [0.4066, 0.2318, 0.4545, 0.9737],\n",
    "                    [0.4606, 0.5159, 0.4220, 0.5786]]]])\n",
    "# print(a.transpose(2, 3))\n",
    "\n",
    "\n",
    "first_head = a[0, 0, :, :]\n",
    "first_res = first_head @ first_head.T\n",
    "print(\"First head:\\n\", first_res)\n",
    "\n",
    "second_head = a[0, 1, :, :]\n",
    "second_res = second_head @ second_head.T\n",
    "print(\"\\nSecond head:\\n\", second_res)\n",
    "\n",
    "print(f\"Batched: \\n{a @ a.transpose(2, 3)}\")"
   ],
   "id": "f20088921d8113c0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-17T20:48:05.684385Z",
     "start_time": "2025-01-17T20:48:05.675451Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from src.MultiHeadAttention import MultiHeadAttention\n",
    "\n",
    "torch.manual_seed(123)\n",
    "batch_size, context_length, d_in = batch.shape\n",
    "print(\"Batch Shape: \\n\", batch.shape)\n",
    "d_out = 2\n",
    "mha = MultiHeadAttention(d_in, d_out, context_length, 0.0, num_heads=2)\n",
    "context_vecs = mha(batch)\n",
    "print(context_vecs)\n",
    "print(\"Context vector shape: \\n\", context_vecs.shape)\n"
   ],
   "id": "df6a05dfc13ebbe4",
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 3, got 2)",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[121], line 4\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01msrc\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mMultiHeadAttention\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m MultiHeadAttention\n\u001B[1;32m      3\u001B[0m torch\u001B[38;5;241m.\u001B[39mmanual_seed(\u001B[38;5;241m123\u001B[39m)\n\u001B[0;32m----> 4\u001B[0m batch_size, context_length, d_in \u001B[38;5;241m=\u001B[39m batch\u001B[38;5;241m.\u001B[39mshape\n\u001B[1;32m      5\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mBatch Shape: \u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m, batch\u001B[38;5;241m.\u001B[39mshape)\n\u001B[1;32m      6\u001B[0m d_out \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m2\u001B[39m\n",
      "\u001B[0;31mValueError\u001B[0m: not enough values to unpack (expected 3, got 2)"
     ]
    }
   ],
   "execution_count": 121
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "\n",
    "---\n",
    "## Chapter 4: Implementing GPT from Scratch to generate text\n",
    "\n",
    "---"
   ],
   "id": "3f958161a85ca549"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-17T20:48:05.688877Z",
     "start_time": "2025-01-17T20:48:05.686809Z"
    }
   },
   "cell_type": "code",
   "source": [
    "GPT_CONFIG_124M = {\n",
    "    \"vocab_size\": 50257,     # Vocabulary size\n",
    "    \"context_length\": 1024,  # Context length\n",
    "    \"emb_dim\": 768,          # Embedding dimension\n",
    "    \"n_heads\": 12,           # Number of attention heads\n",
    "    \"n_layers\": 12,          # Number of layers\n",
    "    \"drop_rate\": 0.1,        # Dropout rate\n",
    "    \"qkv_bias\": False        # Query-Key-Value bias\n",
    "}"
   ],
   "id": "16f34621162871e9",
   "outputs": [],
   "execution_count": 122
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Here is the proposed architecture and order of implementation\n",
    "![image](../data/4-3.png)\n"
   ],
   "id": "5ac1cbf145ad6daf"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-17T20:48:06.231236Z",
     "start_time": "2025-01-17T20:48:05.700578Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from src.DummyGPTModel import DummyGPTModel\n",
    "import tiktoken\n",
    "#\n",
    "torch.manual_seed(123)\n",
    "#\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "batch = []\n",
    "txt1 = \"Every effort moves you\"\n",
    "txt2 = \"Every day holds a\"\n",
    "batch.clear()\n",
    "batch.append(torch.tensor(tokenizer.encode(txt1)))\n",
    "batch.append(torch.tensor(tokenizer.encode(txt2)))\n",
    "batch = torch.stack(batch, dim=0)\n",
    "print(\"Input Batch: \\n\", batch)\n",
    "#\n",
    "# Next, we initialize a new 124-million-parameter DummyGPTModel instance \n",
    "# and feed it the tokenized batch\n",
    "#\n",
    "model = DummyGPTModel(GPT_CONFIG_124M)\n",
    "logits = model(batch)\n",
    "print(\"Output shape: \\n\", logits.shape)\n",
    "#print(logits)\n",
    "#                      \n",
    "#"
   ],
   "id": "75010152b67235a2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Batch: \n",
      " tensor([[6109, 3626, 6100,  345],\n",
      "        [6109, 1110, 6622,  257]])\n",
      "Output shape: \n",
      " torch.Size([2, 4, 50257])\n"
     ]
    }
   ],
   "execution_count": 123
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-17T20:48:06.238626Z",
     "start_time": "2025-01-17T20:48:06.233185Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "# Let’s now implement LAYER Normalization to improve the stability and efficiency of the training.\n",
    "# The main idea behind LAYER Normalization is to adjust the activations (outputs) of a deep\n",
    "# neural network layer to have a mean of 0 and a variance of 1\n",
    "#\n",
    "# This adjustment speeds up the convergence.\n",
    "#\n",
    "torch.manual_seed(123)\n",
    "torch.set_printoptions(sci_mode=False)\n",
    "#\n",
    "batch_example = torch.randn(2, 5)\n",
    "layer = nn.Sequential(nn.Linear(5,6), nn.ReLU())\n",
    "out = layer(batch_example)\n",
    "print(\"Layer: \\n\",out)\n",
    "#\n",
    "# The NN Layer contains the non-linear activation ReLU which 0's out the negative values\n",
    "# \n",
    "mean = out.mean(dim=-1, keepdim=True)\n",
    "var = out.var(dim=-1, keepdim=True)\n",
    "print(\"Mean: \", mean)\n",
    "print(\"Variance: \", var)\n",
    "print(\"\\n\")\n",
    "#\n",
    "# Next, let’s apply layer normalization to the layer outputs we obtained earlier. \n",
    "# The operation consists of subtracting the mean and dividing by the square root \n",
    "# of the variance (also known as the standard deviation):\n",
    "#\n",
    "eps = 1e-5\n",
    "out_norm = (out - mean) / torch.sqrt(var)\n",
    "# Dim = -1 indicates statistics along the last dimention\n",
    "mean = out_norm.mean(dim=-1, keepdim=True)\n",
    "var = out_norm.var(dim=-1, keepdim=True)\n",
    "torch.set_printoptions(sci_mode=True)\n",
    "print(\"Normalized Layer Outputs: \\n\", out_norm)\n",
    "print(\"Mean: \\n\", mean)\n",
    "print(\"Variance: \\n\", var)\n",
    "print(\"-------------------------\\n\")\n",
    "\n",
    "torch.set_printoptions(sci_mode=False)\n",
    "print(\"Mean:\\n\", mean)\n",
    "print(\"Variance:\\n\", var)\n"
   ],
   "id": "c06060ecd0f8f8be",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: \n",
      " tensor([[0.2260, 0.3470, 0.0000, 0.2216, 0.0000, 0.0000],\n",
      "        [0.2133, 0.2394, 0.0000, 0.5198, 0.3297, 0.0000]],\n",
      "       grad_fn=<ReluBackward0>)\n",
      "Mean:  tensor([[0.1324],\n",
      "        [0.2170]], grad_fn=<MeanBackward1>)\n",
      "Variance:  tensor([[0.0231],\n",
      "        [0.0398]], grad_fn=<VarBackward0>)\n",
      "\n",
      "\n",
      "Normalized Layer Outputs: \n",
      " tensor([[6.1585e-01, 1.4126e+00, -8.7188e-01, 5.8723e-01, -8.7188e-01, -8.7188e-01],\n",
      "        [-1.8865e-02, 1.1211e-01, -1.0876e+00, 1.5173e+00, 5.6474e-01, -1.0876e+00]],\n",
      "       grad_fn=<DivBackward0>)\n",
      "Mean: \n",
      " tensor([[-5.9605e-08],\n",
      "        [ 1.9868e-08]], grad_fn=<MeanBackward1>)\n",
      "Variance: \n",
      " tensor([[1.0000e+00],\n",
      "        [1.0000e+00]], grad_fn=<VarBackward0>)\n",
      "-------------------------\n",
      "\n",
      "Mean:\n",
      " tensor([[    -0.0000],\n",
      "        [     0.0000]], grad_fn=<MeanBackward1>)\n",
      "Variance:\n",
      " tensor([[1.0000],\n",
      "        [1.0000]], grad_fn=<VarBackward0>)\n"
     ]
    }
   ],
   "execution_count": 124
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-17T20:48:06.242078Z",
     "start_time": "2025-01-17T20:48:06.239109Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from src.LayerNorm import LayerNorm\n",
    "\n",
    "# Previously we used unbiased = False in our variance calculation. This doesnt \n",
    "# apply Bessel's correction where divisor is n-1 instead of n. But this is \n",
    "# compatible with GPT-2\n",
    "\n",
    "ln = LayerNorm(emb_dim = 5)\n",
    "out_ln = ln(batch_example)\n",
    "mean = out_ln.mean(dim=-1, keepdim=True)\n",
    "var = out_ln.var(dim=-1, unbiased=False, keepdim=True)\n",
    "print(\"Mean: \\n\", mean)\n",
    "print(\"Variance: \\n\", var)\n"
   ],
   "id": "a39e430a037896f0",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean: \n",
      " tensor([[    -0.0000],\n",
      "        [     0.0000]], grad_fn=<MeanBackward1>)\n",
      "Variance: \n",
      " tensor([[1.0000],\n",
      "        [1.0000]], grad_fn=<VarBackward0>)\n"
     ]
    }
   ],
   "execution_count": 125
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-17T20:48:06.325260Z",
     "start_time": "2025-01-17T20:48:06.243263Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Let us see how the GELU (Gaussian Error Linear Unit) stacks up against \n",
    "# # RELU (REctified Linear Unit)\n",
    "from src.GELU import GELU\n",
    "import matplotlib.pyplot as plt\n",
    "gelu, relu = GELU(), nn.ReLU()\n",
    "\n",
    "x = torch.linspace(-3, 3, 100)\n",
    "y_gelu, y_relu = gelu(x), relu(x)\n",
    "plt.figure(figsize=(8, 3))\n",
    "for i, (y, label) in enumerate(zip([y_gelu, y_relu], [\"GELU\", \"ReLU\"]), 1):\n",
    "    plt.subplot(1, 2, i)\n",
    "    plt.plot(x, y)\n",
    "    plt.title(f\"{label} activation function\")\n",
    "    plt.xlabel(\"x\")\n",
    "    plt.ylabel(f\"{label}(x)\")\n",
    "    plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "3130c2c83093ee01",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 800x300 with 2 Axes>"
      ],
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 126
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-17T20:48:06.345220Z",
     "start_time": "2025-01-17T20:48:06.325976Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from src.SimpleFeedForward import SimpleFeedForward\n",
    "\n",
    "# As we can see the smoothness of the GELU can lead to better optimization properties during training\n",
    "# as it allows more nuanced finer adjustments to models parameters. In contrast, RELU has a sharp corner\n",
    "# that can make adjustments difficult for very deep networks.\n",
    "#\n",
    "# Next we look at implementing a feed forward network with GELU activations\n",
    "# See SimpleFeedForward.py\n",
    "#\n",
    "sff = SimpleFeedForward(GPT_CONFIG_124M)\n",
    "x = torch.rand(2, 3, 768)\n",
    "out = sff(x)\n",
    "print(out.shape)"
   ],
   "id": "e496e641f9044a67",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 768])\n"
     ]
    }
   ],
   "execution_count": 127
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-17T20:48:06.360991Z",
     "start_time": "2025-01-17T20:48:06.346033Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from src.ExampleDeepNeuralNetwork import ExampleDeepNeuralNetwork\n",
    "\n",
    "# Next we implement Shortcut Connections\n",
    "# Each layer will be initialized such that it accepts an example with three input \n",
    "# values and returns three output values.\n",
    "layer_sizes = [3, 3, 3, 3, 3, 1]\n",
    "sample_input = torch.tensor([[1., 0., -1.]])\n",
    "torch.manual_seed(123)\n",
    "model_without_shortcut = ExampleDeepNeuralNetwork(layer_sizes, use_shortcut=False)\n",
    "\n",
    "\n",
    "# Next lets print the gradients\n",
    "def print_gradients(nnmodel, input_x):\n",
    "    output = nnmodel(input_x)\n",
    "    target = torch.tensor([[0.]])\n",
    "    loss = nn.MSELoss()\n",
    "    loss = loss(output, target)\n",
    "    loss.backward()\n",
    "    for name, param in model.named_parameters():\n",
    "        # print(name, \" = \", param)\n",
    "        if 'weight' in name:\n",
    "            print(f\"{name} has gradient mean of {param.grad.abs().mean().item()}\")\n",
    "\n",
    "#\n",
    "# Now Lets use this function to print the gradients calculated by loss.backward()\n",
    "print_gradients(model_without_shortcut, sample_input)\n"
   ],
   "id": "98a59bc5db854fa4",
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'abs'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[128], line 26\u001B[0m\n\u001B[1;32m     22\u001B[0m             \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mname\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m has gradient mean of \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mparam\u001B[38;5;241m.\u001B[39mgrad\u001B[38;5;241m.\u001B[39mabs()\u001B[38;5;241m.\u001B[39mmean()\u001B[38;5;241m.\u001B[39mitem()\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m     24\u001B[0m \u001B[38;5;66;03m#\u001B[39;00m\n\u001B[1;32m     25\u001B[0m \u001B[38;5;66;03m# Now Lets use this function to print the gradients calculated by loss.backward()\u001B[39;00m\n\u001B[0;32m---> 26\u001B[0m \u001B[43mprint_gradients\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel_without_shortcut\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msample_input\u001B[49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[0;32mIn[128], line 22\u001B[0m, in \u001B[0;36mprint_gradients\u001B[0;34m(nnmodel, input_x)\u001B[0m\n\u001B[1;32m     19\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m name, param \u001B[38;5;129;01min\u001B[39;00m model\u001B[38;5;241m.\u001B[39mnamed_parameters():\n\u001B[1;32m     20\u001B[0m     \u001B[38;5;66;03m# print(name, \" = \", param)\u001B[39;00m\n\u001B[1;32m     21\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mweight\u001B[39m\u001B[38;5;124m'\u001B[39m \u001B[38;5;129;01min\u001B[39;00m name:\n\u001B[0;32m---> 22\u001B[0m         \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mname\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m has gradient mean of \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[43mparam\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgrad\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mabs\u001B[49m()\u001B[38;5;241m.\u001B[39mmean()\u001B[38;5;241m.\u001B[39mitem()\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n",
      "\u001B[0;31mAttributeError\u001B[0m: 'NoneType' object has no attribute 'abs'"
     ]
    }
   ],
   "execution_count": 128
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-17T20:48:06.361801Z",
     "start_time": "2025-01-17T20:48:06.361752Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# As you can see above gradients become tiny aka Vanishing from Layer4 to Layer1\n",
    "# Let’s now instantiate a model with skip connections and see how it compares:\n",
    "torch.manual_seed(123)\n",
    "model_with_shortcut = ExampleDeepNeuralNetwork(layer_sizes, use_shortcut=True)\n",
    "print_gradients(model_without_shortcut, sample_input)\n"
   ],
   "id": "4bfdfab7cb5bcc5f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# The gradient doesn't approach a vanishingly small value\n",
    "# In conclusion, shortcut connections are important for overcoming the limitations posed \n",
    "# by the vanishing gradient problem in deep neural networks.\n",
    "\n"
   ],
   "id": "a229081ed60e29b5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Next, we’ll connect all the previously covered concepts (layer normalization, GELU activations, feed forward module, and shortcut connections) in a transformer  block, which is the final building block we need to code the GPT architecture.\n",
    "\n",
    "![image](../data/transformer_wiring.png)"
   ],
   "id": "2df6d339f0570629"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# See TransformerBlock.py for the basic wiring and feedforward details\n",
    "from src.TransformerBlock import TransformerBlock\n",
    "\n",
    "torch.manual_seed(123)\n",
    "x = torch.rand(2, 4, 768)\n",
    "tr_block = TransformerBlock(GPT_CONFIG_124M)\n",
    "out = tr_block(x)\n",
    "#\n",
    "print(x.shape)\n",
    "print(out.shape)\n"
   ],
   "id": "cd526dd3047ba477",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "8c3e76bf70259f86"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
