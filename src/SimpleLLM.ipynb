{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8a77807f92f26ee",
   "metadata": {},
   "source": [
    "# This is an attempt to learn by building and training an LLM from Scratch\n",
    "## Chapter 01  "
   ]
  },
  {
   "cell_type": "code",
   "id": "fbc121e30a2defb3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-01T20:32:01.080452Z",
     "start_time": "2025-02-01T20:32:00.110611Z"
    }
   },
   "source": [
    "\n",
    "import os\n",
    "import torch\n",
    "from torch import Tensor\n",
    "import urllib.request\n",
    "    \n",
    "# Check which GPU if any is available\n",
    "# torch.cuda.is_available():\n",
    "#     device = torch.device(\"cuda\")\n",
    "#     x: Tensor = torch.ones(1, device=device)\n",
    "#     print(f\"x = {x} using 'cuda:0' backend\")\n",
    "#     \n",
    "# elif \n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "    x: Tensor = torch.ones(1, device=device)\n",
    "    print(f\"x = {x} using {device} backend\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    # x: Tensor = torch.ones(1, device=device)\n",
    " \n",
    "print(device)\n",
    "\n",
    "def get_some_text():\n",
    "    # Download a text (book)\n",
    "    bookUrl = \"https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/ch02/01_main-chapter-code/the-verdict.txt\"  \n",
    "    filepath = \"../data/the-verdict.txt\"\n",
    "    # print(file_path)\n",
    "    if not os.path.exists(filepath):\n",
    "        urllib.request.urlretrieve(bookUrl, filepath)\n",
    "\n",
    "    with open(filepath, \"r\", encoding=\"utf-8\") as f:\n",
    "        rawtext = f.read()\n",
    "        \n",
    "    print(\"Total characters in the story: \", len(rawtext))\n",
    "    print(\"Total Lines in raw text: \", rawtext.count(\"\\n\"))\n",
    "    return rawtext\n",
    "\n",
    "raw_text = get_some_text()\n",
    "print(\"Some text: \", raw_text[:49])"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x = tensor([1.], device='mps:0') using mps backend\n",
      "mps\n",
      "Total characters in the story:  20479\n",
      "Total Lines in raw text:  164\n",
      "Some text:  I HAD always thought Jack Gisburn rather a cheap \n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "id": "2c7227e79afbcad7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-01T20:32:01.086665Z",
     "start_time": "2025-02-01T20:32:01.082040Z"
    }
   },
   "source": [
    "# Now we have to tokenize the text. The best way to do that is to use a pre-build tokennizer, but first we will try some \n",
    "# basic python regular expressions to do the same things\n",
    "import re\n",
    "preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', raw_text)\n",
    "preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
    "#\n",
    "print(len(preprocessed))\n",
    "print(preprocessed[:30])"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4690\n",
      "['I', 'HAD', 'always', 'thought', 'Jack', 'Gisburn', 'rather', 'a', 'cheap', 'genius', '--', 'though', 'a', 'good', 'fellow', 'enough', '--', 'so', 'it', 'was', 'no', 'great', 'surprise', 'to', 'me', 'to', 'hear', 'that', ',', 'in']\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "id": "b9dc98b584bc6d61",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-01T20:32:01.091004Z",
     "start_time": "2025-02-01T20:32:01.087504Z"
    }
   },
   "source": [
    "# Now we need to generate token IDs\n",
    "# Now let us create a list of all unique tokens and sort them alphabetically to determine the vocabulary size\n",
    "all_uniq_words = sorted(set(preprocessed))\n",
    "vocab_size = len(all_uniq_words)\n",
    "print(\"Vocab size: \", vocab_size)\n",
    "\n",
    "# Now that we know the vocabulary size, lets enumerate and assign some numbers to them\n",
    "vocab = {token:integer for integer,token in enumerate(all_uniq_words)}\n",
    "for i, item in enumerate(vocab.items()):\n",
    "    print(item)\n",
    "    if i >= 20:\n",
    "        break\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size:  1130\n",
      "('!', 0)\n",
      "('\"', 1)\n",
      "(\"'\", 2)\n",
      "('(', 3)\n",
      "(')', 4)\n",
      "(',', 5)\n",
      "('--', 6)\n",
      "('.', 7)\n",
      "(':', 8)\n",
      "(';', 9)\n",
      "('?', 10)\n",
      "('A', 11)\n",
      "('Ah', 12)\n",
      "('Among', 13)\n",
      "('And', 14)\n",
      "('Are', 15)\n",
      "('Arrt', 16)\n",
      "('As', 17)\n",
      "('At', 18)\n",
      "('Be', 19)\n",
      "('Begin', 20)\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "id": "b49e9060996c9f10",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-01T20:32:01.096072Z",
     "start_time": "2025-02-01T20:32:01.091588Z"
    }
   },
   "source": [
    "from src.chapter02.SimpleTokenizerV1 import SimpleTokenizerV1\n",
    "\n",
    "# Now we want to apply this vocabulary to convert new text to generate token id\n",
    "# When we want to convert the outputs of an LLM from numbers back into text, we need a way to turn token IDs into text. \n",
    "# For this, we can create an inverse version of the vocabulary that maps token IDs back to the corresponding text tokens.\n",
    "\n",
    "tokenizer = SimpleTokenizerV1(vocab)\n",
    "text = \"\"\"\"It's the last he painted you know,\" \n",
    "        Mrs Gisburn said with pardonable pride.\"\"\"\n",
    "\n",
    "ids = tokenizer.encode(text)\n",
    "print(ids)\n",
    "print(tokenizer.decode(ids))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 56, 2, 850, 988, 602, 533, 746, 1126, 596, 5, 1, 67, 38, 851, 1108, 754, 793, 7]\n",
      "\" It' s the last he painted you know,\" Mrs Gisburn said with pardonable pride.\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "id": "92b6519fe1741db8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-01T20:32:01.101630Z",
     "start_time": "2025-02-01T20:32:01.098131Z"
    }
   },
   "source": [
    "all_tokens = sorted(list(set(preprocessed))) # Make preprocessed a list so we can extend it\n",
    "all_tokens.extend([\"<|unk|>\", \"<|endoftext|>\"])\n",
    "# redo the vocab population\n",
    "vocab = {token:integer for integer, token in enumerate(all_tokens)}\n",
    "print(len(vocab.items()))\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1132\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "id": "d03ae607c2d20268",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-01T20:32:01.105287Z",
     "start_time": "2025-02-01T20:32:01.102639Z"
    }
   },
   "source": [
    "# Print the last 5 vocab items\n",
    "for i, item in enumerate(list(vocab.items())[-5:]):\n",
    "    print(item)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('younger', 1127)\n",
      "('your', 1128)\n",
      "('yourself', 1129)\n",
      "('<|unk|>', 1130)\n",
      "('<|endoftext|>', 1131)\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "id": "dcdf224cd056d26f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-01T20:32:01.110033Z",
     "start_time": "2025-02-01T20:32:01.106898Z"
    }
   },
   "source": [
    "from src.chapter02.SimpleTokenizerV2 import SimpleTokenizerV2\n",
    "\n",
    "text1 = \"Hello, do you like tea?\"\n",
    "text2 = \"In the sunlit terraces of the palace.\"\n",
    "text = \" <|endoftext|> \".join((text1, text2))\n",
    "print(text)\n",
    "tokenizer = SimpleTokenizerV2(vocab)\n",
    "print(tokenizer.encode(text))\n",
    "print(tokenizer.decode(tokenizer.encode(text)))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, do you like tea? <|endoftext|> In the sunlit terraces of the palace.\n",
      "[1130, 5, 355, 1126, 628, 975, 10, 1131, 55, 988, 956, 984, 722, 988, 1130, 7]\n",
      "<|unk|>, do you like tea? <|endoftext|> In the sunlit terraces of the <|unk|>.\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "id": "e4e4738351ee6b9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-01T20:32:01.122309Z",
     "start_time": "2025-02-01T20:32:01.111784Z"
    }
   },
   "source": [
    "### Byte Pair Encoding \n",
    "from importlib.metadata import version\n",
    "import tiktoken\n",
    "print(\"Tiktoken version: \", version(\"tiktoken\"))\n",
    "#print(\"Tiktoken version: \", tiktoken.__version__)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tiktoken version:  0.8.0\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "id": "ac57b0bafdc676f4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-01T20:32:01.298200Z",
     "start_time": "2025-02-01T20:32:01.123304Z"
    }
   },
   "source": [
    "#### This is the tokenizer using the GPT2 tokenization model\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "text = \"Hello, do you like tea? <|endoftext|> In the sunlit terraces of someunknownPlace.\"\n",
    "integers = tokenizer.encode(text, allowed_special={\"<|endoftext|>\"})\n",
    "print(f\"Encoded: {integers}\")\n",
    "strings = tokenizer.decode(integers)\n",
    "print(f\"Decoded: {strings}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded: [15496, 11, 466, 345, 588, 8887, 30, 220, 50256, 554, 262, 4252, 18250, 8812, 2114, 286, 617, 34680, 27271, 13]\n",
      "Decoded: Hello, do you like tea? <|endoftext|> In the sunlit terraces of someunknownPlace.\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "id": "2a6a5225d4fab946",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-01T20:32:01.302388Z",
     "start_time": "2025-02-01T20:32:01.299671Z"
    }
   },
   "source": [
    "print(tokenizer.encode(\"Akwirw ier\"))\n",
    "print(tokenizer.decode(tokenizer.encode(\"Akwirw ier\")))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[33901, 86, 343, 86, 220, 959]\n",
      "Akwirw ier\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "id": "1ab2edd9fb1ca03",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-01T20:32:01.307910Z",
     "start_time": "2025-02-01T20:32:01.303297Z"
    }
   },
   "source": [
    "# Let's now do Data Sampling with a sliding window\n",
    "# 1. Let's tokenize the entire story with BPE tokenizer first\n",
    "\n",
    "encoded_text = tokenizer.encode(raw_text)\n",
    "print(len(encoded_text))\n",
    "enc_sample = encoded_text[50:]\n",
    "\n",
    "# Now Let's start by defining x and y where x has input tokens and y the output tokens shifted by 1\n",
    "context_size = 4\n",
    "x = enc_sample[:context_size]\n",
    "y = enc_sample[1:context_size+1]\n",
    "# print(f\"x: {x}\")\n",
    "# print(f\"y:      {y}\")\n",
    "\n",
    "\n",
    "#####\n",
    "# Next word prediction tasks can now be created by \n",
    "for i in range(1, context_size + 1):\n",
    "    context = enc_sample[:i]\n",
    "    desired = enc_sample[i]\n",
    "    # print(f\"context input: {context} --> desired prediction: {desired}\")\n",
    "    # Now we create the input output target pairs\n",
    "    print(tokenizer.decode(context), \"---->\", tokenizer.decode([desired]))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5145\n",
      " and ---->  established\n",
      " and established ---->  himself\n",
      " and established himself ---->  in\n",
      " and established himself in ---->  a\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "id": "62094a4513e01008",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-01T20:32:01.310687Z",
     "start_time": "2025-02-01T20:32:01.308610Z"
    }
   },
   "source": [
    "# from Dataloader import Dataloader\n",
    "# \n",
    "# dataloader = Dataloader(batch_size=8, max_length=4, stride=4, shuffle=False, drop_last=True, num_workers=0)\n",
    "# dataloader = dataloader.get_instance(file_path, text_enc='utf-8', mode='r')\n",
    "# if dataloader is not None:\n",
    "#     data_iter = iter(dataloader)\n",
    "#     inputs, targets = next(data_iter)\n",
    "#     print(\"Loaded text data...\\n\")\n",
    "#     print(\"Inputs: \\n\", inputs)\n",
    "#     print(\"\\nTargets: \\n\", targets)\n",
    "# else: \n",
    "#     print(\"Failed loading \", dataloader)\n"
   ],
   "outputs": [],
   "execution_count": 12
  },
  {
   "cell_type": "code",
   "id": "fa0c4b5f389cdc4e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-01T20:32:01.465250Z",
     "start_time": "2025-02-01T20:32:01.311521Z"
    }
   },
   "source": [
    "import torch.nn\n",
    "from src.chapter02.Dataloader import Dataloader\n",
    "file_path = \"../data/the-verdict.txt\"\n",
    "\n",
    "####\n",
    "# Finally we need to create the embeddings for the tokens\n",
    "# If we have a batch size of 8 with 4 tokens each it'll be an 8 x 4 x 256 tensor\n",
    "max_length = 4  \n",
    "\n",
    "mydataloader = Dataloader(batch_size=8, max_length=max_length, stride=4, shuffle=False, drop_last=True, num_workers=0)\n",
    "dataloader = mydataloader.create_dataloader_v1(txt=raw_text)\n",
    "data_iter = iter(dataloader)\n",
    "inputs, targets = next(data_iter)\n",
    "# print(\"Input Token IDs:\\n\", inputs)\n",
    "# print(\"Input tensor shape: \", inputs.shape) \n",
    "\n",
    "# Now since self-attentions are position agnostic, we should add some positional data.\n",
    "# Absolute and relative positional data can be added. So let's create embeddings with say 256 dimensions\n",
    "vocab_size = 50257\n",
    "output_dim = 256\n",
    "# context_length = 1024\n",
    "\n",
    "## Now lets embed the input tensors\n",
    "token_embedding_layer = torch.nn.Embedding(num_embeddings=vocab_size, embedding_dim=output_dim)\n",
    "token_embeddings = token_embedding_layer(inputs)\n",
    "print(\"Token embeddings shape: \", token_embeddings.shape) #8x4x256\n",
    "\n",
    "\n",
    "# For a GPT model’s absolute position embedding approach, we just need to create another embedding \n",
    "# layer that has the same embedding dimension as the token_embedding_ layer:\n",
    "context_length = max_length     #context is length of positions we care about for attention\n",
    "pos_embedding_layer = torch.nn.Embedding(context_length, output_dim)\n",
    "pos_embeddings = pos_embedding_layer(torch.arange(context_length))\n",
    "print(\"Positional Embeddings Shape: \", pos_embeddings.shape) # 4x256\n",
    "#\n",
    "# Add the positional embeddings to token embeddings\n",
    "input_embeddings = token_embeddings + pos_embeddings\n",
    "print(\"Position Merged Input Embeddings Shape: \", input_embeddings.shape)\n",
    "#\n",
    "# Now lets look at the dataloader\n",
    "for batch in dataloader:\n",
    "    inputs, targets = batch\n",
    "    token_embeddings = token_embedding_layer(inputs)\n",
    "    pos_embeddings = pos_embedding_layer(torch.arange(max_length))\n",
    "    input_embeddings = token_embeddings + pos_embeddings\n",
    "    break\n",
    "#\n",
    "print(\"Batch Embeddings Shape: \", input_embeddings.shape)\n",
    "    \n",
    "print(\"Input tensor \", x)\n",
    "print(\"Target tensor\", y)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token embeddings shape:  torch.Size([8, 4, 256])\n",
      "Positional Embeddings Shape:  torch.Size([4, 256])\n",
      "Position Merged Input Embeddings Shape:  torch.Size([8, 4, 256])\n",
      "Batch Embeddings Shape:  torch.Size([8, 4, 256])\n",
      "Input tensor  [290, 4920, 2241, 287]\n",
      "Target tensor [4920, 2241, 287, 257]\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "cell_type": "code",
   "id": "95c2c3a6eb3eea50",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-01T20:32:01.472783Z",
     "start_time": "2025-02-01T20:32:01.468523Z"
    }
   },
   "source": [
    "# Chapter 3 - Attention\n",
    "#\n",
    "import torch\n",
    "\n",
    "# In self-attention our goal is to calculate context vector z(i) for each \n",
    "# element x(i) of the input sequence. Consider the following input sequence \n",
    "#\n",
    "inputs = torch.tensor(\n",
    "  [[0.43, 0.15, 0.89], # Your     (x^1)\n",
    "   [0.55, 0.87, 0.66], # journey  (x^2)\n",
    "   [0.57, 0.85, 0.64], # starts   (x^3)\n",
    "   [0.22, 0.58, 0.33], # with     (x^4)\n",
    "   [0.77, 0.25, 0.10], # one      (x^5)\n",
    "   [0.05, 0.80, 0.55]] # step     (x^6)\n",
    ")\n",
    "# inputs.to(device)\n",
    "print(\"Input sequence shape: \", inputs.shape)\n",
    "# \n",
    "# Now calculate weights for attention\n",
    "# Assume query is the second word \"journey\" or inputs[1] \n",
    "#\n",
    "query = inputs[1]\n",
    "# query.to(device)\n",
    "print(f\"Query is the 2nd word 'journey': {query}\")\n",
    "#\n",
    "attention_scores_2 = torch.empty(inputs.shape[0])\n",
    "# attention_scores_2.to(device)\n",
    "for idx, x_i in enumerate(inputs):\n",
    "    attention_scores_2[idx] = torch.dot(x_i, query)\n",
    "#    print(f\"Sequence Element [{idx}], attention_score: {attention_scores_2}\")\n",
    "print(f\"Final value of attention_score_2: {attention_scores_2}\")\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input sequence shape:  torch.Size([6, 3])\n",
      "Query is the 2nd word 'journey': tensor([0.5500, 0.8700, 0.6600])\n",
      "Final value of attention_score_2: tensor([0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865])\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "cell_type": "code",
   "id": "53ede1deb7d5678e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-01T20:32:01.479947Z",
     "start_time": "2025-02-01T20:32:01.473626Z"
    }
   },
   "source": [
    "\n",
    "## Note: For all elements if we were to calculate attention it'd be a O(n^2) operation\n",
    "# NOW we normalize the attention weights, so they sum up to 1\n",
    "attention_weights_2_tmp = attention_scores_2 / attention_scores_2.sum()\n",
    "# attention_weights_2_tmp.to(device)\n",
    "print(\"Normalized attention weights:\", attention_weights_2_tmp)\n",
    "print(\"Sum of attention weights:\", attention_weights_2_tmp.sum())\n",
    "\n",
    "## Generally we normalize using the softmax to do the normalization\n",
    "\n",
    "# # define a softmax function\n",
    "def softmax_naive(tensor_x):\n",
    "    return torch.exp(tensor_x) / torch.exp(tensor_x).sum(dim=0, keepdim=True)\n",
    "# \n",
    "\n",
    "attention_scores_2_naive = softmax_naive(attention_scores_2)\n",
    "# attention_scores_2_naive.to(device)\n",
    "\n",
    "print(\"Attention weights naive:\", attention_scores_2_naive)\n",
    "print (\"Naive Sum: \", attention_scores_2_naive.sum())\n",
    "# \n",
    "# Generally we normalize using the torch.softmax() to do the normalization\n",
    "# Softmax ensures its always positive and always adds up to 1\n",
    "#\n",
    "attention_weights_2_torch_softmax = torch.softmax(attention_scores_2, dim=0)\n",
    "# attention_weights_2_torch_softmax.to(device)\n",
    "print(\"Attention weights torch softmax:\", attention_weights_2_torch_softmax)\n",
    "# print(\"Attention weights torch softmax Sum: \", attention_weights_2_torch_softmax.sum())\n",
    "\n",
    "# Now that we have calculated the normalized attention weights, we are ready for the final step.\n",
    "# Calculate the context vector z(2) by multiplying the embedded input tokens x(i), \n",
    "# with the corresponding normalized attention weights and then summing the resultant vectors\n",
    "#\n",
    "query = inputs[1]\n",
    "# query.to(device)\n",
    "#\n",
    "context_vec_2 = torch.zeros(query.shape)\n",
    "# context_vec_2.to(device)\n",
    "#\n",
    "for i, x_i in enumerate(inputs):\n",
    "    context_vec_2 += (attention_weights_2_torch_softmax[i] * x_i)\n",
    "print(\"Context vector z2: \", context_vec_2)\n",
    "\n",
    "#\n",
    "# Now in similar fashion lets calculate attention scores for all the input sequences \n",
    "attention_scores = torch.empty(inputs.shape[0],inputs.shape[0])\n",
    "# attention_scores.to(device)\n",
    "print(\"\\nAttention Scores matrix shape: \", attention_scores.shape)\n",
    "#\n",
    "# Using for loops\n",
    "#\n",
    "# for i, x_i in enumerate(inputs):\n",
    "#     for j, x_j in enumerate(inputs):\n",
    "#         attention_scores[i, j] = torch.dot(x_i, x_j)\n",
    "# #\n",
    "#print(attention_scores)\n",
    "#\n",
    "# Using matrix multiplication we can do it faster\n",
    "#\n",
    "attention_scores_m = inputs @ inputs.T\n",
    "# attention_scores_m.to(device)\n",
    "#print(\"Normalized attention scores \\n\", attention_scores_m)\n",
    "\n",
    "# Just as before lets normalize the rows, so they sum up to 1\n",
    "# NOTE: Here dim = -1 means we are applying the softmax along the last dimension of the attention_scores_m tensor\n",
    "#\n",
    "attention_weights = torch.softmax(attention_scores_m, dim=-1)\n",
    "# attention_weights.to(device)\n",
    "#print(\"Normalized ATTENTION weights \\n\", attention_weights)\n",
    "# print(\"Softmax Sums:\\n\", attention_weights.sum(dim=-1))\n",
    "\n",
    "# FINAL STEP\n",
    "# Now let's calculate the context vectors for all the input by multiplying the input with attention weights\n",
    "all_context_vectors = attention_weights @ inputs # Matrix multiplication\n",
    "# all_context_vectors.to(device)\n",
    "#print(\"Context vector for the entire sequence\\n\", all_context_vectors)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalized attention weights: tensor([0.1455, 0.2278, 0.2249, 0.1285, 0.1077, 0.1656])\n",
      "Sum of attention weights: tensor(1.0000)\n",
      "Attention weights naive: tensor([0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581])\n",
      "Naive Sum:  tensor(1.)\n",
      "Attention weights torch softmax: tensor([0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581])\n",
      "Context vector z2:  tensor([0.4419, 0.6515, 0.5683])\n",
      "\n",
      "Attention Scores matrix shape:  torch.Size([6, 6])\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "cell_type": "code",
   "id": "fa980b537b01e646",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-01T20:32:01.486454Z",
     "start_time": "2025-02-01T20:32:01.480658Z"
    }
   },
   "source": [
    "\n",
    "###\n",
    "### 3.4.1 Using weighted matrix\n",
    "###\n",
    "#\n",
    "# Computing the attention weights step by step\n",
    "#\n",
    "torch.manual_seed(123)\n",
    "x_2 = inputs[1]\n",
    "# x_2.to(device)\n",
    "d_in = inputs.shape[1]\n",
    "d_out = 2\n",
    "print(\"x_2: \", x_2)\n",
    "# Now let's initialize 3 weighted matrices Wq, Wk and Wv\n",
    "# Setting requires_grad = False, to reduce clutter, but for model training this should be set to True\n",
    "\n",
    "W_query = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)\n",
    "W_key   = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)\n",
    "W_value = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)\n",
    "#\n",
    "# Next we compute the query, key and value vectors\n",
    "# Note the output is a 2 dimenstional vector because we set dout to 2\n",
    "#\n",
    "# W_query.to(device)\n",
    "# W_key.to(device)\n",
    "# W_value.to(device)\n",
    "#\n",
    "# Now the dot product with the input\n",
    "#\n",
    "query_2 = x_2 @ W_query\n",
    "key_2   = x_2 @ W_key\n",
    "value_2 = x_2 @ W_value\n",
    "#\n",
    "# query_2.to(device)\n",
    "# key_2.to(device)\n",
    "# value_2.to(device)\n",
    "#\n",
    "print(\"Query 2: \", query_2)\n",
    "print(\"Key 2:   \", key_2)\n",
    "print(\"Value 2: \", value_2)\n",
    "#\n",
    "print(\"\\n\")\n",
    "#\n",
    "\n",
    "keys = inputs @ W_key \n",
    "values = inputs @ W_value\n",
    "# keys.to(device)\n",
    "# values.to(device)\n",
    "print(\"keys.shape:\", keys.shape)\n",
    "print(\"values.shape:\", values.shape)\n",
    "\n",
    "keys_2 = keys[1]\n",
    "\n",
    "attn_score_22 = query_2.dot(keys_2)\n",
    "# attn_score_22.to(device)\n",
    "# attn_score_22 = query_2 @ keys_2\n",
    "print(\"Attention (dot) score 22:\", attn_score_22)\n",
    "\n",
    "# Generalizing across all inputs\n",
    "attn_scores_2 = query_2 @ keys.T\n",
    "# attn_scores_2.to(device)\n",
    "print(\"Attention \\\\@ Scores 2: \", attn_scores_2)\n",
    "# Check the second element is same as previously calculated attention score\n",
    "#\n",
    "# We compute the attention weights by scaling the attention scores and using the softmax function. \n",
    "# However, now we scale the attention scores by dividing them by the square root of the embedding \n",
    "# dimension of the keys\n",
    "\n",
    "attn_weights_2 = torch.softmax(attn_scores_2 / keys.shape[-1]**0.5, dim=-1)\n",
    "# attn_weights_2.to(device)\n",
    "print(\"attn_weights_2: \", attn_weights_2)\n",
    "\n",
    "# The reason for the normalization by square root of embedding dimension size is to improve the training performance by avoiding small gradients. \n",
    "# For instance, when scaling up the embedding dimension, which is typically > 1,000 for GPT-like LLMs, large dot products can result in \n",
    "# very small gradients during backpropagation (due to the softmax function applied to them). \n",
    "# As dot products increase, the softmax function behaves more like a step function, resulting in gradients nearing zero. \n",
    "# These small gradients can drastically slow down learning or cause training to stagnate.\n",
    "#\n",
    "# The scaling by the square root of the embedding dimension is the reason why this self-attention mechanism is also called scaled-dot product attention.\n",
    "# Similar to when we computed the context vector as a weighted sum over the input vectors \n",
    "# we now compute the context vector as a weighted sum over the value vectors. \n",
    "# Here, the attention weights serve as a weighting factor that weighs the respective importance of each value vector\n",
    "context_vec_2 = attn_weights_2 @ values\n",
    "# context_vec_2.to(device)\n",
    "print(\"context_vec_2: \", context_vec_2)\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_2:  tensor([0.5500, 0.8700, 0.6600])\n",
      "Query 2:  tensor([0.4306, 1.4551])\n",
      "Key 2:    tensor([0.4433, 1.1419])\n",
      "Value 2:  tensor([0.3951, 1.0037])\n",
      "\n",
      "\n",
      "keys.shape: torch.Size([6, 2])\n",
      "values.shape: torch.Size([6, 2])\n",
      "Attention (dot) score 22: tensor(1.8524)\n",
      "Attention \\@ Scores 2:  tensor([1.2705, 1.8524, 1.8111, 1.0795, 0.5577, 1.5440])\n",
      "attn_weights_2:  tensor([0.1500, 0.2264, 0.2199, 0.1311, 0.0906, 0.1820])\n",
      "context_vec_2:  tensor([0.3061, 0.8210])\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "cell_type": "code",
   "id": "2ca52ea3a908fe20",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-01T20:32:01.496383Z",
     "start_time": "2025-02-01T20:32:01.487182Z"
    }
   },
   "source": [
    "from src.chapter03.SelfAttention_v2 import SelfAttention_v2\n",
    "\n",
    "torch.manual_seed(789)\n",
    "self_attn_v2 = SelfAttention_v2(d_in, d_out)\n",
    "#print(\"Context vectors from SelfAtten_v2: \\n\", self_attn_v2(inputs))\n",
    "\n",
    "# Note since the input contains 6 embedding vectors, the output also has 6 rows of context vectors\n",
    "\n",
    "\n",
    "# Causal Attention \n",
    "# First we apply softmax to the attention scores then mask with 0 above the diagonal and then normalize the rows to 1\n",
    "#\n",
    "queries = self_attn_v2.W_query(inputs)\n",
    "# queries.to(device)\n",
    "\n",
    "keys = self_attn_v2.W_key(inputs)\n",
    "# keys.to(device)\n",
    "\n",
    "attn_scores = queries @ keys.T\n",
    "# attn_scores.to(device)\n",
    "\n",
    "attn_weights = torch.softmax(attn_scores / keys.shape[-1] ** 0.5, dim=-1)\n",
    "# attn_weights.to(device)\n",
    "#print(\"Attention Wrights: \\n\",attn_weights)\n",
    "\n",
    "# Now mask the values above diagonal as 0 using the tril() function\n",
    "#\n",
    "context_length = attn_scores.shape[0]\n",
    "#\n",
    "mask_simple = torch.tril(torch.ones(context_length, context_length))\n",
    "#print(\"Mask Simple: \\n\", mask_simple)\n",
    "#\n",
    "# Now simply multiply them to prevent the look ahead \n",
    "#\n",
    "masked_attention_weights = attn_weights * mask_simple\n",
    "# masked_attention_weights.to(device)\n",
    "#print(\"Masked attention weights: \\n\", masked_attention_weights)\n",
    "\n",
    "#\n",
    "# Now re-normalize to make sure rows add up to 1. To do this we divide each element by sum of each row\n",
    "#\n",
    "row_sums = masked_attention_weights.sum(dim=-1, keepdim=True)\n",
    "#print(\"row_sums: \\n\", row_sums)\n",
    "masked_simple_norm = masked_attention_weights / row_sums\n",
    "print(\"Masked & re-normalized weights: \\n\", masked_simple_norm)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x = tensor([1.], device='mps:0') using 'mps:0' backend\n",
      "Masked & re-normalized weights: \n",
      " tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.5517, 0.4483, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.3800, 0.3097, 0.3103, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2758, 0.2460, 0.2462, 0.2319, 0.0000, 0.0000],\n",
      "        [0.2175, 0.1983, 0.1984, 0.1888, 0.1971, 0.0000],\n",
      "        [0.1935, 0.1663, 0.1666, 0.1542, 0.1666, 0.1529]],\n",
      "       grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "cell_type": "code",
   "id": "f3a442b3d9b7413a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-01T20:32:01.501731Z",
     "start_time": "2025-02-01T20:32:01.497182Z"
    }
   },
   "source": [
    "# A more efficient way to obtain masked attention weights is to mask the attention scores with \n",
    "# negative infinity before applying softmax function. (e^negative infinity -> 0)\n",
    "# We can implement this masking by replacing values above the diagonal with 1 and then replacing them \n",
    "# with negative infinity\n",
    "#\n",
    "mask = torch.triu(torch.ones(context_length, context_length), diagonal=1)\n",
    "# mask.to(device)\n",
    "masked = attn_scores.masked_fill(mask.bool(), -torch.inf)\n",
    "# masked.to(device)\n",
    "print(\"Masked attention weights: \\n\", masked)\n",
    "#\n",
    "# Now apply the softmax function \n",
    "#\n",
    "attn_weights = torch.softmax(masked / keys.shape[-1] ** 0.5, dim=-1)\n",
    "print(\"Softmax'd Attention weights: \\n\", attn_weights)\n",
    "\n",
    "# Now we can use these modified attention weights to calculate the context vector\n",
    "#\n",
    "context_vec = attn_weights @ values\n",
    "print(\"context_vec: \\n\", context_vec)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Masked attention weights: \n",
      " tensor([[0.2899,   -inf,   -inf,   -inf,   -inf,   -inf],\n",
      "        [0.4656, 0.1723,   -inf,   -inf,   -inf,   -inf],\n",
      "        [0.4594, 0.1703, 0.1731,   -inf,   -inf,   -inf],\n",
      "        [0.2642, 0.1024, 0.1036, 0.0186,   -inf,   -inf],\n",
      "        [0.2183, 0.0874, 0.0882, 0.0177, 0.0786,   -inf],\n",
      "        [0.3408, 0.1270, 0.1290, 0.0198, 0.1290, 0.0078]],\n",
      "       grad_fn=<MaskedFillBackward0>)\n",
      "Softmax'd Attention weights: \n",
      " tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.5517, 0.4483, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.3800, 0.3097, 0.3103, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2758, 0.2460, 0.2462, 0.2319, 0.0000, 0.0000],\n",
      "        [0.2175, 0.1983, 0.1984, 0.1888, 0.1971, 0.0000],\n",
      "        [0.1935, 0.1663, 0.1666, 0.1542, 0.1666, 0.1529]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "context_vec: \n",
      " tensor([[0.1855, 0.8812],\n",
      "        [0.2795, 0.9361],\n",
      "        [0.3133, 0.9508],\n",
      "        [0.2994, 0.8595],\n",
      "        [0.2702, 0.7554],\n",
      "        [0.2772, 0.7618]], grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "cell_type": "code",
   "id": "d6b38e5f420f6067",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-01T20:32:01.506635Z",
     "start_time": "2025-02-01T20:32:01.502522Z"
    }
   },
   "source": [
    "# Masking additional weights with dropout\n",
    "# Drop out in the attention mechanism is applied at 2 specific times: \n",
    "# 1. After calculating the attention weights\n",
    "# 2. After applying the attention weights to value vectors\n",
    "# Here we will apply the dropout mask after computing the attention weights\n",
    "#\n",
    "# Lets use a dropout rate of 50% meaning half the attention weights will be masked out. \n",
    "# Normally it's a much lower rate like 0.1 or 0.2\n",
    "torch.manual_seed(123)\n",
    "dropout = torch.nn.Dropout(0.5)\n",
    "example = torch.ones(6, 6)\n",
    "print(dropout(example))\n",
    "#\n",
    "# Since we are applying 50% dropout, to compensate for reduction in active elements\n",
    "# we are going to scale up the values of remaining elements by a factor of 1/0.5 = 2\n",
    "# This scaling is crucial to maintain the balance of the attention weights\n",
    "#\n",
    "torch.manual_seed(123)\n",
    "print(\"Dropped out attention weights: \\n\", dropout(attn_weights))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2., 2., 0., 2., 2., 0.],\n",
      "        [0., 0., 0., 2., 0., 2.],\n",
      "        [2., 2., 2., 2., 0., 2.],\n",
      "        [0., 2., 2., 0., 0., 2.],\n",
      "        [0., 2., 0., 2., 0., 2.],\n",
      "        [0., 2., 2., 2., 2., 0.]])\n",
      "Dropped out attention weights: \n",
      " tensor([[2.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.7599, 0.6194, 0.6206, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.4921, 0.4925, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.3966, 0.0000, 0.3775, 0.0000, 0.0000],\n",
      "        [0.0000, 0.3327, 0.3331, 0.3084, 0.3331, 0.0000]],\n",
      "       grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "cell_type": "code",
   "id": "50701dd69a456857",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-01T20:32:01.511635Z",
     "start_time": "2025-02-01T20:32:01.507361Z"
    }
   },
   "source": [
    "from src.chapter03.CausalAttention import CausalAttention\n",
    "\n",
    "# Let’s ensure that the code can handle batches consisting of more than one input so that \n",
    "# the CausalAttention class supports the batch outputs produced by the data loader\n",
    "# To simulate batch input lets duplicate the input text\n",
    "#\n",
    "batch = torch.stack((inputs, inputs), dim=0)\n",
    "# batch.to(device)\n",
    "print(\"batch: \\n\", batch.shape)\n",
    "# print(batch)\n",
    "#\n",
    "# We can now use the CausalAttention class as follows\n",
    "#\n",
    "torch.manual_seed(123)\n",
    "context_layer = batch.shape[1]\n",
    "causal_attn = CausalAttention(d_in, d_out, context_length, 0.0, False)\n",
    "context_vecs = causal_attn(batch)\n",
    "print(\"context_vecs: \\n\", context_vecs.shape)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch: \n",
      " torch.Size([2, 6, 3])\n",
      "context_vecs: \n",
      " torch.Size([2, 6, 2])\n"
     ]
    }
   ],
   "execution_count": 20
  },
  {
   "cell_type": "code",
   "id": "9d8058957dc253b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-01T20:32:01.516946Z",
     "start_time": "2025-02-01T20:32:01.512346Z"
    }
   },
   "source": [
    "from src.chapter03.MultiHeadAttentionWrapper import MultiHeadAttentionWrapper\n",
    "\n",
    "# Multi Head Attention\n",
    "# Now if we use the MultiHeadAttentionWrapper class with two attention heads, and CausalAttention \n",
    "# output dimension d_out = 2, we get a 4 dimensional context vector (d_out * num_heads = 4).\n",
    "#\n",
    "torch.manual_seed(123)\n",
    "context_length = batch.shape[1]\n",
    "d_in, d_out = 3, 2\n",
    "multi_head_attn = MultiHeadAttentionWrapper(d_in, d_out, context_length, dropout=0.0, num_heads=2, qkv_bias=False)\n",
    "context_vecs = multi_head_attn(batch)\n",
    "print(\"context_vecs: \\n\", context_vecs)\n",
    "print(\"context_vecs.shape: \\n\", context_vecs.shape)\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "context_vecs: \n",
      " tensor([[[-0.4519,  0.2216,  0.4772,  0.1063],\n",
      "         [-0.5874,  0.0058,  0.5891,  0.3257],\n",
      "         [-0.6300, -0.0632,  0.6202,  0.3860],\n",
      "         [-0.5675, -0.0843,  0.5478,  0.3589],\n",
      "         [-0.5526, -0.0981,  0.5321,  0.3428],\n",
      "         [-0.5299, -0.1081,  0.5077,  0.3493]],\n",
      "\n",
      "        [[-0.4519,  0.2216,  0.4772,  0.1063],\n",
      "         [-0.5874,  0.0058,  0.5891,  0.3257],\n",
      "         [-0.6300, -0.0632,  0.6202,  0.3860],\n",
      "         [-0.5675, -0.0843,  0.5478,  0.3589],\n",
      "         [-0.5526, -0.0981,  0.5321,  0.3428],\n",
      "         [-0.5299, -0.1081,  0.5077,  0.3493]]], grad_fn=<CatBackward0>)\n",
      "context_vecs.shape: \n",
      " torch.Size([2, 6, 4])\n"
     ]
    }
   ],
   "execution_count": 21
  },
  {
   "cell_type": "code",
   "id": "f20088921d8113c0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-01T20:32:01.523326Z",
     "start_time": "2025-02-01T20:32:01.518605Z"
    }
   },
   "source": [
    "a = torch.tensor([[[[0.2745, 0.6584, 0.2775, 0.8573],\n",
    "                    [0.8993, 0.0390, 0.9268, 0.7388],\n",
    "                    [0.7179, 0.7058, 0.9156, 0.4340]],\n",
    "\n",
    "                   [[0.0772, 0.3565, 0.1479, 0.5331],\n",
    "                    [0.4066, 0.2318, 0.4545, 0.9737],\n",
    "                    [0.4606, 0.5159, 0.4220, 0.5786]]]])\n",
    "# print(a.transpose(2, 3))\n",
    "a.to(device)\n",
    "\n",
    "first_head = a[0, 0, :, :]\n",
    "first_res = first_head @ first_head.T\n",
    "print(\"First head:\\n\", first_res)\n",
    "\n",
    "second_head = a[0, 1, :, :]\n",
    "second_res = second_head @ second_head.T\n",
    "print(\"\\nSecond head:\\n\", second_res)\n",
    "\n",
    "print(f\"Batched: \\n{a @ a.transpose(2, 3)}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First head:\n",
      " tensor([[1.3208, 1.1631, 1.2879],\n",
      "        [1.1631, 2.2150, 1.8424],\n",
      "        [1.2879, 1.8424, 2.0402]])\n",
      "\n",
      "Second head:\n",
      " tensor([[0.4391, 0.7003, 0.5903],\n",
      "        [0.7003, 1.3737, 1.0620],\n",
      "        [0.5903, 1.0620, 0.9912]])\n",
      "Batched: \n",
      "tensor([[[[1.3208, 1.1631, 1.2879],\n",
      "          [1.1631, 2.2150, 1.8424],\n",
      "          [1.2879, 1.8424, 2.0402]],\n",
      "\n",
      "         [[0.4391, 0.7003, 0.5903],\n",
      "          [0.7003, 1.3737, 1.0620],\n",
      "          [0.5903, 1.0620, 0.9912]]]])\n"
     ]
    }
   ],
   "execution_count": 22
  },
  {
   "cell_type": "code",
   "id": "df6a05dfc13ebbe4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-01T20:32:01.527893Z",
     "start_time": "2025-02-01T20:32:01.523959Z"
    }
   },
   "source": [
    "from src.chapter03.MultiHeadAttention import MultiHeadAttention\n",
    "\n",
    "torch.manual_seed(123)\n",
    "batch_size, context_length, d_in = batch.shape\n",
    "print(\"Batch Shape: \\n\", batch.shape)\n",
    "d_out = 2\n",
    "mha = MultiHeadAttention(d_in, d_out, context_length, 0.0, num_heads=2)\n",
    "context_vecs = mha(batch)\n",
    "print(context_vecs)\n",
    "print(\"Context vector shape: \\n\", context_vecs.shape)\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Shape: \n",
      " torch.Size([2, 6, 3])\n",
      "tensor([[[0.3190, 0.4858],\n",
      "         [0.2943, 0.3897],\n",
      "         [0.2856, 0.3593],\n",
      "         [0.2693, 0.3873],\n",
      "         [0.2639, 0.3928],\n",
      "         [0.2575, 0.4028]],\n",
      "\n",
      "        [[0.3190, 0.4858],\n",
      "         [0.2943, 0.3897],\n",
      "         [0.2856, 0.3593],\n",
      "         [0.2693, 0.3873],\n",
      "         [0.2639, 0.3928],\n",
      "         [0.2575, 0.4028]]], grad_fn=<ViewBackward0>)\n",
      "Context vector shape: \n",
      " torch.Size([2, 6, 2])\n"
     ]
    }
   ],
   "execution_count": 23
  },
  {
   "cell_type": "markdown",
   "id": "3f958161a85ca549",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "## Chapter 4: Implementing GPT from Scratch to generate text\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "id": "16f34621162871e9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-01T20:32:01.530314Z",
     "start_time": "2025-02-01T20:32:01.528530Z"
    }
   },
   "source": [
    "GPT_CONFIG_124M = {\n",
    "    \"vocab_size\": 50257,     # Vocabulary size\n",
    "    \"context_length\": 1024,  # Context length\n",
    "    \"emb_dim\": 768,          # Embedding dimension\n",
    "    \"n_heads\": 12,           # Number of attention heads\n",
    "    \"n_layers\": 12,          # Number of layers\n",
    "    \"drop_rate\": 0.1,        # Dropout rate\n",
    "    \"qkv_bias\": False,\n",
    "    \"model_name\": \"GPTModel\",\n",
    "    \"lr\": 5e-4,\n",
    "    \"weight_decay\": 0.1\n",
    "}"
   ],
   "outputs": [],
   "execution_count": 24
  },
  {
   "cell_type": "markdown",
   "id": "5ac1cbf145ad6daf",
   "metadata": {},
   "source": [
    "## Here is the proposed architecture and order of implementation\n",
    "![image](../data/4-3.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "75010152b67235a2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-01T20:32:02.174956Z",
     "start_time": "2025-02-01T20:32:01.530792Z"
    }
   },
   "source": [
    "from src.chapter04.DummyGPTModel import DummyGPTModel\n",
    "import tiktoken\n",
    "#\n",
    "torch.manual_seed(123)\n",
    "#\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "batch = []\n",
    "txt1 = \"Every effort moves you\"\n",
    "txt2 = \"Every day holds a\"\n",
    "batch.clear()\n",
    "batch.append(torch.tensor(tokenizer.encode(txt1)).to(device))\n",
    "batch.append(torch.tensor(tokenizer.encode(txt2)).to(device))\n",
    "batch = torch.stack(batch, dim=0).to(device)\n",
    "batch.to(device)\n",
    "print(\"Input Batch: \\n\", batch)\n",
    "print(\"Input batch shape: \\n\", batch.shape)\n",
    "#\n",
    "# Next, we initialize a new 124-million-parameter DummyGPTModel instance \n",
    "# and feed it the tokenized batch\n",
    "#\n",
    "model = DummyGPTModel(GPT_CONFIG_124M)\n",
    "model.to(device)\n",
    "logits = model(batch)\n",
    "print(\"Output shape: \\n\", logits.shape)\n",
    "#print(logits)\n",
    "#                      \n",
    "#"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Batch: \n",
      " tensor([[6109, 3626, 6100,  345],\n",
      "        [6109, 1110, 6622,  257]], device='mps:0')\n",
      "Input batch shape: \n",
      " torch.Size([2, 4])\n",
      "Output shape: \n",
      " torch.Size([2, 4, 50257])\n"
     ]
    }
   ],
   "execution_count": 25
  },
  {
   "cell_type": "code",
   "id": "c06060ecd0f8f8be",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-01T20:32:02.181689Z",
     "start_time": "2025-02-01T20:32:02.176239Z"
    }
   },
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "# Let’s now implement LAYER Normalization to improve the stability and efficiency of the training.\n",
    "# The main idea behind LAYER Normalization is to adjust the activations (outputs) of a deep\n",
    "# neural network layer to have a mean of 0 and a variance of 1\n",
    "#\n",
    "# This adjustment speeds up the convergence.\n",
    "#\n",
    "torch.manual_seed(123)\n",
    "torch.set_printoptions(sci_mode=False)\n",
    "#\n",
    "batch_example = torch.randn(2, 5)\n",
    "layer = nn.Sequential(nn.Linear(5,6), nn.ReLU())\n",
    "out = layer(batch_example)\n",
    "print(\"Layer: \\n\",out)\n",
    "#\n",
    "# The NN Layer contains the non-linear activation ReLU which 0's out the negative values\n",
    "# \n",
    "mean = out.mean(dim=-1, keepdim=True)\n",
    "var = out.var(dim=-1, keepdim=True)\n",
    "print(\"Mean: \", mean)\n",
    "print(\"Variance: \", var)\n",
    "print(\"\\n\")\n",
    "#\n",
    "# Next, let’s apply layer normalization to the layer outputs we obtained earlier. \n",
    "# The operation consists of subtracting the mean and dividing by the square root \n",
    "# of the variance (also known as the standard deviation):\n",
    "#\n",
    "eps = 1e-5\n",
    "out_norm = (out - mean) / torch.sqrt(var)\n",
    "# Dim = -1 indicates statistics along the last dimention\n",
    "mean = out_norm.mean(dim=-1, keepdim=True)\n",
    "var = out_norm.var(dim=-1, keepdim=True)\n",
    "torch.set_printoptions(sci_mode=True)\n",
    "print(\"Normalized Layer Outputs: \\n\", out_norm)\n",
    "print(\"Mean: \\n\", mean)\n",
    "print(\"Variance: \\n\", var)\n",
    "print(\"-------------------------\\n\")\n",
    "\n",
    "torch.set_printoptions(sci_mode=False)\n",
    "print(\"Mean:\\n\", mean)\n",
    "print(\"Variance:\\n\", var)\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: \n",
      " tensor([[0.2260, 0.3470, 0.0000, 0.2216, 0.0000, 0.0000],\n",
      "        [0.2133, 0.2394, 0.0000, 0.5198, 0.3297, 0.0000]],\n",
      "       grad_fn=<ReluBackward0>)\n",
      "Mean:  tensor([[0.1324],\n",
      "        [0.2170]], grad_fn=<MeanBackward1>)\n",
      "Variance:  tensor([[0.0231],\n",
      "        [0.0398]], grad_fn=<VarBackward0>)\n",
      "\n",
      "\n",
      "Normalized Layer Outputs: \n",
      " tensor([[6.1585e-01, 1.4126e+00, -8.7188e-01, 5.8723e-01, -8.7188e-01, -8.7188e-01],\n",
      "        [-1.8865e-02, 1.1211e-01, -1.0876e+00, 1.5173e+00, 5.6474e-01, -1.0876e+00]],\n",
      "       grad_fn=<DivBackward0>)\n",
      "Mean: \n",
      " tensor([[-5.9605e-08],\n",
      "        [ 1.9868e-08]], grad_fn=<MeanBackward1>)\n",
      "Variance: \n",
      " tensor([[1.0000e+00],\n",
      "        [1.0000e+00]], grad_fn=<VarBackward0>)\n",
      "-------------------------\n",
      "\n",
      "Mean:\n",
      " tensor([[    -0.0000],\n",
      "        [     0.0000]], grad_fn=<MeanBackward1>)\n",
      "Variance:\n",
      " tensor([[1.0000],\n",
      "        [1.0000]], grad_fn=<VarBackward0>)\n"
     ]
    }
   ],
   "execution_count": 26
  },
  {
   "cell_type": "code",
   "id": "a39e430a037896f0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-01T20:32:02.185553Z",
     "start_time": "2025-02-01T20:32:02.182305Z"
    }
   },
   "source": [
    "from src.chapter04.LayerNorm import LayerNorm\n",
    "\n",
    "# Previously we used unbiased = False in our variance calculation. This doesn't \n",
    "# apply Bessel's correction where divisor is n-1 instead of n. But this is \n",
    "# compatible with GPT-2\n",
    "\n",
    "ln = LayerNorm(emb_dim = 5)\n",
    "out_ln = ln(batch_example)\n",
    "mean = out_ln.mean(dim=-1, keepdim=True)\n",
    "var = out_ln.var(dim=-1, unbiased=False, keepdim=True)\n",
    "print(\"Mean: \\n\", mean)\n",
    "print(\"Variance: \\n\", var)\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean: \n",
      " tensor([[    -0.0000],\n",
      "        [     0.0000]], grad_fn=<MeanBackward1>)\n",
      "Variance: \n",
      " tensor([[1.0000],\n",
      "        [1.0000]], grad_fn=<VarBackward0>)\n"
     ]
    }
   ],
   "execution_count": 27
  },
  {
   "cell_type": "code",
   "id": "3130c2c83093ee01",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-01T20:32:02.526455Z",
     "start_time": "2025-02-01T20:32:02.186283Z"
    }
   },
   "source": [
    "# Let us see how the GELU (Gaussian Error Linear Unit) stacks up against \n",
    "# # RELU (REctified Linear Unit)\n",
    "from src.chapter04.GELU import GELU\n",
    "import matplotlib.pyplot as plt\n",
    "gelu, relu = GELU(), nn.ReLU()\n",
    "\n",
    "x = torch.linspace(-3, 3, 100)\n",
    "y_gelu, y_relu = gelu(x), relu(x)\n",
    "plt.figure(figsize=(8, 3))\n",
    "for i, (y, label) in enumerate(zip([y_gelu, y_relu], [\"GELU\", \"ReLU\"]), 1):\n",
    "    plt.subplot(1, 2, i)\n",
    "    plt.plot(x, y)\n",
    "    plt.title(f\"{label} activation function\")\n",
    "    plt.xlabel(\"x\")\n",
    "    plt.ylabel(f\"{label}(x)\")\n",
    "    plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 800x300 with 2 Axes>"
      ],
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 28
  },
  {
   "cell_type": "code",
   "id": "e496e641f9044a67",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-01T20:32:02.548576Z",
     "start_time": "2025-02-01T20:32:02.527666Z"
    }
   },
   "source": [
    "from src.chapter04.SimpleFeedForward import SimpleFeedForward\n",
    "\n",
    "# As we can see the smoothness of the GELU can lead to better optimization properties during training\n",
    "# as it allows more nuanced finer adjustments to models parameters. In contrast, RELU has a sharp corner\n",
    "# that can make adjustments difficult for very deep networks.\n",
    "#\n",
    "# Next we look at implementing a feed forward network with GELU activations\n",
    "# See SimpleFeedForward.py\n",
    "#\n",
    "sff = SimpleFeedForward(GPT_CONFIG_124M)\n",
    "x = torch.rand(2, 3, 768)\n",
    "out = sff(x)\n",
    "print(out.shape)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 768])\n"
     ]
    }
   ],
   "execution_count": 29
  },
  {
   "cell_type": "code",
   "id": "98a59bc5db854fa4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-01T20:32:02.555523Z",
     "start_time": "2025-02-01T20:32:02.549509Z"
    }
   },
   "source": [
    "from src.chapter04.ExampleDeepNeuralNetwork import ExampleDeepNeuralNetwork\n",
    "\n",
    "# Next we implement Shortcut Connections\n",
    "# Each layer will be initialized such that it accepts an example with three input \n",
    "# values and returns three output values.\n",
    "torch.manual_seed(123)\n",
    "layer_sizes = [3, 3, 3, 3, 3, 1]\n",
    "sample_input = torch.tensor([[1., 0., -1.]])\n",
    "model_without_shortcut = ExampleDeepNeuralNetwork(layer_sizes, use_shortcut=False)\n",
    "# model.to(device)\n",
    "\n",
    "# Next lets print the gradients\n",
    "def print_gradients(nnmodel, input_x):\n",
    "    output = nnmodel(input_x)\n",
    "    target = torch.tensor([[0.]])\n",
    "    loss = nn.MSELoss()\n",
    "    loss = loss(output, target)\n",
    "    torch.autograd.set_detect_anomaly(True)\n",
    "    loss.backward()\n",
    "    for name, param in nnmodel.named_parameters():\n",
    "        # print(name, \" = \", param)\n",
    "        if 'weight' in name:\n",
    "            print(f\"{name} has gradient mean of {param.grad.abs().mean().item()}\")\n",
    "\n",
    "#\n",
    "# Now Lets use this function to print the gradients calculated by loss.backward()\n",
    "print_gradients(model_without_shortcut, sample_input)\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layers.0.0.weight has gradient mean of 0.00020173587836325169\n",
      "layers.1.0.weight has gradient mean of 0.0001201116101583466\n",
      "layers.2.0.weight has gradient mean of 0.0007152041071094573\n",
      "layers.3.0.weight has gradient mean of 0.0013988735154271126\n",
      "layers.4.0.weight has gradient mean of 0.005049645435065031\n"
     ]
    }
   ],
   "execution_count": 30
  },
  {
   "cell_type": "code",
   "id": "4bfdfab7cb5bcc5f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-01T20:32:02.572497Z",
     "start_time": "2025-02-01T20:32:02.559065Z"
    }
   },
   "source": [
    "# As you can see above gradients become tiny aka Vanishing from Layer4 to Layer1\n",
    "# Let’s now instantiate a model with skip connections and see how it compares:\n",
    "torch.manual_seed(123)\n",
    "layer_sizes = [3, 3, 3, 3, 3, 1]\n",
    "sample_input = torch.tensor([[1., 0., -1.]])\n",
    "model_with_shortcut = ExampleDeepNeuralNetwork(layer_sizes, use_shortcut=True)\n",
    "# model.to(device)\n",
    "print_gradients(model_with_shortcut, sample_input)\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layers.0.0.weight has gradient mean of 0.22169791162014008\n",
      "layers.1.0.weight has gradient mean of 0.20694106817245483\n",
      "layers.2.0.weight has gradient mean of 0.32896995544433594\n",
      "layers.3.0.weight has gradient mean of 0.2665732204914093\n",
      "layers.4.0.weight has gradient mean of 1.3258540630340576\n"
     ]
    }
   ],
   "execution_count": 31
  },
  {
   "cell_type": "code",
   "id": "a229081ed60e29b5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-01T20:32:02.575489Z",
     "start_time": "2025-02-01T20:32:02.573281Z"
    }
   },
   "source": [
    "# Note here the gradient doesn't approach a vanishingly small value during backprop.\n",
    "# In conclusion, shortcut connections are important for overcoming the limitations posed \n",
    "# by the vanishing gradient problem in deep neural networks."
   ],
   "outputs": [],
   "execution_count": 32
  },
  {
   "cell_type": "markdown",
   "id": "2df6d339f0570629",
   "metadata": {},
   "source": [
    "#### Next, we’ll connect all the previously covered concepts (layer normalization, GELU activations, feed forward module, and shortcut connections) in a transformer  block, which is the final building block we need to code the GPT architecture.\n",
    "\n",
    "![image](../data/transformer_wiring.png)"
   ]
  },
  {
   "cell_type": "code",
   "id": "cd526dd3047ba477",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-01T20:32:02.628099Z",
     "start_time": "2025-02-01T20:32:02.576102Z"
    }
   },
   "source": [
    "# See TransformerBlock.py for the basic sequence and feedforward details\n",
    "from src.chapter04.TransformerBlock import TransformerBlock\n",
    "\n",
    "torch.manual_seed(123)\n",
    "x = torch.rand(2, 4, 768)\n",
    "tr_block = TransformerBlock(GPT_CONFIG_124M)\n",
    "out = tr_block(x)\n",
    "#\n",
    "print(\"Input Shape:  \", x.shape)\n",
    "print(\"Output Shape: \", out.shape)\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Shape:   torch.Size([2, 4, 768])\n",
      "Output Shape:  torch.Size([2, 4, 768])\n"
     ]
    }
   ],
   "execution_count": 33
  },
  {
   "cell_type": "code",
   "id": "8c3e76bf70259f86",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-01T20:32:03.908195Z",
     "start_time": "2025-02-01T20:32:02.629204Z"
    }
   },
   "source": [
    "from src.chapter04.GPTModel import GPTModel \n",
    "\n",
    "# Let us wire up the actual GPT Model we wrote now\n",
    "\n",
    "torch.manual_seed(123)\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "model.to(device)\n",
    "print(\"Input batch: \", batch)\n",
    "out = model(batch)\n",
    "print(\"Output shape: \", out.shape)\n",
    "# print(\"Out: \\n\", out)\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input batch:  tensor([[6109, 3626, 6100,  345],\n",
      "        [6109, 1110, 6622,  257]], device='mps:0')\n",
      "Output shape:  torch.Size([2, 4, 50257])\n"
     ]
    }
   ],
   "execution_count": 34
  },
  {
   "cell_type": "code",
   "id": "af55c75851ac7e89",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-01T20:32:03.911295Z",
     "start_time": "2025-02-01T20:32:03.909041Z"
    }
   },
   "source": [
    "# Note above the output tensor has the shape [2, 4, 50257], since we passed in two input texts (the two sentences) \n",
    "# with four tokens each. The last dimension, 50257, corresponds to the vocabulary size of the tokenizer.\n",
    "#\n",
    "# To capture the total number of Parameters for a model use numel parameter value\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Total number of parameters: {total_params:,}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of parameters: 163,009,536\n"
     ]
    }
   ],
   "execution_count": 35
  },
  {
   "cell_type": "markdown",
   "id": "ba624b8ce1a7e0a0",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "id": "1a31524b0cdc6ebf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-01T20:32:03.914053Z",
     "start_time": "2025-02-01T20:32:03.912084Z"
    }
   },
   "source": [
    "# Weight Tying: The model reuses weights from the token embedding layer in its output layer\n",
    "print(\"Token embedding layer shape:\", model.tok_emb.weight.shape)\n",
    "print(\"Output layer shape:\", model.out_head.weight.shape)\n",
    "# As we can see both shapes are same"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token embedding layer shape: torch.Size([50257, 768])\n",
      "Output layer shape: torch.Size([50257, 768])\n"
     ]
    }
   ],
   "execution_count": 36
  },
  {
   "cell_type": "code",
   "id": "197e0a5f1199aada",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-01T20:32:03.916940Z",
     "start_time": "2025-02-01T20:32:03.914635Z"
    }
   },
   "source": [
    "# The token embedding and output layers are very large due to the 50,257 rows in the tokenizer’s vocabulary. \n",
    "# If we remove the output layer parameter count from the total GPT-2 model count :\n",
    "total_params_gptmodel = (\n",
    "    total_params - sum(p.numel() for p in model.out_head.parameters())\n",
    ")\n",
    "print(f\"Total number of trainable parameters considering weight tying: \"\n",
    "      f\"{total_params_gptmodel:,}\")\n",
    "\n",
    "# Memory Requirement\n",
    "total_size_bytes = total_params * 4\n",
    "total_size_mb = total_size_bytes / (1024 * 1024)\n",
    "print(f\"Total size of the model: {total_size_mb:.2f} MB\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of trainable parameters considering weight tying: 124,412,160\n",
      "Total size of the model: 621.83 MB\n"
     ]
    }
   ],
   "execution_count": 37
  },
  {
   "cell_type": "markdown",
   "id": "82b6b3cc8d05feb4",
   "metadata": {},
   "source": [
    "# Generating text \n",
    "#### We will now write code to generate text from the predicted tensors by the GPTModel\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "7da3f91bddda842d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-01T20:32:04.044105Z",
     "start_time": "2025-02-01T20:32:03.917503Z"
    }
   },
   "source": [
    "def generate_text_simple(input_model, tokenids, max_new_tokens, context_size):\n",
    "    for _ in range(max_new_tokens):\n",
    "        token_cond = tokenids[:, -context_size:]\n",
    "        with torch.no_grad():\n",
    "            logits = input_model(token_cond)\n",
    "\n",
    "        logits = logits[:, -1, :]\n",
    "        probabs = torch.softmax(logits, dim=-1)\n",
    "        next_token = torch.argmax(probabs, dim=-1, keepdim=True)\n",
    "        tokenids = torch.cat((tokenids, next_token), dim=1)\n",
    "\n",
    "        return tokenids\n",
    "#    \n",
    "#  Try it with a sample sentence  \n",
    "#    \n",
    "start_context = \"Hello, I am \"\n",
    "encoded = tokenizer.encode(start_context)\n",
    "print(\"encoded: \", encoded)\n",
    "#\n",
    "encoded_tensor = torch.tensor(encoded).to(device).unsqueeze(0)\n",
    "print(\"encoded_tensor.shape: \", encoded_tensor.shape)\n",
    "#\n",
    "model.eval()\n",
    "out = generate_text_simple(\n",
    "    input_model=model,\n",
    "    tokenids=encoded_tensor, \n",
    "    max_new_tokens=6, \n",
    "    context_size=GPT_CONFIG_124M[\"context_length\"]\n",
    ")\n",
    "print(\"Output:\", out)\n",
    "print(\"Output length:\", len(out[0]))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoded:  [15496, 11, 314, 716, 220]\n",
      "encoded_tensor.shape:  torch.Size([1, 5])\n",
      "Output: tensor([[15496,    11,   314,   716,   220, 24464]], device='mps:0')\n",
      "Output length: 6\n"
     ]
    }
   ],
   "execution_count": 38
  },
  {
   "cell_type": "code",
   "id": "ab7699859ab25da6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-01T20:32:04.047016Z",
     "start_time": "2025-02-01T20:32:04.044680Z"
    }
   },
   "source": [
    "decoded_text = tokenizer.decode(out.squeeze(0).tolist())\n",
    "print(decoded_text)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, I am opia\n"
     ]
    }
   ],
   "execution_count": 39
  },
  {
   "cell_type": "markdown",
   "id": "7397b4b87caf10c6",
   "metadata": {},
   "source": [
    "# Chapter 5: Pretraining on unlabeled data\n",
    "#### Using GPT to generate text"
   ]
  },
  {
   "cell_type": "code",
   "id": "21d8324b6eace0d0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-01T20:32:04.309856Z",
     "start_time": "2025-02-01T20:32:04.047590Z"
    }
   },
   "source": [
    "# How to calculate loss and relatively randomize next word prediction\n",
    "#\n",
    "inputs = torch.tensor([[16833, 3626, 6100],   # [\"every effort moves\",\n",
    "                       [40,    1107, 588]]).to(device)   #  \"I really like\"\n",
    "#\n",
    "targets = torch.tensor([[3626, 6100, 345  ],  # [\" effort moves you\",\n",
    "                        [1107, 588, 11311]]).to(device)  #  \" really like chocolate\"\n",
    "\n",
    "with torch.no_grad():\n",
    "    logits = model(inputs)\n",
    "probas = torch.softmax(logits, dim=-1)\n",
    "print(f\"probas: {probas.shape}\")\n",
    "\n",
    "token_ids = torch.argmax(probas, dim=-1, keepdim=True)\n",
    "print(f\"Token IDs: {token_ids}\")\n",
    "\n",
    "# Utility function\n",
    "def text_to_token_ids(txt, tokenizr):\n",
    "    encoded_txt = tokenizr.encode(txt, allowed_special={'<|endoftext|>'})\n",
    "    encoded_tnsr = torch.tensor(encoded_txt).to(device).unsqueeze(0)\n",
    "    return encoded_tnsr\n",
    "\n",
    "# Utility function\n",
    "def token_ids_to_text(tokenids, tokenizr):\n",
    "    flat = tokenids.squeeze(0)\n",
    "    return tokenizr.decode(flat.tolist())\n",
    "\n",
    "\n",
    "print(f\"Targets batch 1: {token_ids_to_text(targets[0], tokenizer)}\")\n",
    "print(f\"Outputs batch 1: {token_ids_to_text(token_ids[0].flatten(), tokenizer)}\")\n",
    "\n",
    "torch.set_printoptions(sci_mode=True)\n",
    "text_idx = 0\n",
    "target_probas_1 = probas[text_idx, [0, 1, 2], targets[text_idx]]\n",
    "print(\"Softmax scores for Text 1:\", target_probas_1)\n",
    "\n",
    "text_idx = 1\n",
    "target_probas_2 = probas[text_idx, [0, 1, 2], targets[text_idx]]\n",
    "print(\"Softmax scores for Text 2: \", target_probas_2)\n",
    "\n",
    "# The goal of training an LLM is to maximize the likelihood of the correct token, \n",
    "# which involves increasing its probability relative to other tokens. \n",
    "\n",
    "# Loss of probabilities for the two batches are\n",
    "log_probas = torch.log(torch.cat((target_probas_1, target_probas_2)))\n",
    "print(\"Log probabilities for both batches are: \\n\", log_probas)\n",
    "\n",
    "# Next, we combine the log probabilities into a single score by computing \n",
    "# the average\n",
    "avg_log_probas = torch.mean(log_probas)\n",
    "print(f\"Average log probability: {avg_log_probas}\")\n",
    "\n",
    "# However, in deep learning, the common practice isn’t to push the average log probability \n",
    "# up to 0 but rather to bring the negative average log probability down to 0. The negative \n",
    "# average log probability is simply the average log probability multiplied by –1\n",
    "neg_avg_log_probabs = avg_log_probas * -1\n",
    "print(f\"Negative of average log probability: {neg_avg_log_probabs}\")\n",
    "\n",
    "# In deep learning, the term for turning this negative value, –10.7940, into 10.7940, \n",
    "# is known as the cross entropy loss.\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "probas: torch.Size([2, 3, 50257])\n",
      "Token IDs: tensor([[[36397],\n",
      "         [39619],\n",
      "         [20610]],\n",
      "\n",
      "        [[ 8615],\n",
      "         [49289],\n",
      "         [47105]]], device='mps:0')\n",
      "Targets batch 1:  effort moves you\n",
      "Outputs batch 1:  Gathering SerbianFriday\n",
      "Softmax scores for Text 1: tensor([2.3466e-05, 2.0531e-05, 1.1733e-05], device='mps:0')\n",
      "Softmax scores for Text 2:  tensor([4.2794e-05, 1.6248e-05, 1.1586e-05], device='mps:0')\n",
      "Log probabilities for both batches are: \n",
      " tensor([-1.0660e+01, -1.0794e+01, -1.1353e+01, -1.0059e+01, -1.1028e+01, -1.1366e+01],\n",
      "       device='mps:0')\n",
      "Average log probability: -10.876513481140137\n",
      "Negative of average log probability: 10.876513481140137\n"
     ]
    }
   ],
   "execution_count": 40
  },
  {
   "cell_type": "markdown",
   "id": "2e39970739c5f593",
   "metadata": {},
   "source": [
    "#### NOTE: Our goal during training is to get the average log probability as close to 0 as possible by updating the model’s weights"
   ]
  },
  {
   "cell_type": "code",
   "id": "62d971c360d51b5b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-01T20:32:05.368893Z",
     "start_time": "2025-02-01T20:32:04.316035Z"
    }
   },
   "source": [
    "GPT_CONFIG_124M_2 = {\n",
    "    \"vocab_size\": 50257,\n",
    "    \"context_length\": 256,\n",
    "    \"emb_dim\": 768,\n",
    "    \"n_heads\": 12,\n",
    "    \"n_layers\": 12, \n",
    "    \"drop_rate\": 0.1,\n",
    "    \"qkv_bias\": False,\n",
    "    \"model_name\": \"GPTModel\",\n",
    "    \"lr\": 5e-4,\n",
    "    \"weight_decay\": 0.1\n",
    "}\n",
    "torch.manual_seed(123)\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "model.to(torch.device(\"mps\"))\n",
    "model.eval()\n",
    "\n",
    "\n",
    "\n",
    "start_context = \"Every effort moves you\"\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "token_ids = generate_text_simple(\n",
    "    input_model=model,\n",
    "    tokenids=text_to_token_ids(start_context, tokenizer),\n",
    "    max_new_tokens=10,\n",
    "    context_size=GPT_CONFIG_124M[\"context_length\"]\n",
    ")\n",
    "print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output text:\n",
      " Every effort moves you Ae\n"
     ]
    }
   ],
   "execution_count": 41
  },
  {
   "cell_type": "code",
   "id": "d0465f554d8c18b6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-01T20:32:05.388767Z",
     "start_time": "2025-02-01T20:32:05.369638Z"
    }
   },
   "source": [
    "print(\"Logits shape:\", logits.shape) # batch size, num of tokens, vocab size\n",
    "print(\"Targets shape:\", targets.shape) # batch size, num of tokens\n",
    "\n",
    "# For the cross_entropy loss function in PyTorch, we want to flatten these \n",
    "# tensors by combining them over the batch dimension:\n",
    "logits_flat = logits.flatten(0, 1)\n",
    "targets_flat = targets.flatten()\n",
    "print(\"Flattened logits:\", logits_flat.shape)\n",
    "print(\"Flattened targets:\", targets_flat.shape)\n",
    "\n",
    "# Now we can call CE from torch to calculate the loss\n",
    "celoss = torch.nn.functional.cross_entropy(logits_flat, targets_flat)\n",
    "print(f\"Cross Entropy loss: {celoss}\")\n",
    "\n",
    "# Perplexity measures how well the probability distribution predicted by the model \n",
    "# matches the actual distribution of the words in the dataset. Similar to the loss, \n",
    "# a lower perplexity means the model predictions are closer to the actual distribution.\n",
    "perplexity = torch.exp(celoss)\n",
    "print(f\"Perplexity: {perplexity}\")\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logits shape: torch.Size([2, 3, 50257])\n",
      "Targets shape: torch.Size([2, 3])\n",
      "Flattened logits: torch.Size([6, 50257])\n",
      "Flattened targets: torch.Size([6])\n",
      "Cross Entropy loss: 10.876513481140137\n",
      "Perplexity: 52918.7734375\n"
     ]
    }
   ],
   "execution_count": 42
  },
  {
   "cell_type": "code",
   "id": "9d0758479b082989",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-01T20:32:05.948301Z",
     "start_time": "2025-02-01T20:32:05.389496Z"
    }
   },
   "source": [
    "from src.chapter02.Dataloader import Dataloader\n",
    "import torch.nn.functional as F\n",
    "# To implement the data splitting and loading, we first define a train_ratio \n",
    "# to use 90% of the data for training and the remaining 10% as validation data \n",
    "# for model evaluation during training\n",
    "torch.manual_seed(123)\n",
    "# \n",
    "train_ratio = 0.9\n",
    "split_idx = int(train_ratio * len(raw_text))\n",
    "train_data = raw_text[:split_idx]\n",
    "val_data = raw_text[split_idx:]\n",
    "# \n",
    "train_loader = Dataloader(\n",
    "    batch_size=2,\n",
    "    max_length=GPT_CONFIG_124M_2[\"context_length\"],\n",
    "    stride=GPT_CONFIG_124M_2[\"context_length\"],\n",
    "    drop_last=True,\n",
    "    shuffle=True,\n",
    "    num_workers=0\n",
    ").create_dataloader_v1(train_data)\n",
    "# \n",
    "val_loader = Dataloader(\n",
    "    batch_size=2,\n",
    "    max_length=GPT_CONFIG_124M_2[\"context_length\"],\n",
    "    stride=GPT_CONFIG_124M_2[\"context_length\"],\n",
    "    drop_last=False,\n",
    "    shuffle=False,\n",
    "    num_workers=0\n",
    ").create_dataloader_v1(val_data)\n",
    "# \n",
    "print(\"Train loader:\")\n",
    "for x, y in train_loader:\n",
    "    print(x.shape, y.shape)\n",
    "# \n",
    "print(\"\\nValidation loader:\")\n",
    "for x, y in val_loader:\n",
    "    print(x.shape, y.shape)\n",
    "#     \n",
    "# Calculate per batch loss \n",
    "# Use CrossEntropy\n",
    "# \n",
    "def calculate_batch_loss(input_batch, target_batch, model, device):\n",
    "    input_batch = input_batch.to(device)\n",
    "    target_batch = target_batch.to(device)      \n",
    "    batch_logits = model(input_batch)\n",
    "    batch_loss = F.cross_entropy(\n",
    "        batch_logits.flatten(0, 1), \n",
    "        target_batch.flatten()\n",
    "    )\n",
    "    # batch_loss = F.nll_loss(\n",
    "    #     batch_logits.flatten(0, 1), \n",
    "    #     target_batch.flatten()\n",
    "    # )\n",
    "    return batch_loss\n",
    "\n",
    "# \n",
    "# Calculate loss across batches\n",
    "# \n",
    "def calculate_loss_loader(data_loader, model, device, num_batches=None):\n",
    "    total_loss = 0.\n",
    "    if len(data_loader) == 0:\n",
    "        return float(\"nan\")\n",
    "    elif num_batches is None:\n",
    "        num_batches = len(data_loader)\n",
    "    else:\n",
    "        num_batches = min(num_batches, len(data_loader))\n",
    "    for n, (input_batch, target_batch) in enumerate(data_loader):\n",
    "        if n < num_batches:\n",
    "            loss = calculate_batch_loss(input_batch, target_batch, model, device)\n",
    "            total_loss += loss.item()\n",
    "        else:\n",
    "            break\n",
    "    return total_loss / num_batches\n",
    "# \n",
    "model.to(device)\n",
    "with torch.no_grad():\n",
    "    train_loss = calculate_loss_loader(train_loader, model, device)\n",
    "    val_loss = calculate_loss_loader(val_loader, model, device)\n",
    "print(\"Default Training loss:\", train_loss)\n",
    "print(\"Default Validation loss:\", val_loss)\n",
    "# \n",
    "# Model Evaluation\n",
    "#   Calculate both training and validation losses and return\n",
    "#\n",
    "def evaluate_model(eval_model, training_loader, validn_loader, eval_device, eval_iter):\n",
    "    eval_model.eval()\n",
    "    with torch.no_grad():\n",
    "        training_loss = calculate_loss_loader(\n",
    "            training_loader, eval_model, eval_device, num_batches=eval_iter\n",
    "        )\n",
    "        validn_loss = calculate_loss_loader(\n",
    "            validn_loader, eval_model, eval_device, num_batches=eval_iter\n",
    "        )\n",
    "    eval_model.train()\n",
    "    return training_loss, validn_loss\n",
    "\n",
    "# A convenience function that we use to track whether the model improves during the training. \n",
    "# The generate_and_print_sample() function takes a text snippet (start_context) as input, \n",
    "# converts it into token IDs, and feeds it to the LLM to generate a text sample \n",
    "# using the generate_text_simple function\n",
    "def generate_and_print_sample(model, tokenizer, device, start_context):\n",
    "    model.eval()\n",
    "    context_size = model.pos_emb.weight.shape[0]\n",
    "    encoded = text_to_token_ids(start_context, tokenizer).to(device)\n",
    "    with torch.no_grad():\n",
    "        token_ids = generate_text_simple(\n",
    "            input_model=model, \n",
    "            tokenids=encoded,\n",
    "            max_new_tokens=50, \n",
    "            context_size=context_size\n",
    "        )\n",
    "    decoded_text = token_ids_to_text(token_ids, tokenizer)\n",
    "    print(decoded_text.replace(\"\\n\", \" \"))\n",
    "    model.train()\n",
    "\n",
    "\n",
    "# \n",
    "# Now we implement the model training flow \n",
    "#\n",
    "def train_model_simple(model, train_loader, val_loader,\n",
    "                       optimizer, device, num_epochs,\n",
    "                       eval_freq, eval_iter, start_context, tokenizer):\n",
    "    # \n",
    "    train_losses, val_losses, track_tokens_seen = [], [], []\n",
    "    tokens_seen, global_step = 0, -1\n",
    "    # \n",
    "    model.train()\n",
    "    model.to(device)\n",
    "    # \n",
    "    for epoch in range(num_epochs):\n",
    "        for input_batch, target_batch in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            input_batch = input_batch.to(device)\n",
    "            target_batch = target_batch.to(device)\n",
    "            \n",
    "            loss = calculate_batch_loss(input_batch, target_batch, model, device)\n",
    "            loss.backward() # Back propagation\n",
    "            # Optimizer step function with no closure supplied is below.\n",
    "            # A closure should clear the gradients, compute the loss, and return it.\n",
    "            # For example if it was a Conjugate Grad. optimization method \n",
    "            # We had to pass a closure. A closure is nothing but a function\n",
    "            # for input, target in dataset:\n",
    "            #    def closure():\n",
    "            #        optimizer.zero_grad()\n",
    "            #        output = model(input)\n",
    "            #        loss = loss_fn(output, target)\n",
    "            #        loss.backward()\n",
    "            #        return loss\n",
    "            #    optimizer.step(closure) \n",
    "            # \n",
    "            optimizer.step() \n",
    "            \n",
    "            tokens_seen += input_batch.numel()\n",
    "            global_step += 1\n",
    "\n",
    "            if global_step % eval_freq == 0:\n",
    "                trn_loss, validn_loss = evaluate_model(\n",
    "                    model, train_loader, val_loader, device, eval_iter\n",
    "                )\n",
    "                train_losses.append(trn_loss)\n",
    "                val_losses.append(validn_loss)\n",
    "                track_tokens_seen.append(tokens_seen)\n",
    "                print(f\"Ep {epoch+1} (Step {global_step:06d}): \"\n",
    "                      f\"Train loss {trn_loss:.3f}, \"\n",
    "                      f\"Val loss {validn_loss:.3f}\"\n",
    "                )\n",
    "        # End inner for\n",
    "        generate_and_print_sample(model, tokenizer, device, start_context) # After each epoch\n",
    "    # End outer for\n",
    "    return train_losses, val_losses, track_tokens_seen\n",
    "\n",
    "\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loader:\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "\n",
      "Validation loader:\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "Default Training loss: 10.988501654730904\n",
      "Default Validation loss: 10.990342140197754\n"
     ]
    }
   ],
   "execution_count": 43
  },
  {
   "cell_type": "code",
   "id": "9f203bec3d737b0a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-01T20:33:35.557551Z",
     "start_time": "2025-02-01T20:32:05.949172Z"
    }
   },
   "source": [
    "#\n",
    "# Training Loop\n",
    "#\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "torch.manual_seed(123)\n",
    "model_file_path=\"../models/GPTModel.pth\"\n",
    "num_epochs = 10\n",
    "train_losses, val_losses, tokens_seen = [], [], []\n",
    "epochs_tensor = None\n",
    "\n",
    "def plot_losses(epochs_seen, tokens_seen, train_losses, val_losses):\n",
    "    fig, ax1 = plt.subplots(figsize=(6, 4))\n",
    "    ax1.plot(epochs_seen, train_losses, label=\"Training loss\")\n",
    "    ax1.plot(\n",
    "        epochs_seen, val_losses, linestyle=\"-.\", label=\"Validation loss\"\n",
    "    )\n",
    "    ax1.set_xlabel(\"Epochs\")\n",
    "    ax1.set_ylabel(\"Loss\")\n",
    "    ax1.legend(loc=\"upper right\")\n",
    "    ax1.xaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "    ax2 = ax1.twiny()\n",
    "    ax2.plot(tokens_seen, train_losses, alpha=0)\n",
    "    ax2.set_xlabel(\"Tokens seen\")\n",
    "    fig.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "    # \n",
    "if not os.path.exists(model_file_path):\n",
    "    model = GPTModel(GPT_CONFIG_124M_2)\n",
    "    model.to(device)\n",
    "    optimizer = torch.optim.AdamW(\n",
    "         model.parameters(),\n",
    "        lr=0.0004, \n",
    "        weight_decay=0.1\n",
    "    )\n",
    "    # \n",
    "    # \n",
    "    train_losses, val_losses, tokens_seen = (\n",
    "        train_model_simple(model, train_loader, val_loader, optimizer, device, \n",
    "                           num_epochs=num_epochs, eval_freq=5, eval_iter=5, \n",
    "                           start_context=\"Every effort moves you\", tokenizer=tokenizer\n",
    "                           )\n",
    "    )\n",
    "    if epochs_tensor is None:\n",
    "        epochs_tensor = torch.linspace(0, num_epochs, len(train_losses))\n",
    "        plot_losses(epochs_tensor, tokens_seen, train_losses, val_losses)\n",
    "else:\n",
    "    print(f\"Model already exists\")\n",
    "    pass "
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 1 (Step 000000): Train loss 10.833, Val loss 10.882\n",
      "Ep 1 (Step 000005): Train loss 8.563, Val loss 8.758\n",
      "Every effort moves you,\n",
      "Ep 2 (Step 000010): Train loss 6.945, Val loss 7.338\n",
      "Ep 2 (Step 000015): Train loss 6.287, Val loss 6.739\n",
      "Every effort moves you \n",
      "Ep 3 (Step 000020): Train loss 6.111, Val loss 6.653\n",
      "Ep 3 (Step 000025): Train loss 6.012, Val loss 6.679\n",
      "Every effort moves you,\n",
      "Ep 4 (Step 000030): Train loss 5.997, Val loss 6.717\n",
      "Ep 4 (Step 000035): Train loss 5.992, Val loss 6.750\n",
      "Every effort moves you,\n",
      "Ep 5 (Step 000040): Train loss 5.915, Val loss 6.775\n",
      "Every effort moves you,\n",
      "Ep 6 (Step 000045): Train loss 5.907, Val loss 6.777\n",
      "Ep 6 (Step 000050): Train loss 5.846, Val loss 6.754\n",
      "Every effort moves you,\n",
      "Ep 7 (Step 000055): Train loss 5.793, Val loss 6.728\n",
      "Ep 7 (Step 000060): Train loss 5.749, Val loss 6.796\n",
      "Every effort moves you,\n",
      "Ep 8 (Step 000065): Train loss 5.636, Val loss 6.691\n",
      "Ep 8 (Step 000070): Train loss 5.547, Val loss 6.629\n",
      "Every effort moves you.\n",
      "Ep 9 (Step 000075): Train loss 5.904, Val loss 6.886\n",
      "Ep 9 (Step 000080): Train loss 5.819, Val loss 6.817\n",
      "Every effort moves you.\n",
      "Ep 10 (Step 000085): Train loss 5.671, Val loss 6.774\n",
      "Every effort moves you \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 600x400 with 2 Axes>"
      ],
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 44
  },
  {
   "cell_type": "code",
   "id": "acfc02f2040cd8ed",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-01T20:33:37.715653Z",
     "start_time": "2025-02-01T20:33:35.558904Z"
    }
   },
   "source": [
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "token_ids = generate_text_simple(\n",
    "    input_model=model,\n",
    "    tokenids=text_to_token_ids(\"Every effort moves you\", tokenizer),\n",
    "    max_new_tokens=25,\n",
    "    context_size=GPT_CONFIG_124M[\"context_length\"]\n",
    ")\n",
    "print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))\n",
    "\n",
    "# Now lets look at Temperature Scaling\n",
    "vocab = { \n",
    "    \"closer\": 0,\n",
    "    \"every\": 1, \n",
    "    \"effort\": 2, \n",
    "    \"forward\": 3,\n",
    "    \"inches\": 4,\n",
    "    \"moves\": 5, \n",
    "    \"pizza\": 6,\n",
    "    \"toward\": 7,\n",
    "    \"you\": 8,\n",
    "} \n",
    "inverse_vocab = {v: k for k, v in vocab.items()}\n",
    "# Assume the model generates the following logits \n",
    "next_token_logits = torch.tensor(\n",
    "    [4.51, 0.89, -1.90, 6.75, 1.63, -1.62, -1.89, 6.28, 1.79]\n",
    ").to(device)\n",
    "\n",
    "# We now generally do an argmax of the probabilities \n",
    "probas = torch.softmax(next_token_logits, dim=0).to(device)\n",
    "next_token_id = torch.argmax(probas).item()\n",
    "print(f\"Next token (argmax): {inverse_vocab[next_token_id]}\")\n",
    "#\n",
    "# But we can try replacing argmax with multinomial and get a sampling\n",
    "#\n",
    "torch.manual_seed(123) \n",
    "next_token_id = torch.multinomial(probas, num_samples=1).item()\n",
    "print(f\"Next token (multinomial): {inverse_vocab[next_token_id]}\")\n",
    "# \n",
    "# Let's see if multinomial can produce any other probabilites\n",
    "def print_sampled_tokens(probabs):\n",
    "    torch.manual_seed(123)\n",
    "    # Multinomial Sampling of probabilities instead of max\n",
    "    samples = [torch.multinomial(probabs, num_samples=1).item() for _ in range(1_000)]\n",
    "    sampled_ids = torch.bincount(torch.tensor(samples).to(device))\n",
    "    for cnt, freq in enumerate(sampled_ids):\n",
    "        print(f\"Sampled Freq. {freq} : {inverse_vocab[cnt]}\")\n",
    "\n",
    "print_sampled_tokens(probas)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output text:\n",
      " Every effort moves you\n",
      "\n",
      "Next token (argmax): forward\n",
      "Next token (multinomial): forward\n",
      "Sampled Freq. 72 : closer\n",
      "Sampled Freq. 2 : every\n",
      "Sampled Freq. 0 : effort\n",
      "Sampled Freq. 575 : forward\n",
      "Sampled Freq. 2 : inches\n",
      "Sampled Freq. 0 : moves\n",
      "Sampled Freq. 0 : pizza\n",
      "Sampled Freq. 343 : toward\n",
      "Sampled Freq. 6 : you\n"
     ]
    }
   ],
   "execution_count": 45
  },
  {
   "cell_type": "code",
   "id": "2edb9cac7f426800",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-01T20:33:37.781410Z",
     "start_time": "2025-02-01T20:33:37.716763Z"
    }
   },
   "source": [
    "# We can further control the distribution by a technique called temperature scaling\n",
    "# meaning dividing the logits with a >0 number\n",
    "def softmax_with_temperature(logits, temperature, dim):\n",
    "    scaled_logits = logits / temperature\n",
    "    scaled_probs =  torch.softmax(scaled_logits, dim=dim)\n",
    "    return scaled_probs\n",
    "\n",
    "# Let's check that out\n",
    "temperatures = [1, .01, 5]\n",
    "next_token_logits: Tensor = Tensor.cpu(next_token_logits)\n",
    "scaled_probas = [softmax_with_temperature(next_token_logits, float(T), 0) for T in temperatures]\n",
    "x = torch.arange(len(vocab))\n",
    "bar_width = 0.5\n",
    "fig, ax = plt.subplots(figsize=(6, 4))\n",
    "for i, T in enumerate(temperatures):\n",
    "    rects = ax.bar(x + i * bar_width, \n",
    "                   scaled_probas[i], \n",
    "                   bar_width, \n",
    "                   label=f'Temperature = {T}'\n",
    "    )\n",
    "ax.set_ylabel('Probability')\n",
    "ax.set_xlabel('Words')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(vocab.keys(), rotation=90)\n",
    "ax.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 600x400 with 1 Axes>"
      ],
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 46
  },
  {
   "cell_type": "markdown",
   "id": "48c5d9214db5c7e3",
   "metadata": {},
   "source": [
    "## Top K sampling"
   ]
  },
  {
   "cell_type": "code",
   "id": "af408748892d4b7f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-01T20:33:39.780505Z",
     "start_time": "2025-02-01T20:33:37.782231Z"
    }
   },
   "source": [
    "# Previously we implemented a probabilistic sampling approach coupled with \n",
    "# temperature scaling to increase the diversity of the outputs.  This method \n",
    "# allows for the exploring of less likely but potentially more interesting and \n",
    "# creative paths in the generation process.\n",
    "#\n",
    "# Top-k sampling, when combined with probabilistic sampling and temperature \n",
    "# scaling, can improve the text generation results.\n",
    "#\n",
    "# Here we can restrict the sampled tokens to the top-k most likely tokens \n",
    "# and exclude all other tokens from the selection process by masking their \n",
    "# probability scores\n",
    "# \n",
    "top_k = 3\n",
    "torch.set_printoptions(sci_mode=False)\n",
    "# \n",
    "top_logits, top_pos = torch.topk(next_token_logits, top_k)\n",
    "print(\"Top Logits: \", top_logits)\n",
    "print(\"Top Positions: \", top_pos)\n",
    "print(f\"Next token logits: {next_token_logits}\")\n",
    "\n",
    "# Pytorch WHERE function to set the logit values of tokens that are below the lowest \n",
    "# logit value within our top-three selection to negative infinity (-inf)\n",
    "#\n",
    "new_logits = torch.where(\n",
    "    condition=next_token_logits < top_logits[-1],\n",
    "    input=torch.tensor(float('-inf')),\n",
    "    other=next_token_logits\n",
    ")\n",
    "print(\"New Logits: \", new_logits)\n",
    "\n",
    "# Now apply the softmax\n",
    "topk_probas = torch.softmax(new_logits, dim=0)\n",
    "print(f\"top k probabilities: {topk_probas}\")\n",
    "# \n",
    "# We can now apply the temperature scaling and multinomial function for probabilistic \n",
    "# sampling to select the next token among these three non-zero probability scores to \n",
    "# GENERATE THE NEXT TOKEN with more diversity.\n",
    "# \n",
    "def generate(model, idx, max_new_tokens, context_size, temperature=0.0, top_k: int=None, eos_id=None):\n",
    "    # print(\"Entering generate()..\")\n",
    "    # print(idx.shape)\n",
    "    for i in range(max_new_tokens):\n",
    "        # print(f\"idx: [{i}]: {idx}\")\n",
    "        idx_cond = idx[:, -context_size:]\n",
    "        with torch.no_grad():\n",
    "            logits = model(idx_cond)\n",
    "        #     \n",
    "        logits = logits[:, -1, :] # ([1, 50257])\n",
    "        # print(logits.shape)\n",
    "        if top_k is not None:\n",
    "            top_logits, _ = torch.topk(logits, top_k)\n",
    "            min_val: Tensor = top_logits[:, -1] # Less than the lowest value of top k\n",
    "            # Now mark the minvals with -inf, so softmax becomes 0\n",
    "            logits = torch.where(\n",
    "                logits < min_val,\n",
    "                torch.tensor(float('-inf')).to(logits.device),\n",
    "                logits\n",
    "            )\n",
    "        if temperature > 0.0:\n",
    "            probs = softmax_with_temperature(logits, temperature, dim=-1)\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)\n",
    "        else: \n",
    "            idx_next = torch.argmax(logits, dim=-1, keepdim=True)\n",
    "\n",
    "        if idx_next == eos_id:\n",
    "            break\n",
    "\n",
    "        # Next word\n",
    "        idx = torch.cat((idx, idx_next), dim=1)\n",
    "\n",
    "    return idx\n",
    "\n",
    "token_ids = generate(\n",
    "    model=model,\n",
    "    idx=text_to_token_ids(\"Every effort moves you\", tokenizer),\n",
    "    max_new_tokens=15,\n",
    "    context_size=GPT_CONFIG_124M_2[\"context_length\"],\n",
    "    top_k=25,\n",
    "    temperature=1.4\n",
    ")\n",
    "print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))\n",
    "\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top Logits:  tensor([6.7500, 6.2800, 4.5100])\n",
      "Top Positions:  tensor([3, 7, 0])\n",
      "Next token logits: tensor([ 4.5100,  0.8900, -1.9000,  6.7500,  1.6300, -1.6200, -1.8900,  6.2800,\n",
      "         1.7900])\n",
      "New Logits:  tensor([4.5100,   -inf,   -inf, 6.7500,   -inf,   -inf,   -inf, 6.2800,   -inf])\n",
      "top k probabilities: tensor([0.0615, 0.0000, 0.0000, 0.5775, 0.0000, 0.0000, 0.0000, 0.3610, 0.0000])\n",
      "Output text:\n",
      " Every effort moves you with\n",
      "\n",
      "\"I\n",
      "\"\"\"\n",
      "\n",
      "\n",
      "\n",
      "\"I\n"
     ]
    }
   ],
   "execution_count": 47
  },
  {
   "cell_type": "code",
   "id": "7e164cfeef1f9707",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-01T20:33:43.966812Z",
     "start_time": "2025-02-01T20:33:39.781357Z"
    }
   },
   "source": [
    "# torch.save(model.state_dict(), f\"/Users/amlanchatterjee/Documents/ws/python/PycharmProjects/SimpleLLMProject/models/{GPT_CONFIG_124M_2['model_name']}.pth\")\n",
    "MODEL_PATH = f\"/Users/amlanchatterjee/Documents/ws/python/PycharmProjects/SimpleLLMProject/models/{GPT_CONFIG_124M_2['model_name']}.pth\"\n",
    "# \n",
    "try:\n",
    "    torch.save({\n",
    "        \"model_state_dict\": model.state_dict(),\n",
    "        \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "        }, \n",
    "        MODEL_PATH\n",
    "    )\n",
    "    print(f\"Model saved at {MODEL_PATH}\\n\")\n",
    "except Exception as e:\n",
    "    print(f\"Encountered exception : {e}\")\n",
    "    \n",
    "# "
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved at /Users/amlanchatterjee/Documents/ws/python/PycharmProjects/SimpleLLMProject/models/GPTModel.pth\n",
      "\n"
     ]
    }
   ],
   "execution_count": 48
  },
  {
   "cell_type": "code",
   "id": "d0a3f72c8cebc09d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-01T20:33:45.618642Z",
     "start_time": "2025-02-01T20:33:43.967527Z"
    }
   },
   "source": [
    "# Load the model back\n",
    "checkpoint = torch.load(MODEL_PATH, map_location=device,weights_only=True)\n",
    "loaded_model = GPTModel(GPT_CONFIG_124M_2).to(device)\n",
    "try:\n",
    "    loaded_model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "    optimizer = torch.optim.AdamW(loaded_model.parameters(), \n",
    "                                 lr=GPT_CONFIG_124M_2[\"lr\"], \n",
    "                                 weight_decay=GPT_CONFIG_124M_2[\"weight_decay\"])\n",
    "    optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n",
    "    print(f\"Model and Optimizer successfully loaded from \\n{MODEL_PATH}\\n\")\n",
    "except Exception as e:\n",
    "    print(f\"Encountered exception : {e}\")\n",
    "    \n",
    "loaded_model.train()\n",
    "print(\"Model set to train mode\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model and Optimizer successfully loaded from \n",
      "/Users/amlanchatterjee/Documents/ws/python/PycharmProjects/SimpleLLMProject/models/GPTModel.pth\n",
      "\n",
      "Model set to train mode\n"
     ]
    }
   ],
   "execution_count": 49
  },
  {
   "cell_type": "code",
   "id": "f6108362246432af",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-01T20:33:45.621738Z",
     "start_time": "2025-02-01T20:33:45.619461Z"
    }
   },
   "source": [
    "# Try training the model a couple of times more\n",
    "# epochs = 2\n",
    "# train_ratio = 0.9\n",
    "# # \n",
    "# print(f\"Model is on {next(model.parameters()).device}\")  \n",
    "# train_losses, val_losses, tokens_seen = (\n",
    "#     train_model_simple(\n",
    "#         loaded_model, \n",
    "#         train_loader, \n",
    "#         val_loader, \n",
    "#         optimizer, \n",
    "#         device, \n",
    "#         num_epochs=epochs, \n",
    "#         eval_freq=5, \n",
    "#         eval_iter=5, \n",
    "#         start_context=\"Every effort moves you\", \n",
    "#         tokenizer=tokenizer\n",
    "#     )\n",
    "# )\n",
    "# epochs_tensor = torch.linspace(0, num_epochs, len(train_losses))\n",
    "# plot_losses(epochs_tensor, tokens_seen, train_losses, val_losses)"
   ],
   "outputs": [],
   "execution_count": 50
  },
  {
   "cell_type": "markdown",
   "id": "d38b09e853311dec",
   "metadata": {},
   "source": [
    "### OpenAI also shares the weights of larger models: 355M, 774M, and 1558M \n",
    "![image](../data/model_arch_stack.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "68e160927f07d23a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-01T20:34:36.169077Z",
     "start_time": "2025-02-01T20:34:34.001126Z"
    }
   },
   "source": [
    "# Load the downloaded GPT Data\n",
    "import urllib.request\n",
    "from src.chapter05.gpt_download import download_and_load_gpt2\n",
    "\n",
    "url = (\n",
    "    \"https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/ch05/01_main-chapter-code/gpt_download.py\"\n",
    ")\n",
    "filename = url.split('/')[-1]\n",
    "urllib.request.urlretrieve(url, filename)\n",
    "\n",
    "settings, gpt_params = download_and_load_gpt2(model_size=\"124M\", models_dir=\"../data/gpt2\")\n",
    "print(f\"\\nParams: {gpt_params.keys()}\")\n",
    "print(f\"Settings: {settings}\")\n",
    "print(f\"Token embedding layer weight tensor dimensions: {gpt_params[\"wte\"].shape}\")    \n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File already exists and is up-to-date: ../data/gpt2/124M/checkpoint\n",
      "File already exists and is up-to-date: ../data/gpt2/124M/encoder.json\n",
      "File already exists and is up-to-date: ../data/gpt2/124M/hparams.json\n",
      "File already exists and is up-to-date: ../data/gpt2/124M/model.ckpt.data-00000-of-00001\n",
      "File already exists and is up-to-date: ../data/gpt2/124M/model.ckpt.index\n",
      "File already exists and is up-to-date: ../data/gpt2/124M/model.ckpt.meta\n",
      "File already exists and is up-to-date: ../data/gpt2/124M/vocab.bpe\n",
      "\n",
      "Params: dict_keys(['blocks', 'b', 'g', 'wpe', 'wte'])\n",
      "Settings: {'n_vocab': 50257, 'n_ctx': 1024, 'n_embd': 768, 'n_head': 12, 'n_layer': 12}\n",
      "Token embedding layer weight tensor dimensions: (50257, 768)\n"
     ]
    }
   ],
   "execution_count": 52
  },
  {
   "cell_type": "code",
   "id": "ba48ffe795182755",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-01T20:34:50.801105Z",
     "start_time": "2025-02-01T20:34:49.844142Z"
    }
   },
   "source": [
    "\n",
    "# After loading the GPT-2 model weights into Python, we still need to transfer \n",
    "# them from the settings and params dictionaries into our GPTModel instance. \n",
    "# First, we create a dictionary that lists the differences between the \n",
    "# different GPT model sizes\n",
    "model_configs = {\n",
    "    \"gpt2-small (124M)\": {\"emb_dim\": 768, \"n_layers\": 12, \"n_heads\": 12},\n",
    "    \"gpt2-medium (355M)\": {\"emb_dim\": 1024, \"n_layers\": 24, \"n_heads\": 16},\n",
    "    \"gpt2-large (774M)\": {\"emb_dim\": 1280, \"n_layers\": 36, \"n_heads\": 20},\n",
    "    \"gpt2-xl (1558M)\": {\"emb_dim\": 1600, \"n_layers\": 48, \"n_heads\": 25},\n",
    "}\n",
    "model_name=\"gpt2-small (124M)\"\n",
    "# \n",
    "NEW_GPT_CONFIG = GPT_CONFIG_124M.copy()\n",
    "NEW_GPT_CONFIG.update({\"model_name\": model_name})\n",
    "# Update the value ex. {\"emb_dim\": 768, \"n_layers\": 12, \"n_heads\": 12},\n",
    "NEW_GPT_CONFIG.update(model_configs[model_name]) \n",
    "\n",
    "NEW_GPT_CONFIG.update({\"context_length\": 1024})\n",
    "NEW_GPT_CONFIG.update({\"qkv_bias\": True})\n",
    "# \n",
    "print(f\"{model_name}: {model_configs[model_name]}\")\n",
    "print(\"NEW_GPT_CONFIG:\\n\"+\"\".join(f\"\\t{k}: {v}\\n\" for k, v in sorted(NEW_GPT_CONFIG.items())))\n",
    "# \n",
    "newgpt = GPTModel(NEW_GPT_CONFIG).to(device)\n",
    "newgpt.eval()\n",
    "# \n",
    "# Before we assign the loaded openai weights into the model, we will first define \n",
    "# a small assign utility function that checks whether two tensors or arrays \n",
    "# (left and right) have the same dimensions or shape and returns the right tensor \n",
    "# as trainable PyTorch parameters\n",
    "def assign(left: Tensor, right: Tensor) -> Tensor:\n",
    "    if left.shape != right.shape:\n",
    "        raise ValueError(f\"Shape mismatch Left shape: {left.shape} Right shape: {right.shape}\")\n",
    "    return torch.nn.Parameter(torch.tensor(right).to(device))\n",
    "# \n",
    "print(f\"Params: {gpt_params.keys()}\")\n",
    "# print(f\"Params: {gpt_params}\")\n",
    "print(f\"GPT Parameter Blocks Count: {len(gpt_params[\"blocks\"])}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpt2-small (124M): {'emb_dim': 768, 'n_layers': 12, 'n_heads': 12}\n",
      "NEW_GPT_CONFIG:\n",
      "\tcontext_length: 1024\n",
      "\tdrop_rate: 0.1\n",
      "\temb_dim: 768\n",
      "\tlr: 0.0005\n",
      "\tmodel_name: gpt2-small (124M)\n",
      "\tn_heads: 12\n",
      "\tn_layers: 12\n",
      "\tqkv_bias: True\n",
      "\tvocab_size: 50257\n",
      "\tweight_decay: 0.1\n",
      "\n",
      "Params: dict_keys(['blocks', 'b', 'g', 'wpe', 'wte'])\n",
      "GPT Parameter Blocks Count: 12\n"
     ]
    }
   ],
   "execution_count": 53
  },
  {
   "cell_type": "code",
   "id": "dbc4d3fb2c383826",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-01T20:34:50.807670Z",
     "start_time": "2025-02-01T20:34:50.801786Z"
    }
   },
   "source": [
    "#\n",
    "# Load OpenAI Weights into our GPTModel code\n",
    "#\n",
    "import numpy as np\n",
    "\n",
    "def load_weights_into_gpt(gpt, params):\n",
    "    gpt.pos_emb.weight = assign(gpt.pos_emb.weight, params['wpe'])\n",
    "    gpt.tok_emb.weight = assign(gpt.tok_emb.weight, params['wte'])\n",
    "\n",
    "    for b in range(len(params[\"blocks\"])):\n",
    "        q_w, k_w, v_w = np.split(\n",
    "            (params[\"blocks\"][b][\"attn\"][\"c_attn\"])[\"w\"], 3, axis=-1)\n",
    "        gpt.trf_blocks[b].att.W_query.weight = assign(\n",
    "            gpt.trf_blocks[b].att.W_query.weight, q_w.T)\n",
    "        gpt.trf_blocks[b].att.W_key.weight = assign(\n",
    "            gpt.trf_blocks[b].att.W_key.weight, k_w.T)\n",
    "        gpt.trf_blocks[b].att.W_value.weight = assign(\n",
    "            gpt.trf_blocks[b].att.W_value.weight, v_w.T)\n",
    "\n",
    "        q_b, k_b, v_b = np.split(\n",
    "            (params[\"blocks\"][b][\"attn\"][\"c_attn\"])[\"b\"], 3, axis=-1)\n",
    "        gpt.trf_blocks[b].att.W_query.bias = assign(\n",
    "            gpt.trf_blocks[b].att.W_query.bias, q_b)\n",
    "        gpt.trf_blocks[b].att.W_key.bias = assign(\n",
    "            gpt.trf_blocks[b].att.W_key.bias, k_b)\n",
    "        gpt.trf_blocks[b].att.W_value.bias = assign(\n",
    "            gpt.trf_blocks[b].att.W_value.bias, v_b)\n",
    "\n",
    "        gpt.trf_blocks[b].att.out_proj.weight = assign(\n",
    "            gpt.trf_blocks[b].att.out_proj.weight, \n",
    "            params[\"blocks\"][b][\"attn\"][\"c_proj\"][\"w\"].T)\n",
    "        gpt.trf_blocks[b].att.out_proj.bias = assign(\n",
    "            gpt.trf_blocks[b].att.out_proj.bias, \n",
    "            params[\"blocks\"][b][\"attn\"][\"c_proj\"][\"b\"])\n",
    "\n",
    "        gpt.trf_blocks[b].sff.layers[0].weight = assign(\n",
    "            gpt.trf_blocks[b].sff.layers[0].weight, \n",
    "            params[\"blocks\"][b][\"mlp\"][\"c_fc\"][\"w\"].T)\n",
    "        gpt.trf_blocks[b].sff.layers[0].bias = assign(\n",
    "            gpt.trf_blocks[b].sff.layers[0].bias, \n",
    "            params[\"blocks\"][b][\"mlp\"][\"c_fc\"][\"b\"])\n",
    "        gpt.trf_blocks[b].sff.layers[2].weight = assign(\n",
    "            gpt.trf_blocks[b].sff.layers[2].weight, \n",
    "            params[\"blocks\"][b][\"mlp\"][\"c_proj\"][\"w\"].T)\n",
    "        gpt.trf_blocks[b].sff.layers[2].bias = assign(\n",
    "            gpt.trf_blocks[b].sff.layers[2].bias, \n",
    "            params[\"blocks\"][b][\"mlp\"][\"c_proj\"][\"b\"])\n",
    "\n",
    "        gpt.trf_blocks[b].norm1.scale = assign(\n",
    "            gpt.trf_blocks[b].norm1.scale, \n",
    "            params[\"blocks\"][b][\"ln_1\"][\"g\"])\n",
    "        gpt.trf_blocks[b].norm1.shift = assign(\n",
    "            gpt.trf_blocks[b].norm1.shift, \n",
    "            params[\"blocks\"][b][\"ln_1\"][\"b\"])\n",
    "        gpt.trf_blocks[b].norm2.scale = assign(\n",
    "            gpt.trf_blocks[b].norm2.scale, \n",
    "            params[\"blocks\"][b][\"ln_2\"][\"g\"])\n",
    "        gpt.trf_blocks[b].norm2.shift = assign(\n",
    "            gpt.trf_blocks[b].norm2.shift, \n",
    "            params[\"blocks\"][b][\"ln_2\"][\"b\"])\n",
    "\n",
    "    gpt.final_norm.scale = assign(gpt.final_norm.scale, params[\"g\"])\n",
    "    gpt.final_norm.shift = assign(gpt.final_norm.shift, params[\"b\"])\n",
    "    gpt.out_head.weight = assign(gpt.out_head.weight, params[\"wte\"])\n",
    "    "
   ],
   "outputs": [],
   "execution_count": 54
  },
  {
   "cell_type": "code",
   "id": "c0eeef9e5cadf037",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-01T20:34:51.157936Z",
     "start_time": "2025-02-01T20:34:51.016305Z"
    }
   },
   "source": [
    "# Now lets try to load the weights and see\n",
    "load_weights_into_gpt(newgpt, gpt_params)\n",
    "newgpt.to(device)\n",
    "print(\"Loaded weights into GPTModel..\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded weights into GPTModel..\n"
     ]
    }
   ],
   "execution_count": 55
  },
  {
   "cell_type": "code",
   "id": "53993f0b2ef4021a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-01T20:35:07.732567Z",
     "start_time": "2025-02-01T20:35:06.073509Z"
    }
   },
   "source": [
    "# Now lets generate using the actual GPT trained weights\n",
    "torch.manual_seed(123)\n",
    "token_ids = generate(\n",
    "    model=newgpt,\n",
    "    idx=text_to_token_ids(\"Every effort moves you\", tokenizer).to(device),\n",
    "    max_new_tokens=25,\n",
    "    context_size=NEW_GPT_CONFIG[\"context_length\"],\n",
    "    top_k=50,\n",
    "    temperature=1.5\n",
    ")\n",
    "print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output text:\n",
      " Every effort moves you toward more efficient and efficient processes, like in the car's oil and gas operation,\" the study said. To see if that\n"
     ]
    }
   ],
   "execution_count": 56
  },
  {
   "cell_type": "markdown",
   "id": "850187dca8e7693f",
   "metadata": {},
   "source": [
    "# Fine-tuning for Classification"
   ]
  },
  {
   "cell_type": "code",
   "id": "533a90b85f4e1468",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-01T20:35:07.737155Z",
     "start_time": "2025-02-01T20:35:07.733293Z"
    }
   },
   "source": [
    "from pathlib import Path\n",
    "from src.chapter06 import DownloadDataset\n",
    "# \n",
    "# Download ehtSPAM Dataset\n",
    "# \n",
    "url = \"https://archive.ics.uci.edu/static/public/228/sms+spam+collection.zip\"\n",
    "zip_path = \"../data/sms_spam_collection.zip\"\n",
    "extracted_path = \"../data/sms_spam_collection\"\n",
    "data_file_path = Path(extracted_path) / \"SMSSpamCollection.tsv\"\n",
    "\n",
    "# \n",
    "DownloadDataset.download_and_unzip_spam_data(url, \n",
    "                                             zip_path, \n",
    "                                             extracted_path, \n",
    "                                             data_file_path)\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and unzipping spam dataset...\n",
      "../data/sms_spam_collection/SMSSpamCollection.tsv already exists. Skipping download and extraction.\n"
     ]
    }
   ],
   "execution_count": 57
  },
  {
   "cell_type": "code",
   "id": "bcc8d30ddb5d1c8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-01T20:35:47.694235Z",
     "start_time": "2025-02-01T20:35:47.685100Z"
    }
   },
   "source": [
    "from pandas import DataFrame\n",
    "import pandas as pd\n",
    "\n",
    "# At this point the spam dataset should have been downloaded at {data_file_path}\n",
    "df: DataFrame = None\n",
    "\n",
    "if data_file_path.exists() and data_file_path.is_file():\n",
    "    df = pd.read_csv(data_file_path, sep=\"\\t\", header=None, names=[\"Label\", \"Text\"])\n",
    "   \n",
    "# df.head(10)\n",
    "\n",
    "#Let's take a look at class distributions\n",
    "print(f\"Class counts: {df[\"Label\"].value_counts()}\")\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class counts: Label\n",
      "ham     4825\n",
      "spam     747\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "execution_count": 62
  },
  {
   "cell_type": "code",
   "id": "eb7fba4d33fca94f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-01T20:35:07.763518Z",
     "start_time": "2025-02-01T20:35:07.750123Z"
    }
   },
   "source": [
    "# Considering there are so many more hams than spams we need to create a somewhat balanced dataset\n",
    "import os\n",
    "def create_balanced_dataset(df: DataFrame) -> DataFrame:\n",
    "    # print(df.shape)\n",
    "    num_spam = df[df[\"Label\"] == \"spam\"].shape[0]\n",
    "    num_ham = df[df[\"Label\"] == \"ham\"].shape[0]\n",
    "    print(f\"Spam and Ham counts: {num_spam}, {num_ham} \\n\")\n",
    "    # If num_spam is a lot less than num_ham\n",
    "    if num_spam < num_ham or num_spam == 0:\n",
    "        ham_subset = df[df[\"Label\"] == \"ham\"].sample(num_spam, random_state=123)\n",
    "    \n",
    "    bal_df = pd.concat([ham_subset, df[df[\"Label\"] == \"spam\"]])\n",
    "    return bal_df\n",
    "\n",
    "\n",
    "balanced_df = create_balanced_dataset(df)\n",
    "# print(f\"Rebalanced dataset \\n {balanced_df[\"Label\"].value_counts()}\")\n",
    "\n",
    "# Now we are going to change the string class labels to ints\n",
    "balanced_df[\"Label\"] = balanced_df[\"Label\"].map({\"ham\": 0, \"spam\": 1})\n",
    "\n",
    "# This process is similar to converting text into token IDs. However, instead \n",
    "# of using the GPT vocabulary, which consists of more than 50,000 words, we \n",
    "# are dealing with just two token IDs: 0 and 1.\n",
    "# \n",
    "def random_split(df, train_frac, validation_frac):\n",
    "    df = df.sample(frac=1, random_state=123).reset_index(drop=True)\n",
    "    train_end = int(len(df) * train_frac) # * 0.7\n",
    "    validation_end = train_end + int(len(df) * validation_frac) \n",
    "    \n",
    "    train_df = df[:train_end] # 70%\n",
    "    valid_df = df[train_end:validation_end] # 10%\n",
    "    test_df = df[validation_end:]   # 20%\n",
    "    \n",
    "    return train_df, valid_df, test_df\n",
    "\n",
    "# Next, we create a random_split function to split the dataset into three parts: \n",
    "# 70% for training, 10% for validation, and 20% for testing\n",
    "training_df, validation_df, testing_df = random_split(balanced_df, 0.7, 0.1)\n",
    "# \n",
    "print(f\"Training dataset \\n {training_df['Label'].value_counts()}\")\n",
    "print(f\"Validation dataset \\n {validation_df['Label'].value_counts()}\")\n",
    "print(f\"Training dataset \\n {testing_df['Label'].value_counts()}\")\n",
    "# \n",
    "# Save the files\n",
    "if not os.path.exists(\"../data/train.csv\"):\n",
    "    training_df.to_csv(\"../data/train.csv\", index=None)\n",
    "    \n",
    "if not os.path.exists(\"../data/validation.csv\"):   \n",
    "    validation_df.to_csv(\"../data/validate.csv\", index=None)\n",
    "    \n",
    "if not os.path.exists(\"../data/test.csv\"):    \n",
    "    testing_df.to_csv(\"../data/test.csv\", index=None)\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spam and Ham counts: 747, 4825 \n",
      "\n",
      "Training dataset \n",
      " Label\n",
      "0    528\n",
      "1    517\n",
      "Name: count, dtype: int64\n",
      "Validation dataset \n",
      " Label\n",
      "1    79\n",
      "0    70\n",
      "Name: count, dtype: int64\n",
      "Training dataset \n",
      " Label\n",
      "1    151\n",
      "0    149\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "execution_count": 59
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 6.3 Setting up PyTorch Data Loaders",
   "id": "75ffdb033fba532f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-01T20:35:07.766742Z",
     "start_time": "2025-02-01T20:35:07.764393Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import tiktoken\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "print(tokenizer.encode(\"<|endoftext|>\", allowed_special={\"<|endoftext|>\"}))\n"
   ],
   "id": "a13c23eaebccca8a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[50256]\n"
     ]
    }
   ],
   "execution_count": 60
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-01T20:39:46.200181Z",
     "start_time": "2025-02-01T20:39:46.171337Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from src.chapter06.SpamDataset import SpamDataset\n",
    "\n",
    "# Since each row of training data has varying length, we are going to padd all \n",
    "# rows to the size of the max length of the longest row using \"<|endoftext|>\" or rather \n",
    "# its token equivalent i.e. 50256\n",
    "train_dataset = SpamDataset(\n",
    "    csv_file=\"../data/train.csv\",\n",
    "    max_length=None,\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "print(f\"Max length of training set : {train_dataset.max_length}\\n\")\n",
    "\n",
    "val_dataset = SpamDataset(\n",
    "    csv_file=\"../data/validate.csv\",\n",
    "    max_length=train_dataset.max_length,\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "print(f\"Max length of validate set : {val_dataset.max_length}\\n\")\n",
    "\n",
    "test_dataset = SpamDataset(\n",
    "    csv_file=\"../data/test.csv\",\n",
    "    max_length=train_dataset.max_length,\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "print(f\"Max length of test set : {test_dataset.max_length}\")\n"
   ],
   "id": "dfb2d42b723f5113",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of ../data/train.csv: (1045, 2)\n",
      "Max length of training set : 120\n",
      "\n",
      "Shape of ../data/validate.csv: (149, 2)\n",
      "Max length of validate set : 120\n",
      "\n",
      "Shape of ../data/test.csv: (300, 2)\n",
      "Max length of test set : 120\n"
     ]
    }
   ],
   "execution_count": 66
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### NOTE: The difference with text prediction here is that for each sample we have a class label\n",
    "#### associated with it using the datasets as inputs, we can now instantiate the data loaders\n",
    "#### similarly to when we were working with text data. However, in this case,  the targets\n",
    "#### represent class labels rather than the next tokens in the text. For instance, if we \n",
    "#### choose a batch size of 8, each batch will consist of eight training examples of length \n",
    "#### '120' and the corresponding class label of each example"
   ],
   "id": "828481d7cb91412c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-01T20:43:03.482999Z",
     "start_time": "2025-02-01T20:43:03.449679Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "torch.manual_seed(123)\n",
    "num_workers = 0\n",
    "batch_size = 8\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    dataset=train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=num_workers,\n",
    "    drop_last=True,\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    dataset=val_dataset,\n",
    "    batch_size=batch_size,\n",
    "    num_workers=num_workers,\n",
    "    drop_last=False,\n",
    ")\n",
    "test_loader = DataLoader(\n",
    "    dataset=test_dataset,\n",
    "    batch_size=batch_size,\n",
    "    num_workers=num_workers,\n",
    "    drop_last=False,\n",
    ")\n",
    "\n",
    "for input_batch, target_batch in train_loader:\n",
    "    pass\n",
    "print(\"Input batch dimensions:\", input_batch.shape)\n",
    "print(\"Label batch dimensions\", target_batch.shape)"
   ],
   "id": "8a0f692e09c92e85",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input batch dimensions: torch.Size([8, 120])\n",
      "Label batch dimensions torch.Size([8])\n"
     ]
    }
   ],
   "execution_count": 67
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "177359b3787f5cf7"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
