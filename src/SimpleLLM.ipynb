{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8a77807f92f26ee",
   "metadata": {},
   "source": [
    "# This is an attempt to learn by building and training an LLM from Scratch\n",
    "## Chapter 01  "
   ]
  },
  {
   "cell_type": "code",
   "id": "fbc121e30a2defb3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-02T18:27:17.944257Z",
     "start_time": "2025-02-02T18:27:17.078262Z"
    }
   },
   "source": [
    "\n",
    "import os\n",
    "import torch\n",
    "from torch import Tensor\n",
    "import urllib.request\n",
    "    \n",
    "# Check which GPU if any is available\n",
    "# torch.cuda.is_available():\n",
    "#     device = torch.device(\"cuda\")\n",
    "#     x: Tensor = torch.ones(1, device=device)\n",
    "#     print(f\"x = {x} using 'cuda:0' backend\")\n",
    "#     \n",
    "# elif \n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "    x: Tensor = torch.ones(1, device=device)\n",
    "    print(f\"x = {x} using {device} backend\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    # x: Tensor = torch.ones(1, device=device)\n",
    " \n",
    "print(device)\n",
    "\n",
    "def get_some_text():\n",
    "    # Download a text (book)\n",
    "    bookUrl = \"https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/ch02/01_main-chapter-code/the-verdict.txt\"  \n",
    "    filepath = \"../data/the-verdict.txt\"\n",
    "    # print(file_path)\n",
    "    if not os.path.exists(filepath):\n",
    "        urllib.request.urlretrieve(bookUrl, filepath)\n",
    "\n",
    "    with open(filepath, \"r\", encoding=\"utf-8\") as f:\n",
    "        rawtext = f.read()\n",
    "        \n",
    "    print(\"Total characters in the story: \", len(rawtext))\n",
    "    print(\"Total Lines in raw text: \", rawtext.count(\"\\n\"))\n",
    "    return rawtext\n",
    "\n",
    "raw_text = get_some_text()\n",
    "print(\"Some text: \", raw_text[:49])"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x = tensor([1.], device='mps:0') using mps backend\n",
      "mps\n",
      "Total characters in the story:  20479\n",
      "Total Lines in raw text:  164\n",
      "Some text:  I HAD always thought Jack Gisburn rather a cheap \n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "id": "2c7227e79afbcad7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-02T18:27:17.948329Z",
     "start_time": "2025-02-02T18:27:17.945299Z"
    }
   },
   "source": [
    "# Now we have to tokenize the text. The best way to do that is to use a pre-build tokennizer, but first we will try some \n",
    "# basic python regular expressions to do the same things\n",
    "import re\n",
    "preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', raw_text)\n",
    "preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
    "#\n",
    "print(len(preprocessed))\n",
    "print(preprocessed[:30])"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4690\n",
      "['I', 'HAD', 'always', 'thought', 'Jack', 'Gisburn', 'rather', 'a', 'cheap', 'genius', '--', 'though', 'a', 'good', 'fellow', 'enough', '--', 'so', 'it', 'was', 'no', 'great', 'surprise', 'to', 'me', 'to', 'hear', 'that', ',', 'in']\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "id": "b9dc98b584bc6d61",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-02T18:27:17.951624Z",
     "start_time": "2025-02-02T18:27:17.949092Z"
    }
   },
   "source": [
    "# Now we need to generate token IDs\n",
    "# Now let us create a list of all unique tokens and sort them alphabetically to determine the vocabulary size\n",
    "all_uniq_words = sorted(set(preprocessed))\n",
    "vocab_size = len(all_uniq_words)\n",
    "print(\"Vocab size: \", vocab_size)\n",
    "\n",
    "# Now that we know the vocabulary size, lets enumerate and assign some numbers to them\n",
    "vocab = {token:integer for integer,token in enumerate(all_uniq_words)}\n",
    "for i, item in enumerate(vocab.items()):\n",
    "    print(item)\n",
    "    if i >= 20:\n",
    "        break\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size:  1130\n",
      "('!', 0)\n",
      "('\"', 1)\n",
      "(\"'\", 2)\n",
      "('(', 3)\n",
      "(')', 4)\n",
      "(',', 5)\n",
      "('--', 6)\n",
      "('.', 7)\n",
      "(':', 8)\n",
      "(';', 9)\n",
      "('?', 10)\n",
      "('A', 11)\n",
      "('Ah', 12)\n",
      "('Among', 13)\n",
      "('And', 14)\n",
      "('Are', 15)\n",
      "('Arrt', 16)\n",
      "('As', 17)\n",
      "('At', 18)\n",
      "('Be', 19)\n",
      "('Begin', 20)\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "id": "b49e9060996c9f10",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-02T18:27:17.955923Z",
     "start_time": "2025-02-02T18:27:17.952941Z"
    }
   },
   "source": [
    "from src.chapter02.SimpleTokenizerV1 import SimpleTokenizerV1\n",
    "\n",
    "# Now we want to apply this vocabulary to convert new text to generate token id\n",
    "# When we want to convert the outputs of an LLM from numbers back into text, we need a way to turn token IDs into text. \n",
    "# For this, we can create an inverse version of the vocabulary that maps token IDs back to the corresponding text tokens.\n",
    "\n",
    "tokenizer = SimpleTokenizerV1(vocab)\n",
    "text = \"\"\"\"It's the last he painted you know,\" \n",
    "        Mrs Gisburn said with pardonable pride.\"\"\"\n",
    "\n",
    "ids = tokenizer.encode(text)\n",
    "print(ids)\n",
    "print(tokenizer.decode(ids))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 56, 2, 850, 988, 602, 533, 746, 1126, 596, 5, 1, 67, 38, 851, 1108, 754, 793, 7]\n",
      "\" It' s the last he painted you know,\" Mrs Gisburn said with pardonable pride.\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "id": "92b6519fe1741db8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-02T18:27:17.958785Z",
     "start_time": "2025-02-02T18:27:17.956456Z"
    }
   },
   "source": [
    "all_tokens = sorted(list(set(preprocessed))) # Make preprocessed a list so we can extend it\n",
    "all_tokens.extend([\"<|unk|>\", \"<|endoftext|>\"])\n",
    "# redo the vocab population\n",
    "vocab = {token:integer for integer, token in enumerate(all_tokens)}\n",
    "print(len(vocab.items()))\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1132\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "id": "d03ae607c2d20268",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-02T18:27:17.961266Z",
     "start_time": "2025-02-02T18:27:17.959371Z"
    }
   },
   "source": [
    "# Print the last 5 vocab items\n",
    "for i, item in enumerate(list(vocab.items())[-5:]):\n",
    "    print(item)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('younger', 1127)\n",
      "('your', 1128)\n",
      "('yourself', 1129)\n",
      "('<|unk|>', 1130)\n",
      "('<|endoftext|>', 1131)\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "id": "dcdf224cd056d26f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-02T18:27:17.964367Z",
     "start_time": "2025-02-02T18:27:17.961867Z"
    }
   },
   "source": [
    "from src.chapter02.SimpleTokenizerV2 import SimpleTokenizerV2\n",
    "\n",
    "text1 = \"Hello, do you like tea?\"\n",
    "text2 = \"In the sunlit terraces of the palace.\"\n",
    "text = \" <|endoftext|> \".join((text1, text2))\n",
    "print(text)\n",
    "tokenizer = SimpleTokenizerV2(vocab)\n",
    "print(tokenizer.encode(text))\n",
    "print(tokenizer.decode(tokenizer.encode(text)))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, do you like tea? <|endoftext|> In the sunlit terraces of the palace.\n",
      "[1130, 5, 355, 1126, 628, 975, 10, 1131, 55, 988, 956, 984, 722, 988, 1130, 7]\n",
      "<|unk|>, do you like tea? <|endoftext|> In the sunlit terraces of the <|unk|>.\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "id": "e4e4738351ee6b9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-02T18:27:17.975285Z",
     "start_time": "2025-02-02T18:27:17.964971Z"
    }
   },
   "source": [
    "### Byte Pair Encoding \n",
    "from importlib.metadata import version\n",
    "import tiktoken\n",
    "print(\"Tiktoken version: \", version(\"tiktoken\"))\n",
    "#print(\"Tiktoken version: \", tiktoken.__version__)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tiktoken version:  0.8.0\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "id": "ac57b0bafdc676f4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-02T18:27:18.137800Z",
     "start_time": "2025-02-02T18:27:17.976176Z"
    }
   },
   "source": [
    "#### This is the tokenizer using the GPT2 tokenization model\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "text = \"Hello, do you like tea? <|endoftext|> In the sunlit terraces of someunknownPlace.\"\n",
    "integers = tokenizer.encode(text, allowed_special={\"<|endoftext|>\"})\n",
    "print(f\"Encoded: {integers}\")\n",
    "strings = tokenizer.decode(integers)\n",
    "print(f\"Decoded: {strings}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded: [15496, 11, 466, 345, 588, 8887, 30, 220, 50256, 554, 262, 4252, 18250, 8812, 2114, 286, 617, 34680, 27271, 13]\n",
      "Decoded: Hello, do you like tea? <|endoftext|> In the sunlit terraces of someunknownPlace.\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "id": "2a6a5225d4fab946",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-02T18:27:18.141797Z",
     "start_time": "2025-02-02T18:27:18.139809Z"
    }
   },
   "source": [
    "print(tokenizer.encode(\"Akwirw ier\"))\n",
    "print(tokenizer.decode(tokenizer.encode(\"Akwirw ier\")))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[33901, 86, 343, 86, 220, 959]\n",
      "Akwirw ier\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "id": "1ab2edd9fb1ca03",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-02T18:27:18.146500Z",
     "start_time": "2025-02-02T18:27:18.142518Z"
    }
   },
   "source": [
    "# Let's now do Data Sampling with a sliding window\n",
    "# 1. Let's tokenize the entire story with BPE tokenizer first\n",
    "\n",
    "encoded_text = tokenizer.encode(raw_text)\n",
    "print(len(encoded_text))\n",
    "enc_sample = encoded_text[50:]\n",
    "\n",
    "# Now Let's start by defining x and y where x has input tokens and y the output tokens shifted by 1\n",
    "context_size = 4\n",
    "x = enc_sample[:context_size]\n",
    "y = enc_sample[1:context_size+1]\n",
    "# print(f\"x: {x}\")\n",
    "# print(f\"y:      {y}\")\n",
    "\n",
    "\n",
    "#####\n",
    "# Next word prediction tasks can now be created by \n",
    "for i in range(1, context_size + 1):\n",
    "    context = enc_sample[:i]\n",
    "    desired = enc_sample[i]\n",
    "    # print(f\"context input: {context} --> desired prediction: {desired}\")\n",
    "    # Now we create the input output target pairs\n",
    "    print(tokenizer.decode(context), \"---->\", tokenizer.decode([desired]))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5145\n",
      " and ---->  established\n",
      " and established ---->  himself\n",
      " and established himself ---->  in\n",
      " and established himself in ---->  a\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "id": "62094a4513e01008",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-02T18:27:18.148683Z",
     "start_time": "2025-02-02T18:27:18.147153Z"
    }
   },
   "source": [
    "# from Dataloader import Dataloader\n",
    "# \n",
    "# dataloader = Dataloader(batch_size=8, max_length=4, stride=4, shuffle=False, drop_last=True, num_workers=0)\n",
    "# dataloader = dataloader.get_instance(file_path, text_enc='utf-8', mode='r')\n",
    "# if dataloader is not None:\n",
    "#     data_iter = iter(dataloader)\n",
    "#     inputs, targets = next(data_iter)\n",
    "#     print(\"Loaded text data...\\n\")\n",
    "#     print(\"Inputs: \\n\", inputs)\n",
    "#     print(\"\\nTargets: \\n\", targets)\n",
    "# else: \n",
    "#     print(\"Failed loading \", dataloader)\n"
   ],
   "outputs": [],
   "execution_count": 12
  },
  {
   "cell_type": "code",
   "id": "fa0c4b5f389cdc4e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-02T18:27:18.297793Z",
     "start_time": "2025-02-02T18:27:18.149182Z"
    }
   },
   "source": [
    "import torch.nn\n",
    "from src.chapter02.Dataloader import Dataloader\n",
    "file_path = \"../data/the-verdict.txt\"\n",
    "\n",
    "####\n",
    "# Finally we need to create the embeddings for the tokens\n",
    "# If we have a batch size of 8 with 4 tokens each it'll be an 8 x 4 x 256 tensor\n",
    "max_length = 4  \n",
    "\n",
    "mydataloader = Dataloader(batch_size=8, max_length=max_length, stride=4, shuffle=False, drop_last=True, num_workers=0)\n",
    "dataloader = mydataloader.create_dataloader_v1(txt=raw_text)\n",
    "data_iter = iter(dataloader)\n",
    "inputs, targets = next(data_iter)\n",
    "# print(\"Input Token IDs:\\n\", inputs)\n",
    "# print(\"Input tensor shape: \", inputs.shape) \n",
    "\n",
    "# Now since self-attentions are position agnostic, we should add some positional data.\n",
    "# Absolute and relative positional data can be added. So let's create embeddings with say 256 dimensions\n",
    "vocab_size = 50257\n",
    "output_dim = 256\n",
    "# context_length = 1024\n",
    "\n",
    "## Now lets embed the input tensors\n",
    "token_embedding_layer = torch.nn.Embedding(num_embeddings=vocab_size, embedding_dim=output_dim)\n",
    "token_embeddings = token_embedding_layer(inputs)\n",
    "print(\"Token embeddings shape: \", token_embeddings.shape) #8x4x256\n",
    "\n",
    "\n",
    "# For a GPT model’s absolute position embedding approach, we just need to create another embedding \n",
    "# layer that has the same embedding dimension as the token_embedding_ layer:\n",
    "context_length = max_length     #context is length of positions we care about for attention\n",
    "pos_embedding_layer = torch.nn.Embedding(context_length, output_dim)\n",
    "pos_embeddings = pos_embedding_layer(torch.arange(context_length))\n",
    "print(\"Positional Embeddings Shape: \", pos_embeddings.shape) # 4x256\n",
    "#\n",
    "# Add the positional embeddings to token embeddings\n",
    "input_embeddings = token_embeddings + pos_embeddings\n",
    "print(\"Position Merged Input Embeddings Shape: \", input_embeddings.shape)\n",
    "#\n",
    "# Now lets look at the dataloader\n",
    "for batch in dataloader:\n",
    "    inputs, targets = batch\n",
    "    token_embeddings = token_embedding_layer(inputs)\n",
    "    pos_embeddings = pos_embedding_layer(torch.arange(max_length))\n",
    "    input_embeddings = token_embeddings + pos_embeddings\n",
    "    break\n",
    "#\n",
    "print(\"Batch Embeddings Shape: \", input_embeddings.shape)\n",
    "    \n",
    "print(\"Input tensor \", x)\n",
    "print(\"Target tensor\", y)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token embeddings shape:  torch.Size([8, 4, 256])\n",
      "Positional Embeddings Shape:  torch.Size([4, 256])\n",
      "Position Merged Input Embeddings Shape:  torch.Size([8, 4, 256])\n",
      "Batch Embeddings Shape:  torch.Size([8, 4, 256])\n",
      "Input tensor  [290, 4920, 2241, 287]\n",
      "Target tensor [4920, 2241, 287, 257]\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "cell_type": "code",
   "id": "95c2c3a6eb3eea50",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-02T18:27:18.302547Z",
     "start_time": "2025-02-02T18:27:18.298782Z"
    }
   },
   "source": [
    "# Chapter 3 - Attention\n",
    "#\n",
    "import torch\n",
    "\n",
    "# In self-attention our goal is to calculate context vector z(i) for each \n",
    "# element x(i) of the input sequence. Consider the following input sequence \n",
    "#\n",
    "inputs = torch.tensor(\n",
    "  [[0.43, 0.15, 0.89], # Your     (x^1)\n",
    "   [0.55, 0.87, 0.66], # journey  (x^2)\n",
    "   [0.57, 0.85, 0.64], # starts   (x^3)\n",
    "   [0.22, 0.58, 0.33], # with     (x^4)\n",
    "   [0.77, 0.25, 0.10], # one      (x^5)\n",
    "   [0.05, 0.80, 0.55]] # step     (x^6)\n",
    ")\n",
    "# inputs.to(device)\n",
    "print(\"Input sequence shape: \", inputs.shape)\n",
    "# \n",
    "# Now calculate weights for attention\n",
    "# Assume query is the second word \"journey\" or inputs[1] \n",
    "#\n",
    "query = inputs[1]\n",
    "# query.to(device)\n",
    "print(f\"Query is the 2nd word 'journey': {query}\")\n",
    "#\n",
    "attention_scores_2 = torch.empty(inputs.shape[0])\n",
    "# attention_scores_2.to(device)\n",
    "for idx, x_i in enumerate(inputs):\n",
    "    attention_scores_2[idx] = torch.dot(x_i, query)\n",
    "#    print(f\"Sequence Element [{idx}], attention_score: {attention_scores_2}\")\n",
    "print(f\"Final value of attention_score_2: {attention_scores_2}\")\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input sequence shape:  torch.Size([6, 3])\n",
      "Query is the 2nd word 'journey': tensor([0.5500, 0.8700, 0.6600])\n",
      "Final value of attention_score_2: tensor([0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865])\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "cell_type": "code",
   "id": "53ede1deb7d5678e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-02T18:27:18.308038Z",
     "start_time": "2025-02-02T18:27:18.303314Z"
    }
   },
   "source": [
    "\n",
    "## Note: For all elements if we were to calculate attention it'd be a O(n^2) operation\n",
    "# NOW we normalize the attention weights, so they sum up to 1\n",
    "attention_weights_2_tmp = attention_scores_2 / attention_scores_2.sum()\n",
    "# attention_weights_2_tmp.to(device)\n",
    "print(\"Normalized attention weights:\", attention_weights_2_tmp)\n",
    "print(\"Sum of attention weights:\", attention_weights_2_tmp.sum())\n",
    "\n",
    "## Generally we normalize using the softmax to do the normalization\n",
    "\n",
    "# # define a softmax function\n",
    "def softmax_naive(tensor_x):\n",
    "    return torch.exp(tensor_x) / torch.exp(tensor_x).sum(dim=0, keepdim=True)\n",
    "# \n",
    "\n",
    "attention_scores_2_naive = softmax_naive(attention_scores_2)\n",
    "# attention_scores_2_naive.to(device)\n",
    "\n",
    "print(\"Attention weights naive:\", attention_scores_2_naive)\n",
    "print (\"Naive Sum: \", attention_scores_2_naive.sum())\n",
    "# \n",
    "# Generally we normalize using the torch.softmax() to do the normalization\n",
    "# Softmax ensures its always positive and always adds up to 1\n",
    "#\n",
    "attention_weights_2_torch_softmax = torch.softmax(attention_scores_2, dim=0)\n",
    "# attention_weights_2_torch_softmax.to(device)\n",
    "print(\"Attention weights torch softmax:\", attention_weights_2_torch_softmax)\n",
    "# print(\"Attention weights torch softmax Sum: \", attention_weights_2_torch_softmax.sum())\n",
    "\n",
    "# Now that we have calculated the normalized attention weights, we are ready for the final step.\n",
    "# Calculate the context vector z(2) by multiplying the embedded input tokens x(i), \n",
    "# with the corresponding normalized attention weights and then summing the resultant vectors\n",
    "#\n",
    "query = inputs[1]\n",
    "# query.to(device)\n",
    "#\n",
    "context_vec_2 = torch.zeros(query.shape)\n",
    "# context_vec_2.to(device)\n",
    "#\n",
    "for i, x_i in enumerate(inputs):\n",
    "    context_vec_2 += (attention_weights_2_torch_softmax[i] * x_i)\n",
    "print(\"Context vector z2: \", context_vec_2)\n",
    "\n",
    "#\n",
    "# Now in similar fashion lets calculate attention scores for all the input sequences \n",
    "attention_scores = torch.empty(inputs.shape[0],inputs.shape[0])\n",
    "# attention_scores.to(device)\n",
    "print(\"\\nAttention Scores matrix shape: \", attention_scores.shape)\n",
    "#\n",
    "# Using for loops\n",
    "#\n",
    "# for i, x_i in enumerate(inputs):\n",
    "#     for j, x_j in enumerate(inputs):\n",
    "#         attention_scores[i, j] = torch.dot(x_i, x_j)\n",
    "# #\n",
    "#print(attention_scores)\n",
    "#\n",
    "# Using matrix multiplication we can do it faster\n",
    "#\n",
    "attention_scores_m = inputs @ inputs.T\n",
    "# attention_scores_m.to(device)\n",
    "#print(\"Normalized attention scores \\n\", attention_scores_m)\n",
    "\n",
    "# Just as before lets normalize the rows, so they sum up to 1\n",
    "# NOTE: Here dim = -1 means we are applying the softmax along the last dimension of the attention_scores_m tensor\n",
    "#\n",
    "attention_weights = torch.softmax(attention_scores_m, dim=-1)\n",
    "# attention_weights.to(device)\n",
    "#print(\"Normalized ATTENTION weights \\n\", attention_weights)\n",
    "# print(\"Softmax Sums:\\n\", attention_weights.sum(dim=-1))\n",
    "\n",
    "# FINAL STEP\n",
    "# Now let's calculate the context vectors for all the input by multiplying the input with attention weights\n",
    "all_context_vectors = attention_weights @ inputs # Matrix multiplication\n",
    "# all_context_vectors.to(device)\n",
    "#print(\"Context vector for the entire sequence\\n\", all_context_vectors)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalized attention weights: tensor([0.1455, 0.2278, 0.2249, 0.1285, 0.1077, 0.1656])\n",
      "Sum of attention weights: tensor(1.0000)\n",
      "Attention weights naive: tensor([0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581])\n",
      "Naive Sum:  tensor(1.)\n",
      "Attention weights torch softmax: tensor([0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581])\n",
      "Context vector z2:  tensor([0.4419, 0.6515, 0.5683])\n",
      "\n",
      "Attention Scores matrix shape:  torch.Size([6, 6])\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "cell_type": "code",
   "id": "fa980b537b01e646",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-02T18:27:18.314575Z",
     "start_time": "2025-02-02T18:27:18.308643Z"
    }
   },
   "source": [
    "\n",
    "###\n",
    "### 3.4.1 Using weighted matrix\n",
    "###\n",
    "#\n",
    "# Computing the attention weights step by step\n",
    "#\n",
    "torch.manual_seed(123)\n",
    "x_2 = inputs[1]\n",
    "# x_2.to(device)\n",
    "d_in = inputs.shape[1]\n",
    "d_out = 2\n",
    "print(\"x_2: \", x_2)\n",
    "# Now let's initialize 3 weighted matrices Wq, Wk and Wv\n",
    "# Setting requires_grad = False, to reduce clutter, but for model training this should be set to True\n",
    "\n",
    "W_query = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)\n",
    "W_key   = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)\n",
    "W_value = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)\n",
    "#\n",
    "# Next we compute the query, key and value vectors\n",
    "# Note the output is a 2 dimenstional vector because we set dout to 2\n",
    "#\n",
    "# W_query.to(device)\n",
    "# W_key.to(device)\n",
    "# W_value.to(device)\n",
    "#\n",
    "# Now the dot product with the input\n",
    "#\n",
    "query_2 = x_2 @ W_query\n",
    "key_2   = x_2 @ W_key\n",
    "value_2 = x_2 @ W_value\n",
    "#\n",
    "# query_2.to(device)\n",
    "# key_2.to(device)\n",
    "# value_2.to(device)\n",
    "#\n",
    "print(\"Query 2: \", query_2)\n",
    "print(\"Key 2:   \", key_2)\n",
    "print(\"Value 2: \", value_2)\n",
    "#\n",
    "print(\"\\n\")\n",
    "#\n",
    "\n",
    "keys = inputs @ W_key \n",
    "values = inputs @ W_value\n",
    "# keys.to(device)\n",
    "# values.to(device)\n",
    "print(\"keys.shape:\", keys.shape)\n",
    "print(\"values.shape:\", values.shape)\n",
    "\n",
    "keys_2 = keys[1]\n",
    "\n",
    "attn_score_22 = query_2.dot(keys_2)\n",
    "# attn_score_22.to(device)\n",
    "# attn_score_22 = query_2 @ keys_2\n",
    "print(\"Attention (dot) score 22:\", attn_score_22)\n",
    "\n",
    "# Generalizing across all inputs\n",
    "attn_scores_2 = query_2 @ keys.T\n",
    "# attn_scores_2.to(device)\n",
    "print(\"Attention \\\\@ Scores 2: \", attn_scores_2)\n",
    "# Check the second element is same as previously calculated attention score\n",
    "#\n",
    "# We compute the attention weights by scaling the attention scores and using the softmax function. \n",
    "# However, now we scale the attention scores by dividing them by the square root of the embedding \n",
    "# dimension of the keys\n",
    "\n",
    "attn_weights_2 = torch.softmax(attn_scores_2 / keys.shape[-1]**0.5, dim=-1)\n",
    "# attn_weights_2.to(device)\n",
    "print(\"attn_weights_2: \", attn_weights_2)\n",
    "\n",
    "# The reason for the normalization by square root of embedding dimension size is to improve the training performance by avoiding small gradients. \n",
    "# For instance, when scaling up the embedding dimension, which is typically > 1,000 for GPT-like LLMs, large dot products can result in \n",
    "# very small gradients during backpropagation (due to the softmax function applied to them). \n",
    "# As dot products increase, the softmax function behaves more like a step function, resulting in gradients nearing zero. \n",
    "# These small gradients can drastically slow down learning or cause training to stagnate.\n",
    "#\n",
    "# The scaling by the square root of the embedding dimension is the reason why this self-attention mechanism is also called scaled-dot product attention.\n",
    "# Similar to when we computed the context vector as a weighted sum over the input vectors \n",
    "# we now compute the context vector as a weighted sum over the value vectors. \n",
    "# Here, the attention weights serve as a weighting factor that weighs the respective importance of each value vector\n",
    "context_vec_2 = attn_weights_2 @ values\n",
    "# context_vec_2.to(device)\n",
    "print(\"context_vec_2: \", context_vec_2)\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_2:  tensor([0.5500, 0.8700, 0.6600])\n",
      "Query 2:  tensor([0.4306, 1.4551])\n",
      "Key 2:    tensor([0.4433, 1.1419])\n",
      "Value 2:  tensor([0.3951, 1.0037])\n",
      "\n",
      "\n",
      "keys.shape: torch.Size([6, 2])\n",
      "values.shape: torch.Size([6, 2])\n",
      "Attention (dot) score 22: tensor(1.8524)\n",
      "Attention \\@ Scores 2:  tensor([1.2705, 1.8524, 1.8111, 1.0795, 0.5577, 1.5440])\n",
      "attn_weights_2:  tensor([0.1500, 0.2264, 0.2199, 0.1311, 0.0906, 0.1820])\n",
      "context_vec_2:  tensor([0.3061, 0.8210])\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "cell_type": "code",
   "id": "2ca52ea3a908fe20",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-02T18:27:18.323584Z",
     "start_time": "2025-02-02T18:27:18.315202Z"
    }
   },
   "source": [
    "from src.chapter03.SelfAttention_v2 import SelfAttention_v2\n",
    "\n",
    "torch.manual_seed(789)\n",
    "self_attn_v2 = SelfAttention_v2(d_in, d_out)\n",
    "#print(\"Context vectors from SelfAtten_v2: \\n\", self_attn_v2(inputs))\n",
    "\n",
    "# Note since the input contains 6 embedding vectors, the output also has 6 rows of context vectors\n",
    "\n",
    "\n",
    "# Causal Attention \n",
    "# First we apply softmax to the attention scores then mask with 0 above the diagonal and then normalize the rows to 1\n",
    "#\n",
    "queries = self_attn_v2.W_query(inputs)\n",
    "# queries.to(device)\n",
    "\n",
    "keys = self_attn_v2.W_key(inputs)\n",
    "# keys.to(device)\n",
    "\n",
    "attn_scores = queries @ keys.T\n",
    "# attn_scores.to(device)\n",
    "\n",
    "attn_weights = torch.softmax(attn_scores / keys.shape[-1] ** 0.5, dim=-1)\n",
    "# attn_weights.to(device)\n",
    "#print(\"Attention Wrights: \\n\",attn_weights)\n",
    "\n",
    "# Now mask the values above diagonal as 0 using the tril() function\n",
    "#\n",
    "context_length = attn_scores.shape[0]\n",
    "#\n",
    "mask_simple = torch.tril(torch.ones(context_length, context_length))\n",
    "#print(\"Mask Simple: \\n\", mask_simple)\n",
    "#\n",
    "# Now simply multiply them to prevent the look ahead \n",
    "#\n",
    "masked_attention_weights = attn_weights * mask_simple\n",
    "# masked_attention_weights.to(device)\n",
    "#print(\"Masked attention weights: \\n\", masked_attention_weights)\n",
    "\n",
    "#\n",
    "# Now re-normalize to make sure rows add up to 1. To do this we divide each element by sum of each row\n",
    "#\n",
    "row_sums = masked_attention_weights.sum(dim=-1, keepdim=True)\n",
    "#print(\"row_sums: \\n\", row_sums)\n",
    "masked_simple_norm = masked_attention_weights / row_sums\n",
    "print(\"Masked & re-normalized weights: \\n\", masked_simple_norm)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x = tensor([1.], device='mps:0') using 'mps:0' backend\n",
      "Masked & re-normalized weights: \n",
      " tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.5517, 0.4483, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.3800, 0.3097, 0.3103, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2758, 0.2460, 0.2462, 0.2319, 0.0000, 0.0000],\n",
      "        [0.2175, 0.1983, 0.1984, 0.1888, 0.1971, 0.0000],\n",
      "        [0.1935, 0.1663, 0.1666, 0.1542, 0.1666, 0.1529]],\n",
      "       grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "cell_type": "code",
   "id": "f3a442b3d9b7413a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-02T18:27:18.328454Z",
     "start_time": "2025-02-02T18:27:18.324353Z"
    }
   },
   "source": [
    "# A more efficient way to obtain masked attention weights is to mask the attention scores with \n",
    "# negative infinity before applying softmax function. (e^negative infinity -> 0)\n",
    "# We can implement this masking by replacing values above the diagonal with 1 and then replacing them \n",
    "# with negative infinity\n",
    "#\n",
    "mask = torch.triu(torch.ones(context_length, context_length), diagonal=1)\n",
    "# mask.to(device)\n",
    "masked = attn_scores.masked_fill(mask.bool(), -torch.inf)\n",
    "# masked.to(device)\n",
    "print(\"Masked attention weights: \\n\", masked)\n",
    "#\n",
    "# Now apply the softmax function \n",
    "#\n",
    "attn_weights = torch.softmax(masked / keys.shape[-1] ** 0.5, dim=-1)\n",
    "print(\"Softmax'd Attention weights: \\n\", attn_weights)\n",
    "\n",
    "# Now we can use these modified attention weights to calculate the context vector\n",
    "#\n",
    "context_vec = attn_weights @ values\n",
    "print(\"context_vec: \\n\", context_vec)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Masked attention weights: \n",
      " tensor([[0.2899,   -inf,   -inf,   -inf,   -inf,   -inf],\n",
      "        [0.4656, 0.1723,   -inf,   -inf,   -inf,   -inf],\n",
      "        [0.4594, 0.1703, 0.1731,   -inf,   -inf,   -inf],\n",
      "        [0.2642, 0.1024, 0.1036, 0.0186,   -inf,   -inf],\n",
      "        [0.2183, 0.0874, 0.0882, 0.0177, 0.0786,   -inf],\n",
      "        [0.3408, 0.1270, 0.1290, 0.0198, 0.1290, 0.0078]],\n",
      "       grad_fn=<MaskedFillBackward0>)\n",
      "Softmax'd Attention weights: \n",
      " tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.5517, 0.4483, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.3800, 0.3097, 0.3103, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2758, 0.2460, 0.2462, 0.2319, 0.0000, 0.0000],\n",
      "        [0.2175, 0.1983, 0.1984, 0.1888, 0.1971, 0.0000],\n",
      "        [0.1935, 0.1663, 0.1666, 0.1542, 0.1666, 0.1529]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "context_vec: \n",
      " tensor([[0.1855, 0.8812],\n",
      "        [0.2795, 0.9361],\n",
      "        [0.3133, 0.9508],\n",
      "        [0.2994, 0.8595],\n",
      "        [0.2702, 0.7554],\n",
      "        [0.2772, 0.7618]], grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "cell_type": "code",
   "id": "d6b38e5f420f6067",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-02T18:27:18.333244Z",
     "start_time": "2025-02-02T18:27:18.329263Z"
    }
   },
   "source": [
    "# Masking additional weights with dropout\n",
    "# Drop out in the attention mechanism is applied at 2 specific times: \n",
    "# 1. After calculating the attention weights\n",
    "# 2. After applying the attention weights to value vectors\n",
    "# Here we will apply the dropout mask after computing the attention weights\n",
    "#\n",
    "# Lets use a dropout rate of 50% meaning half the attention weights will be masked out. \n",
    "# Normally it's a much lower rate like 0.1 or 0.2\n",
    "torch.manual_seed(123)\n",
    "dropout = torch.nn.Dropout(0.5)\n",
    "example = torch.ones(6, 6)\n",
    "print(dropout(example))\n",
    "#\n",
    "# Since we are applying 50% dropout, to compensate for reduction in active elements\n",
    "# we are going to scale up the values of remaining elements by a factor of 1/0.5 = 2\n",
    "# This scaling is crucial to maintain the balance of the attention weights\n",
    "#\n",
    "torch.manual_seed(123)\n",
    "print(\"Dropped out attention weights: \\n\", dropout(attn_weights))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2., 2., 0., 2., 2., 0.],\n",
      "        [0., 0., 0., 2., 0., 2.],\n",
      "        [2., 2., 2., 2., 0., 2.],\n",
      "        [0., 2., 2., 0., 0., 2.],\n",
      "        [0., 2., 0., 2., 0., 2.],\n",
      "        [0., 2., 2., 2., 2., 0.]])\n",
      "Dropped out attention weights: \n",
      " tensor([[2.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.7599, 0.6194, 0.6206, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.4921, 0.4925, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.3966, 0.0000, 0.3775, 0.0000, 0.0000],\n",
      "        [0.0000, 0.3327, 0.3331, 0.3084, 0.3331, 0.0000]],\n",
      "       grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "cell_type": "code",
   "id": "50701dd69a456857",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-02T18:27:18.337744Z",
     "start_time": "2025-02-02T18:27:18.333826Z"
    }
   },
   "source": [
    "from src.chapter03.CausalAttention import CausalAttention\n",
    "\n",
    "# Let’s ensure that the code can handle batches consisting of more than one input so that \n",
    "# the CausalAttention class supports the batch outputs produced by the data loader\n",
    "# To simulate batch input lets duplicate the input text\n",
    "#\n",
    "batch = torch.stack((inputs, inputs), dim=0)\n",
    "# batch.to(device)\n",
    "print(\"batch: \\n\", batch.shape)\n",
    "# print(batch)\n",
    "#\n",
    "# We can now use the CausalAttention class as follows\n",
    "#\n",
    "torch.manual_seed(123)\n",
    "context_layer = batch.shape[1]\n",
    "causal_attn = CausalAttention(d_in, d_out, context_length, 0.0, False)\n",
    "context_vecs = causal_attn(batch)\n",
    "print(\"context_vecs: \\n\", context_vecs.shape)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch: \n",
      " torch.Size([2, 6, 3])\n",
      "context_vecs: \n",
      " torch.Size([2, 6, 2])\n"
     ]
    }
   ],
   "execution_count": 20
  },
  {
   "cell_type": "code",
   "id": "9d8058957dc253b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-02T18:27:18.346843Z",
     "start_time": "2025-02-02T18:27:18.338573Z"
    }
   },
   "source": [
    "from src.chapter03.MultiHeadAttentionWrapper import MultiHeadAttentionWrapper\n",
    "\n",
    "# Multi Head Attention\n",
    "# Now if we use the MultiHeadAttentionWrapper class with two attention heads, and CausalAttention \n",
    "# output dimension d_out = 2, we get a 4 dimensional context vector (d_out * num_heads = 4).\n",
    "#\n",
    "torch.manual_seed(123)\n",
    "context_length = batch.shape[1]\n",
    "d_in, d_out = 3, 2\n",
    "multi_head_attn = MultiHeadAttentionWrapper(d_in, d_out, context_length, dropout=0.0, num_heads=2, qkv_bias=False)\n",
    "context_vecs = multi_head_attn(batch)\n",
    "print(\"context_vecs: \\n\", context_vecs)\n",
    "print(\"context_vecs.shape: \\n\", context_vecs.shape)\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "context_vecs: \n",
      " tensor([[[-0.4519,  0.2216,  0.4772,  0.1063],\n",
      "         [-0.5874,  0.0058,  0.5891,  0.3257],\n",
      "         [-0.6300, -0.0632,  0.6202,  0.3860],\n",
      "         [-0.5675, -0.0843,  0.5478,  0.3589],\n",
      "         [-0.5526, -0.0981,  0.5321,  0.3428],\n",
      "         [-0.5299, -0.1081,  0.5077,  0.3493]],\n",
      "\n",
      "        [[-0.4519,  0.2216,  0.4772,  0.1063],\n",
      "         [-0.5874,  0.0058,  0.5891,  0.3257],\n",
      "         [-0.6300, -0.0632,  0.6202,  0.3860],\n",
      "         [-0.5675, -0.0843,  0.5478,  0.3589],\n",
      "         [-0.5526, -0.0981,  0.5321,  0.3428],\n",
      "         [-0.5299, -0.1081,  0.5077,  0.3493]]], grad_fn=<CatBackward0>)\n",
      "context_vecs.shape: \n",
      " torch.Size([2, 6, 4])\n"
     ]
    }
   ],
   "execution_count": 21
  },
  {
   "cell_type": "code",
   "id": "f20088921d8113c0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-02T18:27:18.351275Z",
     "start_time": "2025-02-02T18:27:18.347557Z"
    }
   },
   "source": [
    "a = torch.tensor([[[[0.2745, 0.6584, 0.2775, 0.8573],\n",
    "                    [0.8993, 0.0390, 0.9268, 0.7388],\n",
    "                    [0.7179, 0.7058, 0.9156, 0.4340]],\n",
    "\n",
    "                   [[0.0772, 0.3565, 0.1479, 0.5331],\n",
    "                    [0.4066, 0.2318, 0.4545, 0.9737],\n",
    "                    [0.4606, 0.5159, 0.4220, 0.5786]]]])\n",
    "# print(a.transpose(2, 3))\n",
    "a.to(device)\n",
    "\n",
    "first_head = a[0, 0, :, :]\n",
    "first_res = first_head @ first_head.T\n",
    "print(\"First head:\\n\", first_res)\n",
    "\n",
    "second_head = a[0, 1, :, :]\n",
    "second_res = second_head @ second_head.T\n",
    "print(\"\\nSecond head:\\n\", second_res)\n",
    "\n",
    "print(f\"Batched: \\n{a @ a.transpose(2, 3)}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First head:\n",
      " tensor([[1.3208, 1.1631, 1.2879],\n",
      "        [1.1631, 2.2150, 1.8424],\n",
      "        [1.2879, 1.8424, 2.0402]])\n",
      "\n",
      "Second head:\n",
      " tensor([[0.4391, 0.7003, 0.5903],\n",
      "        [0.7003, 1.3737, 1.0620],\n",
      "        [0.5903, 1.0620, 0.9912]])\n",
      "Batched: \n",
      "tensor([[[[1.3208, 1.1631, 1.2879],\n",
      "          [1.1631, 2.2150, 1.8424],\n",
      "          [1.2879, 1.8424, 2.0402]],\n",
      "\n",
      "         [[0.4391, 0.7003, 0.5903],\n",
      "          [0.7003, 1.3737, 1.0620],\n",
      "          [0.5903, 1.0620, 0.9912]]]])\n"
     ]
    }
   ],
   "execution_count": 22
  },
  {
   "cell_type": "code",
   "id": "df6a05dfc13ebbe4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-02T18:27:18.356698Z",
     "start_time": "2025-02-02T18:27:18.351935Z"
    }
   },
   "source": [
    "from src.chapter03.MultiHeadAttention import MultiHeadAttention\n",
    "\n",
    "torch.manual_seed(123)\n",
    "batch_size, context_length, d_in = batch.shape\n",
    "print(\"Batch Shape: \\n\", batch.shape)\n",
    "d_out = 2\n",
    "mha = MultiHeadAttention(d_in, d_out, context_length, 0.0, num_heads=2)\n",
    "context_vecs = mha(batch)\n",
    "print(context_vecs)\n",
    "print(\"Context vector shape: \\n\", context_vecs.shape)\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Shape: \n",
      " torch.Size([2, 6, 3])\n",
      "tensor([[[0.3190, 0.4858],\n",
      "         [0.2943, 0.3897],\n",
      "         [0.2856, 0.3593],\n",
      "         [0.2693, 0.3873],\n",
      "         [0.2639, 0.3928],\n",
      "         [0.2575, 0.4028]],\n",
      "\n",
      "        [[0.3190, 0.4858],\n",
      "         [0.2943, 0.3897],\n",
      "         [0.2856, 0.3593],\n",
      "         [0.2693, 0.3873],\n",
      "         [0.2639, 0.3928],\n",
      "         [0.2575, 0.4028]]], grad_fn=<ViewBackward0>)\n",
      "Context vector shape: \n",
      " torch.Size([2, 6, 2])\n"
     ]
    }
   ],
   "execution_count": 23
  },
  {
   "cell_type": "markdown",
   "id": "3f958161a85ca549",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "## Chapter 4: Implementing GPT from Scratch to generate text\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "id": "16f34621162871e9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-02T18:27:18.362913Z",
     "start_time": "2025-02-02T18:27:18.360837Z"
    }
   },
   "source": [
    "GPT_CONFIG_124M = {\n",
    "    \"vocab_size\": 50257,     # Vocabulary size\n",
    "    \"context_length\": 1024,  # Context length\n",
    "    \"emb_dim\": 768,          # Embedding dimension\n",
    "    \"n_heads\": 12,           # Number of attention heads\n",
    "    \"n_layers\": 12,          # Number of layers\n",
    "    \"drop_rate\": 0.1,        # Dropout rate\n",
    "    \"qkv_bias\": False,\n",
    "    \"model_name\": \"GPTModel\",\n",
    "    \"lr\": 5e-4,\n",
    "    \"weight_decay\": 0.1\n",
    "}"
   ],
   "outputs": [],
   "execution_count": 24
  },
  {
   "cell_type": "markdown",
   "id": "5ac1cbf145ad6daf",
   "metadata": {},
   "source": [
    "## Here is the proposed architecture and order of implementation\n",
    "![image](../data/4-3.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "75010152b67235a2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-02T18:27:19.011801Z",
     "start_time": "2025-02-02T18:27:18.363989Z"
    }
   },
   "source": [
    "from src.chapter04.DummyGPTModel import DummyGPTModel\n",
    "import tiktoken\n",
    "#\n",
    "torch.manual_seed(123)\n",
    "#\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "batch = []\n",
    "txt1 = \"Every effort moves you\"\n",
    "txt2 = \"Every day holds a\"\n",
    "batch.clear()\n",
    "batch.append(torch.tensor(tokenizer.encode(txt1)).to(device))\n",
    "batch.append(torch.tensor(tokenizer.encode(txt2)).to(device))\n",
    "batch = torch.stack(batch, dim=0).to(device)\n",
    "batch.to(device)\n",
    "print(\"Input Batch: \\n\", batch)\n",
    "print(\"Input batch shape: \\n\", batch.shape)\n",
    "#\n",
    "# Next, we initialize a new 124-million-parameter DummyGPTModel instance \n",
    "# and feed it the tokenized batch\n",
    "#\n",
    "model = DummyGPTModel(GPT_CONFIG_124M)\n",
    "model.to(device)\n",
    "logits = model(batch)\n",
    "print(\"Output shape: \\n\", logits.shape)\n",
    "#print(logits)\n",
    "#                      \n",
    "#"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Batch: \n",
      " tensor([[6109, 3626, 6100,  345],\n",
      "        [6109, 1110, 6622,  257]], device='mps:0')\n",
      "Input batch shape: \n",
      " torch.Size([2, 4])\n",
      "Output shape: \n",
      " torch.Size([2, 4, 50257])\n"
     ]
    }
   ],
   "execution_count": 25
  },
  {
   "cell_type": "code",
   "id": "c06060ecd0f8f8be",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-02T18:27:19.017862Z",
     "start_time": "2025-02-02T18:27:19.012533Z"
    }
   },
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "# Let’s now implement LAYER Normalization to improve the stability and efficiency of the training.\n",
    "# The main idea behind LAYER Normalization is to adjust the activations (outputs) of a deep\n",
    "# neural network layer to have a mean of 0 and a variance of 1\n",
    "#\n",
    "# This adjustment speeds up the convergence.\n",
    "#\n",
    "torch.manual_seed(123)\n",
    "torch.set_printoptions(sci_mode=False)\n",
    "#\n",
    "batch_example = torch.randn(2, 5)\n",
    "layer = nn.Sequential(nn.Linear(5,6), nn.ReLU())\n",
    "out = layer(batch_example)\n",
    "print(\"Layer: \\n\",out)\n",
    "#\n",
    "# The NN Layer contains the non-linear activation ReLU which 0's out the negative values\n",
    "# \n",
    "mean = out.mean(dim=-1, keepdim=True)\n",
    "var = out.var(dim=-1, keepdim=True)\n",
    "print(\"Mean: \", mean)\n",
    "print(\"Variance: \", var)\n",
    "print(\"\\n\")\n",
    "#\n",
    "# Next, let’s apply layer normalization to the layer outputs we obtained earlier. \n",
    "# The operation consists of subtracting the mean and dividing by the square root \n",
    "# of the variance (also known as the standard deviation):\n",
    "#\n",
    "eps = 1e-5\n",
    "out_norm = (out - mean) / torch.sqrt(var)\n",
    "# Dim = -1 indicates statistics along the last dimention\n",
    "mean = out_norm.mean(dim=-1, keepdim=True)\n",
    "var = out_norm.var(dim=-1, keepdim=True)\n",
    "torch.set_printoptions(sci_mode=True)\n",
    "print(\"Normalized Layer Outputs: \\n\", out_norm)\n",
    "print(\"Mean: \\n\", mean)\n",
    "print(\"Variance: \\n\", var)\n",
    "print(\"-------------------------\\n\")\n",
    "\n",
    "torch.set_printoptions(sci_mode=False)\n",
    "print(\"Mean:\\n\", mean)\n",
    "print(\"Variance:\\n\", var)\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: \n",
      " tensor([[0.2260, 0.3470, 0.0000, 0.2216, 0.0000, 0.0000],\n",
      "        [0.2133, 0.2394, 0.0000, 0.5198, 0.3297, 0.0000]],\n",
      "       grad_fn=<ReluBackward0>)\n",
      "Mean:  tensor([[0.1324],\n",
      "        [0.2170]], grad_fn=<MeanBackward1>)\n",
      "Variance:  tensor([[0.0231],\n",
      "        [0.0398]], grad_fn=<VarBackward0>)\n",
      "\n",
      "\n",
      "Normalized Layer Outputs: \n",
      " tensor([[6.1585e-01, 1.4126e+00, -8.7188e-01, 5.8723e-01, -8.7188e-01, -8.7188e-01],\n",
      "        [-1.8865e-02, 1.1211e-01, -1.0876e+00, 1.5173e+00, 5.6474e-01, -1.0876e+00]],\n",
      "       grad_fn=<DivBackward0>)\n",
      "Mean: \n",
      " tensor([[-5.9605e-08],\n",
      "        [ 1.9868e-08]], grad_fn=<MeanBackward1>)\n",
      "Variance: \n",
      " tensor([[1.0000e+00],\n",
      "        [1.0000e+00]], grad_fn=<VarBackward0>)\n",
      "-------------------------\n",
      "\n",
      "Mean:\n",
      " tensor([[    -0.0000],\n",
      "        [     0.0000]], grad_fn=<MeanBackward1>)\n",
      "Variance:\n",
      " tensor([[1.0000],\n",
      "        [1.0000]], grad_fn=<VarBackward0>)\n"
     ]
    }
   ],
   "execution_count": 26
  },
  {
   "cell_type": "code",
   "id": "a39e430a037896f0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-02T18:27:19.021446Z",
     "start_time": "2025-02-02T18:27:19.018446Z"
    }
   },
   "source": [
    "from src.chapter04.LayerNorm import LayerNorm\n",
    "\n",
    "# Previously we used unbiased = False in our variance calculation. This doesn't \n",
    "# apply Bessel's correction where divisor is n-1 instead of n. But this is \n",
    "# compatible with GPT-2\n",
    "\n",
    "ln = LayerNorm(emb_dim = 5)\n",
    "out_ln = ln(batch_example)\n",
    "mean = out_ln.mean(dim=-1, keepdim=True)\n",
    "var = out_ln.var(dim=-1, unbiased=False, keepdim=True)\n",
    "print(\"Mean: \\n\", mean)\n",
    "print(\"Variance: \\n\", var)\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean: \n",
      " tensor([[    -0.0000],\n",
      "        [     0.0000]], grad_fn=<MeanBackward1>)\n",
      "Variance: \n",
      " tensor([[1.0000],\n",
      "        [1.0000]], grad_fn=<VarBackward0>)\n"
     ]
    }
   ],
   "execution_count": 27
  },
  {
   "cell_type": "code",
   "id": "3130c2c83093ee01",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-02T18:27:19.351495Z",
     "start_time": "2025-02-02T18:27:19.022702Z"
    }
   },
   "source": [
    "# Let us see how the GELU (Gaussian Error Linear Unit) stacks up against \n",
    "# # RELU (REctified Linear Unit)\n",
    "from src.chapter04.GELU import GELU\n",
    "import matplotlib.pyplot as plt\n",
    "gelu, relu = GELU(), nn.ReLU()\n",
    "\n",
    "x = torch.linspace(-3, 3, 100)\n",
    "y_gelu, y_relu = gelu(x), relu(x)\n",
    "plt.figure(figsize=(8, 3))\n",
    "for i, (y, label) in enumerate(zip([y_gelu, y_relu], [\"GELU\", \"ReLU\"]), 1):\n",
    "    plt.subplot(1, 2, i)\n",
    "    plt.plot(x, y)\n",
    "    plt.title(f\"{label} activation function\")\n",
    "    plt.xlabel(\"x\")\n",
    "    plt.ylabel(f\"{label}(x)\")\n",
    "    plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 800x300 with 2 Axes>"
      ],
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 28
  },
  {
   "cell_type": "code",
   "id": "e496e641f9044a67",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-02T18:27:19.371339Z",
     "start_time": "2025-02-02T18:27:19.352311Z"
    }
   },
   "source": [
    "from src.chapter04.SimpleFeedForward import SimpleFeedForward\n",
    "\n",
    "# As we can see the smoothness of the GELU can lead to better optimization properties during training\n",
    "# as it allows more nuanced finer adjustments to models parameters. In contrast, RELU has a sharp corner\n",
    "# that can make adjustments difficult for very deep networks.\n",
    "#\n",
    "# Next we look at implementing a feed forward network with GELU activations\n",
    "# See SimpleFeedForward.py\n",
    "#\n",
    "sff = SimpleFeedForward(GPT_CONFIG_124M)\n",
    "x = torch.rand(2, 3, 768)\n",
    "out = sff(x)\n",
    "print(out.shape)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 768])\n"
     ]
    }
   ],
   "execution_count": 29
  },
  {
   "cell_type": "code",
   "id": "98a59bc5db854fa4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-02T18:27:19.377943Z",
     "start_time": "2025-02-02T18:27:19.372136Z"
    }
   },
   "source": [
    "from src.chapter04.ExampleDeepNeuralNetwork import ExampleDeepNeuralNetwork\n",
    "\n",
    "# Next we implement Shortcut Connections\n",
    "# Each layer will be initialized such that it accepts an example with three input \n",
    "# values and returns three output values.\n",
    "torch.manual_seed(123)\n",
    "layer_sizes = [3, 3, 3, 3, 3, 1]\n",
    "sample_input = torch.tensor([[1., 0., -1.]])\n",
    "model_without_shortcut = ExampleDeepNeuralNetwork(layer_sizes, use_shortcut=False)\n",
    "# model.to(device)\n",
    "\n",
    "# Next lets print the gradients\n",
    "def print_gradients(nnmodel, input_x):\n",
    "    output = nnmodel(input_x)\n",
    "    target = torch.tensor([[0.]])\n",
    "    loss = nn.MSELoss()\n",
    "    loss = loss(output, target)\n",
    "    torch.autograd.set_detect_anomaly(True)\n",
    "    loss.backward()\n",
    "    for name, param in nnmodel.named_parameters():\n",
    "        # print(name, \" = \", param)\n",
    "        if 'weight' in name:\n",
    "            print(f\"{name} has gradient mean of {param.grad.abs().mean().item()}\")\n",
    "\n",
    "#\n",
    "# Now Lets use this function to print the gradients calculated by loss.backward()\n",
    "print_gradients(model_without_shortcut, sample_input)\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layers.0.0.weight has gradient mean of 0.00020173587836325169\n",
      "layers.1.0.weight has gradient mean of 0.0001201116101583466\n",
      "layers.2.0.weight has gradient mean of 0.0007152041071094573\n",
      "layers.3.0.weight has gradient mean of 0.0013988735154271126\n",
      "layers.4.0.weight has gradient mean of 0.005049645435065031\n"
     ]
    }
   ],
   "execution_count": 30
  },
  {
   "cell_type": "code",
   "id": "4bfdfab7cb5bcc5f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-02T18:27:19.393219Z",
     "start_time": "2025-02-02T18:27:19.378702Z"
    }
   },
   "source": [
    "# As you can see above gradients become tiny aka Vanishing from Layer4 to Layer1\n",
    "# Let’s now instantiate a model with skip connections and see how it compares:\n",
    "torch.manual_seed(123)\n",
    "layer_sizes = [3, 3, 3, 3, 3, 1]\n",
    "sample_input = torch.tensor([[1., 0., -1.]])\n",
    "model_with_shortcut = ExampleDeepNeuralNetwork(layer_sizes, use_shortcut=True)\n",
    "# model.to(device)\n",
    "print_gradients(model_with_shortcut, sample_input)\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layers.0.0.weight has gradient mean of 0.22169791162014008\n",
      "layers.1.0.weight has gradient mean of 0.20694106817245483\n",
      "layers.2.0.weight has gradient mean of 0.32896995544433594\n",
      "layers.3.0.weight has gradient mean of 0.2665732204914093\n",
      "layers.4.0.weight has gradient mean of 1.3258540630340576\n"
     ]
    }
   ],
   "execution_count": 31
  },
  {
   "cell_type": "code",
   "id": "a229081ed60e29b5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-02T18:27:19.395781Z",
     "start_time": "2025-02-02T18:27:19.394168Z"
    }
   },
   "source": [
    "# Note here the gradient doesn't approach a vanishingly small value during backprop.\n",
    "# In conclusion, shortcut connections are important for overcoming the limitations posed \n",
    "# by the vanishing gradient problem in deep neural networks."
   ],
   "outputs": [],
   "execution_count": 32
  },
  {
   "cell_type": "markdown",
   "id": "2df6d339f0570629",
   "metadata": {},
   "source": [
    "#### Next, we’ll connect all the previously covered concepts (layer normalization, GELU activations, feed forward module, and shortcut connections) in a transformer  block, which is the final building block we need to code the GPT architecture.\n",
    "\n",
    "![image](../data/transformer_wiring.png)"
   ]
  },
  {
   "cell_type": "code",
   "id": "cd526dd3047ba477",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-02T18:27:19.444017Z",
     "start_time": "2025-02-02T18:27:19.396610Z"
    }
   },
   "source": [
    "# See TransformerBlock.py for the basic sequence and feedforward details\n",
    "from src.chapter04.TransformerBlock import TransformerBlock\n",
    "\n",
    "torch.manual_seed(123)\n",
    "x = torch.rand(2, 4, 768)\n",
    "tr_block = TransformerBlock(GPT_CONFIG_124M)\n",
    "out = tr_block(x)\n",
    "#\n",
    "print(\"Input Shape:  \", x.shape)\n",
    "print(\"Output Shape: \", out.shape)\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Shape:   torch.Size([2, 4, 768])\n",
      "Output Shape:  torch.Size([2, 4, 768])\n"
     ]
    }
   ],
   "execution_count": 33
  },
  {
   "cell_type": "code",
   "id": "8c3e76bf70259f86",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-02T18:27:20.591273Z",
     "start_time": "2025-02-02T18:27:19.444897Z"
    }
   },
   "source": [
    "from src.chapter04.GPTModel import GPTModel \n",
    "\n",
    "# Let us wire up the actual GPT Model we wrote now\n",
    "\n",
    "torch.manual_seed(123)\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "model.to(device)\n",
    "print(\"Input batch: \", batch)\n",
    "out = model(batch)\n",
    "print(\"Output shape: \", out.shape)\n",
    "# print(\"Out: \\n\", out)\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input batch:  tensor([[6109, 3626, 6100,  345],\n",
      "        [6109, 1110, 6622,  257]], device='mps:0')\n",
      "Output shape:  torch.Size([2, 4, 50257])\n"
     ]
    }
   ],
   "execution_count": 34
  },
  {
   "cell_type": "code",
   "id": "af55c75851ac7e89",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-02T18:27:20.594264Z",
     "start_time": "2025-02-02T18:27:20.591875Z"
    }
   },
   "source": [
    "# Note above the output tensor has the shape [2, 4, 50257], since we passed in two input texts (the two sentences) \n",
    "# with four tokens each. The last dimension, 50257, corresponds to the vocabulary size of the tokenizer.\n",
    "#\n",
    "# To capture the total number of Parameters for a model use numel parameter value\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Total number of parameters: {total_params:,}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of parameters: 163,009,536\n"
     ]
    }
   ],
   "execution_count": 35
  },
  {
   "cell_type": "markdown",
   "id": "ba624b8ce1a7e0a0",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "id": "1a31524b0cdc6ebf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-02T18:27:20.596760Z",
     "start_time": "2025-02-02T18:27:20.594919Z"
    }
   },
   "source": [
    "# Weight Tying: The model reuses weights from the token embedding layer in its output layer\n",
    "print(\"Token embedding layer shape:\", model.tok_emb.weight.shape)\n",
    "print(\"Output layer shape:\", model.out_head.weight.shape)\n",
    "# As we can see both shapes are same"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token embedding layer shape: torch.Size([50257, 768])\n",
      "Output layer shape: torch.Size([50257, 768])\n"
     ]
    }
   ],
   "execution_count": 36
  },
  {
   "cell_type": "code",
   "id": "197e0a5f1199aada",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-02T18:27:20.599462Z",
     "start_time": "2025-02-02T18:27:20.597388Z"
    }
   },
   "source": [
    "# The token embedding and output layers are very large due to the 50,257 rows in the tokenizer’s vocabulary. \n",
    "# If we remove the output layer parameter count from the total GPT-2 model count :\n",
    "total_params_gptmodel = (\n",
    "    total_params - sum(p.numel() for p in model.out_head.parameters())\n",
    ")\n",
    "print(f\"Total number of trainable parameters considering weight tying: \"\n",
    "      f\"{total_params_gptmodel:,}\")\n",
    "\n",
    "# Memory Requirement\n",
    "total_size_bytes = total_params * 4\n",
    "total_size_mb = total_size_bytes / (1024 * 1024)\n",
    "print(f\"Total size of the model: {total_size_mb:.2f} MB\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of trainable parameters considering weight tying: 124,412,160\n",
      "Total size of the model: 621.83 MB\n"
     ]
    }
   ],
   "execution_count": 37
  },
  {
   "cell_type": "markdown",
   "id": "82b6b3cc8d05feb4",
   "metadata": {},
   "source": [
    "# Generating text \n",
    "#### We will now write code to generate text from the predicted tensors by the GPTModel\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "7da3f91bddda842d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-02T18:27:20.726873Z",
     "start_time": "2025-02-02T18:27:20.601771Z"
    }
   },
   "source": [
    "def generate_text_simple(input_model, tokenids, max_new_tokens, context_size):\n",
    "    for _ in range(max_new_tokens):\n",
    "        token_cond = tokenids[:, -context_size:]\n",
    "        with torch.no_grad():\n",
    "            logits = input_model(token_cond)\n",
    "\n",
    "        logits = logits[:, -1, :]\n",
    "        probabs = torch.softmax(logits, dim=-1)\n",
    "        next_token = torch.argmax(probabs, dim=-1, keepdim=True)\n",
    "        tokenids = torch.cat((tokenids, next_token), dim=1)\n",
    "\n",
    "        return tokenids\n",
    "#    \n",
    "#  Try it with a sample sentence  \n",
    "#    \n",
    "start_context = \"Hello, I am \"\n",
    "encoded = tokenizer.encode(start_context)\n",
    "print(\"encoded: \", encoded)\n",
    "#\n",
    "encoded_tensor = torch.tensor(encoded).to(device).unsqueeze(0)\n",
    "print(\"encoded_tensor.shape: \", encoded_tensor.shape)\n",
    "#\n",
    "model.eval()\n",
    "out = generate_text_simple(\n",
    "    input_model=model,\n",
    "    tokenids=encoded_tensor, \n",
    "    max_new_tokens=6, \n",
    "    context_size=GPT_CONFIG_124M[\"context_length\"]\n",
    ")\n",
    "print(\"Output:\", out)\n",
    "print(\"Output length:\", len(out[0]))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoded:  [15496, 11, 314, 716, 220]\n",
      "encoded_tensor.shape:  torch.Size([1, 5])\n",
      "Output: tensor([[15496,    11,   314,   716,   220, 24464]], device='mps:0')\n",
      "Output length: 6\n"
     ]
    }
   ],
   "execution_count": 38
  },
  {
   "cell_type": "code",
   "id": "ab7699859ab25da6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-02T18:27:20.729879Z",
     "start_time": "2025-02-02T18:27:20.727789Z"
    }
   },
   "source": [
    "decoded_text = tokenizer.decode(out.squeeze(0).tolist())\n",
    "print(decoded_text)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, I am opia\n"
     ]
    }
   ],
   "execution_count": 39
  },
  {
   "cell_type": "markdown",
   "id": "7397b4b87caf10c6",
   "metadata": {},
   "source": [
    "# Chapter 5: Pretraining on unlabeled data\n",
    "#### Using GPT to generate text"
   ]
  },
  {
   "cell_type": "code",
   "id": "21d8324b6eace0d0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-02T18:27:20.932753Z",
     "start_time": "2025-02-02T18:27:20.730488Z"
    }
   },
   "source": [
    "# How to calculate loss and relatively randomize next word prediction\n",
    "#\n",
    "inputs = torch.tensor([[16833, 3626, 6100],   # [\"every effort moves\",\n",
    "                       [40,    1107, 588]]).to(device)   #  \"I really like\"\n",
    "#\n",
    "targets = torch.tensor([[3626, 6100, 345  ],  # [\" effort moves you\",\n",
    "                        [1107, 588, 11311]]).to(device)  #  \" really like chocolate\"\n",
    "\n",
    "with torch.no_grad():\n",
    "    logits = model(inputs)\n",
    "probas = torch.softmax(logits, dim=-1)\n",
    "print(f\"probas: {probas.shape}\")\n",
    "\n",
    "token_ids = torch.argmax(probas, dim=-1, keepdim=True)\n",
    "print(f\"Token IDs: {token_ids}\")\n",
    "\n",
    "# Utility function\n",
    "def text_to_token_ids(txt, tokenizr):\n",
    "    encoded_txt = tokenizr.encode(txt, allowed_special={'<|endoftext|>'})\n",
    "    encoded_tnsr = torch.tensor(encoded_txt).to(device).unsqueeze(0)\n",
    "    return encoded_tnsr\n",
    "\n",
    "# Utility function\n",
    "def token_ids_to_text(tokenids, tokenizr):\n",
    "    flat = tokenids.squeeze(0)\n",
    "    return tokenizr.decode(flat.tolist())\n",
    "\n",
    "\n",
    "print(f\"Targets batch 1: {token_ids_to_text(targets[0], tokenizer)}\")\n",
    "print(f\"Outputs batch 1: {token_ids_to_text(token_ids[0].flatten(), tokenizer)}\")\n",
    "\n",
    "torch.set_printoptions(sci_mode=True)\n",
    "text_idx = 0\n",
    "target_probas_1 = probas[text_idx, [0, 1, 2], targets[text_idx]]\n",
    "print(\"Softmax scores for Text 1:\", target_probas_1)\n",
    "\n",
    "text_idx = 1\n",
    "target_probas_2 = probas[text_idx, [0, 1, 2], targets[text_idx]]\n",
    "print(\"Softmax scores for Text 2: \", target_probas_2)\n",
    "\n",
    "# The goal of training an LLM is to maximize the likelihood of the correct token, \n",
    "# which involves increasing its probability relative to other tokens. \n",
    "\n",
    "# Loss of probabilities for the two batches are\n",
    "log_probas = torch.log(torch.cat((target_probas_1, target_probas_2)))\n",
    "print(\"Log probabilities for both batches are: \\n\", log_probas)\n",
    "\n",
    "# Next, we combine the log probabilities into a single score by computing \n",
    "# the average\n",
    "avg_log_probas = torch.mean(log_probas)\n",
    "print(f\"Average log probability: {avg_log_probas}\")\n",
    "\n",
    "# However, in deep learning, the common practice isn’t to push the average log probability \n",
    "# up to 0 but rather to bring the negative average log probability down to 0. The negative \n",
    "# average log probability is simply the average log probability multiplied by –1\n",
    "neg_avg_log_probabs = avg_log_probas * -1\n",
    "print(f\"Negative of average log probability: {neg_avg_log_probabs}\")\n",
    "\n",
    "# In deep learning, the term for turning this negative value, –10.7940, into 10.7940, \n",
    "# is known as the cross entropy loss.\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "probas: torch.Size([2, 3, 50257])\n",
      "Token IDs: tensor([[[36397],\n",
      "         [39619],\n",
      "         [20610]],\n",
      "\n",
      "        [[ 8615],\n",
      "         [49289],\n",
      "         [47105]]], device='mps:0')\n",
      "Targets batch 1:  effort moves you\n",
      "Outputs batch 1:  Gathering SerbianFriday\n",
      "Softmax scores for Text 1: tensor([2.3466e-05, 2.0531e-05, 1.1733e-05], device='mps:0')\n",
      "Softmax scores for Text 2:  tensor([4.2794e-05, 1.6248e-05, 1.1586e-05], device='mps:0')\n",
      "Log probabilities for both batches are: \n",
      " tensor([-1.0660e+01, -1.0794e+01, -1.1353e+01, -1.0059e+01, -1.1028e+01, -1.1366e+01],\n",
      "       device='mps:0')\n",
      "Average log probability: -10.876513481140137\n",
      "Negative of average log probability: 10.876513481140137\n"
     ]
    }
   ],
   "execution_count": 40
  },
  {
   "cell_type": "markdown",
   "id": "2e39970739c5f593",
   "metadata": {},
   "source": [
    "#### NOTE: Our goal during training is to get the average log probability as close to 0 as possible by updating the model’s weights"
   ]
  },
  {
   "cell_type": "code",
   "id": "62d971c360d51b5b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-02T18:27:21.860160Z",
     "start_time": "2025-02-02T18:27:20.933395Z"
    }
   },
   "source": [
    "GPT_CONFIG_124M_2 = {\n",
    "    \"vocab_size\": 50257,\n",
    "    \"context_length\": 256,\n",
    "    \"emb_dim\": 768,\n",
    "    \"n_heads\": 12,\n",
    "    \"n_layers\": 12, \n",
    "    \"drop_rate\": 0.1,\n",
    "    \"qkv_bias\": False,\n",
    "    \"model_name\": \"GPTModel\",\n",
    "    \"lr\": 5e-4,\n",
    "    \"weight_decay\": 0.1\n",
    "}\n",
    "torch.manual_seed(123)\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "model.to(torch.device(\"mps\"))\n",
    "model.eval()\n",
    "\n",
    "\n",
    "\n",
    "start_context = \"Every effort moves you\"\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "token_ids = generate_text_simple(\n",
    "    input_model=model,\n",
    "    tokenids=text_to_token_ids(start_context, tokenizer),\n",
    "    max_new_tokens=10,\n",
    "    context_size=GPT_CONFIG_124M[\"context_length\"]\n",
    ")\n",
    "print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output text:\n",
      " Every effort moves you Ae\n"
     ]
    }
   ],
   "execution_count": 41
  },
  {
   "cell_type": "code",
   "id": "d0465f554d8c18b6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-02T18:27:21.877195Z",
     "start_time": "2025-02-02T18:27:21.860747Z"
    }
   },
   "source": [
    "print(\"Logits shape:\", logits.shape) # batch size, num of tokens, vocab size\n",
    "print(\"Targets shape:\", targets.shape) # batch size, num of tokens\n",
    "\n",
    "# For the cross_entropy loss function in PyTorch, we want to flatten these \n",
    "# tensors by combining them over the batch dimension:\n",
    "logits_flat = logits.flatten(0, 1)\n",
    "targets_flat = targets.flatten()\n",
    "print(\"Flattened logits:\", logits_flat.shape)\n",
    "print(\"Flattened targets:\", targets_flat.shape)\n",
    "\n",
    "# Now we can call CE from torch to calculate the loss\n",
    "celoss = torch.nn.functional.cross_entropy(logits_flat, targets_flat)\n",
    "print(f\"Cross Entropy loss: {celoss}\")\n",
    "\n",
    "# Perplexity measures how well the probability distribution predicted by the model \n",
    "# matches the actual distribution of the words in the dataset. Similar to the loss, \n",
    "# a lower perplexity means the model predictions are closer to the actual distribution.\n",
    "perplexity = torch.exp(celoss)\n",
    "print(f\"Perplexity: {perplexity}\")\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logits shape: torch.Size([2, 3, 50257])\n",
      "Targets shape: torch.Size([2, 3])\n",
      "Flattened logits: torch.Size([6, 50257])\n",
      "Flattened targets: torch.Size([6])\n",
      "Cross Entropy loss: 10.876513481140137\n",
      "Perplexity: 52918.7734375\n"
     ]
    }
   ],
   "execution_count": 42
  },
  {
   "cell_type": "code",
   "id": "9d0758479b082989",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-02T18:27:22.397699Z",
     "start_time": "2025-02-02T18:27:21.877887Z"
    }
   },
   "source": [
    "from src.chapter02.Dataloader import Dataloader\n",
    "import torch.nn.functional as F\n",
    "# To implement the data splitting and loading, we first define a train_ratio \n",
    "# to use 90% of the data for training and the remaining 10% as validation data \n",
    "# for model evaluation during training\n",
    "torch.manual_seed(123)\n",
    "# \n",
    "train_ratio = 0.9\n",
    "split_idx = int(train_ratio * len(raw_text))\n",
    "train_data = raw_text[:split_idx]\n",
    "val_data = raw_text[split_idx:]\n",
    "# \n",
    "train_loader = Dataloader(\n",
    "    batch_size=2,\n",
    "    max_length=GPT_CONFIG_124M_2[\"context_length\"],\n",
    "    stride=GPT_CONFIG_124M_2[\"context_length\"],\n",
    "    drop_last=True,\n",
    "    shuffle=True,\n",
    "    num_workers=0\n",
    ").create_dataloader_v1(train_data)\n",
    "# \n",
    "val_loader = Dataloader(\n",
    "    batch_size=2,\n",
    "    max_length=GPT_CONFIG_124M_2[\"context_length\"],\n",
    "    stride=GPT_CONFIG_124M_2[\"context_length\"],\n",
    "    drop_last=False,\n",
    "    shuffle=False,\n",
    "    num_workers=0\n",
    ").create_dataloader_v1(val_data)\n",
    "# \n",
    "print(\"Train loader:\")\n",
    "for x, y in train_loader:\n",
    "    print(x.shape, y.shape)\n",
    "# \n",
    "print(\"\\nValidation loader:\")\n",
    "for x, y in val_loader:\n",
    "    print(x.shape, y.shape)\n",
    "#     \n",
    "# Calculate per batch loss \n",
    "# Use CrossEntropy\n",
    "# \n",
    "def calculate_batch_loss(input_batch, target_batch, model, device):\n",
    "    input_batch = input_batch.to(device)\n",
    "    target_batch = target_batch.to(device)      \n",
    "    batch_logits = model(input_batch)\n",
    "    batch_loss = F.cross_entropy(\n",
    "        batch_logits.flatten(0, 1), \n",
    "        target_batch.flatten()\n",
    "    )\n",
    "    # batch_loss = F.nll_loss(\n",
    "    #     batch_logits.flatten(0, 1), \n",
    "    #     target_batch.flatten()\n",
    "    # )\n",
    "    return batch_loss\n",
    "\n",
    "# \n",
    "# Calculate loss across batches\n",
    "# \n",
    "def calculate_loss_loader(data_loader, model, device, num_batches=None):\n",
    "    total_loss = 0.\n",
    "    if len(data_loader) == 0:\n",
    "        return float(\"nan\")\n",
    "    elif num_batches is None:\n",
    "        num_batches = len(data_loader)\n",
    "    else:\n",
    "        num_batches = min(num_batches, len(data_loader))\n",
    "    for n, (input_batch, target_batch) in enumerate(data_loader):\n",
    "        if n < num_batches:\n",
    "            loss = calculate_batch_loss(input_batch, target_batch, model, device)\n",
    "            total_loss += loss.item()\n",
    "        else:\n",
    "            break\n",
    "    return total_loss / num_batches\n",
    "# \n",
    "model.to(device)\n",
    "with torch.no_grad():\n",
    "    train_loss = calculate_loss_loader(train_loader, model, device)\n",
    "    val_loss = calculate_loss_loader(val_loader, model, device)\n",
    "print(\"Default Training loss:\", train_loss)\n",
    "print(\"Default Validation loss:\", val_loss)\n",
    "# \n",
    "# Model Evaluation\n",
    "#   Calculate both training and validation losses and return\n",
    "#\n",
    "def evaluate_model(eval_model, training_loader, validn_loader, eval_device, eval_iter):\n",
    "    eval_model.eval()\n",
    "    with torch.no_grad():\n",
    "        training_loss = calculate_loss_loader(\n",
    "            training_loader, eval_model, eval_device, num_batches=eval_iter\n",
    "        )\n",
    "        validn_loss = calculate_loss_loader(\n",
    "            validn_loader, eval_model, eval_device, num_batches=eval_iter\n",
    "        )\n",
    "    eval_model.train()\n",
    "    return training_loss, validn_loss\n",
    "\n",
    "# A convenience function that we use to track whether the model improves during the training. \n",
    "# The generate_and_print_sample() function takes a text snippet (start_context) as input, \n",
    "# converts it into token IDs, and feeds it to the LLM to generate a text sample \n",
    "# using the generate_text_simple function\n",
    "def generate_and_print_sample(model, tokenizer, device, start_context):\n",
    "    model.eval()\n",
    "    context_size = model.pos_emb.weight.shape[0]\n",
    "    encoded = text_to_token_ids(start_context, tokenizer).to(device)\n",
    "    with torch.no_grad():\n",
    "        token_ids = generate_text_simple(\n",
    "            input_model=model, \n",
    "            tokenids=encoded,\n",
    "            max_new_tokens=50, \n",
    "            context_size=context_size\n",
    "        )\n",
    "    decoded_text = token_ids_to_text(token_ids, tokenizer)\n",
    "    print(decoded_text.replace(\"\\n\", \" \"))\n",
    "    model.train()\n",
    "\n",
    "\n",
    "# \n",
    "# Now we implement the model training flow \n",
    "#\n",
    "def train_model_simple(model, train_loader, val_loader,\n",
    "                       optimizer, device, num_epochs,\n",
    "                       eval_freq, eval_iter, start_context, tokenizer):\n",
    "    # \n",
    "    train_losses, val_losses, track_tokens_seen = [], [], []\n",
    "    tokens_seen, global_step = 0, -1\n",
    "    # \n",
    "    model.train()\n",
    "    model.to(device)\n",
    "    # \n",
    "    for epoch in range(num_epochs):\n",
    "        for input_batch, target_batch in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            input_batch = input_batch.to(device)\n",
    "            target_batch = target_batch.to(device)\n",
    "            \n",
    "            loss = calculate_batch_loss(input_batch, target_batch, model, device)\n",
    "            loss.backward() # Back propagation\n",
    "            # Optimizer step function with no closure supplied is below.\n",
    "            # A closure should clear the gradients, compute the loss, and return it.\n",
    "            # For example if it was a Conjugate Grad. optimization method \n",
    "            # We had to pass a closure. A closure is nothing but a function\n",
    "            # for input, target in dataset:\n",
    "            #    def closure():\n",
    "            #        optimizer.zero_grad()\n",
    "            #        output = model(input)\n",
    "            #        loss = loss_fn(output, target)\n",
    "            #        loss.backward()\n",
    "            #        return loss\n",
    "            #    optimizer.step(closure) \n",
    "            # \n",
    "            optimizer.step() \n",
    "            \n",
    "            tokens_seen += input_batch.numel()\n",
    "            global_step += 1\n",
    "\n",
    "            if global_step % eval_freq == 0:\n",
    "                trn_loss, validn_loss = evaluate_model(\n",
    "                    model, train_loader, val_loader, device, eval_iter\n",
    "                )\n",
    "                train_losses.append(trn_loss)\n",
    "                val_losses.append(validn_loss)\n",
    "                track_tokens_seen.append(tokens_seen)\n",
    "                print(f\"Ep {epoch+1} (Step {global_step:06d}): \"\n",
    "                      f\"Train loss {trn_loss:.3f}, \"\n",
    "                      f\"Val loss {validn_loss:.3f}\"\n",
    "                )\n",
    "        # End inner for\n",
    "        generate_and_print_sample(model, tokenizer, device, start_context) # After each epoch\n",
    "    # End outer for\n",
    "    return train_losses, val_losses, track_tokens_seen\n",
    "\n",
    "\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loader:\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "\n",
      "Validation loader:\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "Default Training loss: 10.988501654730904\n",
      "Default Validation loss: 10.990342140197754\n"
     ]
    }
   ],
   "execution_count": 43
  },
  {
   "cell_type": "code",
   "id": "9f203bec3d737b0a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-02T18:27:22.402543Z",
     "start_time": "2025-02-02T18:27:22.398264Z"
    }
   },
   "source": [
    "#\n",
    "# Training Loop\n",
    "#\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "torch.manual_seed(123)\n",
    "model_file_path=\"../models/GPTModel.pth\"\n",
    "num_epochs = 10\n",
    "train_losses, val_losses, tokens_seen = [], [], []\n",
    "epochs_tensor = None\n",
    "\n",
    "def plot_losses(epochs_seen, tokens_seen, train_losses, val_losses):\n",
    "    fig, ax1 = plt.subplots(figsize=(6, 4))\n",
    "    ax1.plot(epochs_seen, train_losses, label=\"Training loss\")\n",
    "    ax1.plot(\n",
    "        epochs_seen, val_losses, linestyle=\"-.\", label=\"Validation loss\"\n",
    "    )\n",
    "    ax1.set_xlabel(\"Epochs\")\n",
    "    ax1.set_ylabel(\"Loss\")\n",
    "    ax1.legend(loc=\"upper right\")\n",
    "    ax1.xaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "    ax2 = ax1.twiny()\n",
    "    ax2.plot(tokens_seen, train_losses, alpha=0)\n",
    "    ax2.set_xlabel(\"Tokens seen\")\n",
    "    fig.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "    # \n",
    "if not os.path.exists(model_file_path):\n",
    "    model = GPTModel(GPT_CONFIG_124M_2)\n",
    "    model.to(device)\n",
    "    optimizer = torch.optim.AdamW(\n",
    "         model.parameters(),\n",
    "        lr=0.0004, \n",
    "        weight_decay=0.1\n",
    "    )\n",
    "    # \n",
    "    # \n",
    "    train_losses, val_losses, tokens_seen = (\n",
    "        train_model_simple(model, train_loader, val_loader, optimizer, device, \n",
    "                           num_epochs=num_epochs, eval_freq=5, eval_iter=5, \n",
    "                           start_context=\"Every effort moves you\", tokenizer=tokenizer\n",
    "                           )\n",
    "    )\n",
    "    if epochs_tensor is None:\n",
    "        epochs_tensor = torch.linspace(0, num_epochs, len(train_losses))\n",
    "        plot_losses(epochs_tensor, tokens_seen, train_losses, val_losses)\n",
    "else:\n",
    "    print(f\"Model already exists\")\n",
    "    pass "
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model already exists\n"
     ]
    }
   ],
   "execution_count": 44
  },
  {
   "cell_type": "code",
   "id": "acfc02f2040cd8ed",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-02T18:27:24.072654Z",
     "start_time": "2025-02-02T18:27:22.403166Z"
    }
   },
   "source": [
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "token_ids = generate_text_simple(\n",
    "    input_model=model,\n",
    "    tokenids=text_to_token_ids(\"Every effort moves you\", tokenizer),\n",
    "    max_new_tokens=25,\n",
    "    context_size=GPT_CONFIG_124M[\"context_length\"]\n",
    ")\n",
    "print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))\n",
    "\n",
    "# Now lets look at Temperature Scaling\n",
    "vocab = { \n",
    "    \"closer\": 0,\n",
    "    \"every\": 1, \n",
    "    \"effort\": 2, \n",
    "    \"forward\": 3,\n",
    "    \"inches\": 4,\n",
    "    \"moves\": 5, \n",
    "    \"pizza\": 6,\n",
    "    \"toward\": 7,\n",
    "    \"you\": 8,\n",
    "} \n",
    "inverse_vocab = {v: k for k, v in vocab.items()}\n",
    "# Assume the model generates the following logits \n",
    "next_token_logits = torch.tensor(\n",
    "    [4.51, 0.89, -1.90, 6.75, 1.63, -1.62, -1.89, 6.28, 1.79]\n",
    ").to(device)\n",
    "\n",
    "# We now generally do an argmax of the probabilities \n",
    "probas = torch.softmax(next_token_logits, dim=0).to(device)\n",
    "next_token_id = torch.argmax(probas).item()\n",
    "print(f\"Next token (argmax): {inverse_vocab[next_token_id]}\")\n",
    "#\n",
    "# But we can try replacing argmax with multinomial and get a sampling\n",
    "#\n",
    "torch.manual_seed(123) \n",
    "next_token_id = torch.multinomial(probas, num_samples=1).item()\n",
    "print(f\"Next token (multinomial): {inverse_vocab[next_token_id]}\")\n",
    "# \n",
    "# Let's see if multinomial can produce any other probabilites\n",
    "def print_sampled_tokens(probabs):\n",
    "    torch.manual_seed(123)\n",
    "    # Multinomial Sampling of probabilities instead of max\n",
    "    samples = [torch.multinomial(probabs, num_samples=1).item() for _ in range(1_000)]\n",
    "    sampled_ids = torch.bincount(torch.tensor(samples).to(device))\n",
    "    for cnt, freq in enumerate(sampled_ids):\n",
    "        print(f\"Sampled Freq. {freq} : {inverse_vocab[cnt]}\")\n",
    "\n",
    "print_sampled_tokens(probas)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output text:\n",
      " Every effort moves you Ae\n",
      "Next token (argmax): forward\n",
      "Next token (multinomial): forward\n",
      "Sampled Freq. 72 : closer\n",
      "Sampled Freq. 2 : every\n",
      "Sampled Freq. 0 : effort\n",
      "Sampled Freq. 575 : forward\n",
      "Sampled Freq. 2 : inches\n",
      "Sampled Freq. 0 : moves\n",
      "Sampled Freq. 0 : pizza\n",
      "Sampled Freq. 343 : toward\n",
      "Sampled Freq. 6 : you\n"
     ]
    }
   ],
   "execution_count": 45
  },
  {
   "cell_type": "code",
   "id": "2edb9cac7f426800",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-02T18:27:24.133146Z",
     "start_time": "2025-02-02T18:27:24.073508Z"
    }
   },
   "source": [
    "# We can further control the distribution by a technique called temperature scaling\n",
    "# meaning dividing the logits with a >0 number\n",
    "def softmax_with_temperature(logits, temperature, dim):\n",
    "    scaled_logits = logits / temperature\n",
    "    scaled_probs =  torch.softmax(scaled_logits, dim=dim)\n",
    "    return scaled_probs\n",
    "\n",
    "# Let's check that out\n",
    "temperatures = [1, .01, 5]\n",
    "next_token_logits: Tensor = Tensor.cpu(next_token_logits)\n",
    "scaled_probas = [softmax_with_temperature(next_token_logits, float(T), 0) for T in temperatures]\n",
    "x = torch.arange(len(vocab))\n",
    "bar_width = 0.5\n",
    "fig, ax = plt.subplots(figsize=(6, 4))\n",
    "for i, T in enumerate(temperatures):\n",
    "    rects = ax.bar(x + i * bar_width, \n",
    "                   scaled_probas[i], \n",
    "                   bar_width, \n",
    "                   label=f'Temperature = {T}'\n",
    "    )\n",
    "ax.set_ylabel('Probability')\n",
    "ax.set_xlabel('Words')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(vocab.keys(), rotation=90)\n",
    "ax.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 600x400 with 1 Axes>"
      ],
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 46
  },
  {
   "cell_type": "markdown",
   "id": "48c5d9214db5c7e3",
   "metadata": {},
   "source": [
    "## Top K sampling"
   ]
  },
  {
   "cell_type": "code",
   "id": "af408748892d4b7f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-02T18:27:25.589738Z",
     "start_time": "2025-02-02T18:27:24.133819Z"
    }
   },
   "source": [
    "# Previously we implemented a probabilistic sampling approach coupled with \n",
    "# temperature scaling to increase the diversity of the outputs.  This method \n",
    "# allows for the exploring of less likely but potentially more interesting and \n",
    "# creative paths in the generation process.\n",
    "#\n",
    "# Top-k sampling, when combined with probabilistic sampling and temperature \n",
    "# scaling, can improve the text generation results.\n",
    "#\n",
    "# Here we can restrict the sampled tokens to the top-k most likely tokens \n",
    "# and exclude all other tokens from the selection process by masking their \n",
    "# probability scores\n",
    "# \n",
    "top_k = 3\n",
    "torch.set_printoptions(sci_mode=False)\n",
    "# \n",
    "top_logits, top_pos = torch.topk(next_token_logits, top_k)\n",
    "print(\"Top Logits: \", top_logits)\n",
    "print(\"Top Positions: \", top_pos)\n",
    "print(f\"Next token logits: {next_token_logits}\")\n",
    "\n",
    "# Pytorch WHERE function to set the logit values of tokens that are below the lowest \n",
    "# logit value within our top-three selection to negative infinity (-inf)\n",
    "#\n",
    "new_logits = torch.where(\n",
    "    condition=next_token_logits < top_logits[-1],\n",
    "    input=torch.tensor(float('-inf')),\n",
    "    other=next_token_logits\n",
    ")\n",
    "print(\"New Logits: \", new_logits)\n",
    "\n",
    "# Now apply the softmax\n",
    "topk_probas = torch.softmax(new_logits, dim=0)\n",
    "print(f\"top k probabilities: {topk_probas}\")\n",
    "# \n",
    "# We can now apply the temperature scaling and multinomial function for probabilistic \n",
    "# sampling to select the next token among these three non-zero probability scores to \n",
    "# GENERATE THE NEXT TOKEN with more diversity.\n",
    "# \n",
    "def generate(model, idx, max_new_tokens, context_size, temperature=0.0, top_k: int=None, eos_id=None):\n",
    "    # print(\"Entering generate()..\")\n",
    "    # print(idx.shape)\n",
    "    for i in range(max_new_tokens):\n",
    "        # print(f\"idx: [{i}]: {idx}\")\n",
    "        idx_cond = idx[:, -context_size:]\n",
    "        with torch.no_grad():\n",
    "            logits = model(idx_cond)\n",
    "        #     \n",
    "        logits = logits[:, -1, :] # ([1, 50257])\n",
    "        # print(logits.shape)\n",
    "        if top_k is not None:\n",
    "            top_logits, _ = torch.topk(logits, top_k)\n",
    "            min_val: Tensor = top_logits[:, -1] # Less than the lowest value of top k\n",
    "            # Now mark the minvals with -inf, so softmax becomes 0\n",
    "            logits = torch.where(\n",
    "                logits < min_val,\n",
    "                torch.tensor(float('-inf')).to(logits.device),\n",
    "                logits\n",
    "            )\n",
    "        if temperature > 0.0:\n",
    "            probs = softmax_with_temperature(logits, temperature, dim=-1)\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)\n",
    "        else: \n",
    "            idx_next = torch.argmax(logits, dim=-1, keepdim=True)\n",
    "\n",
    "        if idx_next == eos_id:\n",
    "            break\n",
    "\n",
    "        # Next word\n",
    "        idx = torch.cat((idx, idx_next), dim=1)\n",
    "\n",
    "    return idx\n",
    "\n",
    "token_ids = generate(\n",
    "    model=model,\n",
    "    idx=text_to_token_ids(\"Every effort moves you\", tokenizer),\n",
    "    max_new_tokens=15,\n",
    "    context_size=GPT_CONFIG_124M_2[\"context_length\"],\n",
    "    top_k=25,\n",
    "    temperature=1.4\n",
    ")\n",
    "print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))\n",
    "\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top Logits:  tensor([6.7500, 6.2800, 4.5100])\n",
      "Top Positions:  tensor([3, 7, 0])\n",
      "Next token logits: tensor([ 4.5100,  0.8900, -1.9000,  6.7500,  1.6300, -1.6200, -1.8900,  6.2800,\n",
      "         1.7900])\n",
      "New Logits:  tensor([4.5100,   -inf,   -inf, 6.7500,   -inf,   -inf,   -inf, 6.2800,   -inf])\n",
      "top k probabilities: tensor([0.0615, 0.0000, 0.0000, 0.5775, 0.0000, 0.0000, 0.0000, 0.3610, 0.0000])\n",
      "Output text:\n",
      " Every effort moves you Between unveiling Dice Pass Reload AF Interactive disinformation headacherael Barg breathtakingcatentry Clemson echoes\n"
     ]
    }
   ],
   "execution_count": 47
  },
  {
   "cell_type": "code",
   "id": "7e164cfeef1f9707",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-02T18:29:17.904602Z",
     "start_time": "2025-02-02T18:29:16.599964Z"
    }
   },
   "source": [
    "# torch.save(model.state_dict(), f\"/Users/amlanchatterjee/Documents/ws/python/PycharmProjects/SimpleLLMProject/models/{GPT_CONFIG_124M_2['model_name']}.pth\")\n",
    "MODEL_PATH = f\"../models/{GPT_CONFIG_124M_2['model_name']}.pth\"\n",
    "# \n",
    "try:\n",
    "    torch.save({\n",
    "        \"model_state_dict\": model.state_dict(),\n",
    "        \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "        }, \n",
    "        MODEL_PATH\n",
    "    )\n",
    "    print(f\"Model saved at {MODEL_PATH}\\n\")\n",
    "except Exception as e:\n",
    "    print(f\"Encountered exception : {e}\")\n",
    "    \n",
    "# "
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved at ../models/GPTModel.pth\n",
      "\n"
     ]
    }
   ],
   "execution_count": 73
  },
  {
   "cell_type": "code",
   "id": "d0a3f72c8cebc09d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-02T18:29:53.016929Z",
     "start_time": "2025-02-02T18:29:51.810633Z"
    }
   },
   "source": [
    "# Load the model back\n",
    "checkpoint = torch.load(MODEL_PATH, map_location=device, weights_only=True)\n",
    "loaded_model = GPTModel(GPT_CONFIG_124M_2).to(device)\n",
    "try:\n",
    "    loaded_model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "    optimizer = torch.optim.AdamW(loaded_model.parameters(), \n",
    "                                 lr=GPT_CONFIG_124M_2[\"lr\"], \n",
    "                                 weight_decay=GPT_CONFIG_124M_2[\"weight_decay\"])\n",
    "    optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n",
    "    print(f\"Model and Optimizer successfully loaded from \\n{MODEL_PATH}\\n\")\n",
    "except Exception as ex:\n",
    "    print(f\"Encountered exception : {ex}\")\n",
    "    \n",
    "loaded_model.train()\n",
    "print(\"Model set to train mode\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encountered exception : Error(s) in loading state_dict for GPTModel:\n",
      "\tsize mismatch for pos_emb.weight: copying a param with shape torch.Size([1024, 768]) from checkpoint, the shape in current model is torch.Size([256, 768]).\n",
      "\tsize mismatch for trf_blocks.0.att.mask: copying a param with shape torch.Size([1024, 1024]) from checkpoint, the shape in current model is torch.Size([256, 256]).\n",
      "\tsize mismatch for trf_blocks.1.att.mask: copying a param with shape torch.Size([1024, 1024]) from checkpoint, the shape in current model is torch.Size([256, 256]).\n",
      "\tsize mismatch for trf_blocks.2.att.mask: copying a param with shape torch.Size([1024, 1024]) from checkpoint, the shape in current model is torch.Size([256, 256]).\n",
      "\tsize mismatch for trf_blocks.3.att.mask: copying a param with shape torch.Size([1024, 1024]) from checkpoint, the shape in current model is torch.Size([256, 256]).\n",
      "\tsize mismatch for trf_blocks.4.att.mask: copying a param with shape torch.Size([1024, 1024]) from checkpoint, the shape in current model is torch.Size([256, 256]).\n",
      "\tsize mismatch for trf_blocks.5.att.mask: copying a param with shape torch.Size([1024, 1024]) from checkpoint, the shape in current model is torch.Size([256, 256]).\n",
      "\tsize mismatch for trf_blocks.6.att.mask: copying a param with shape torch.Size([1024, 1024]) from checkpoint, the shape in current model is torch.Size([256, 256]).\n",
      "\tsize mismatch for trf_blocks.7.att.mask: copying a param with shape torch.Size([1024, 1024]) from checkpoint, the shape in current model is torch.Size([256, 256]).\n",
      "\tsize mismatch for trf_blocks.8.att.mask: copying a param with shape torch.Size([1024, 1024]) from checkpoint, the shape in current model is torch.Size([256, 256]).\n",
      "\tsize mismatch for trf_blocks.9.att.mask: copying a param with shape torch.Size([1024, 1024]) from checkpoint, the shape in current model is torch.Size([256, 256]).\n",
      "\tsize mismatch for trf_blocks.10.att.mask: copying a param with shape torch.Size([1024, 1024]) from checkpoint, the shape in current model is torch.Size([256, 256]).\n",
      "\tsize mismatch for trf_blocks.11.att.mask: copying a param with shape torch.Size([1024, 1024]) from checkpoint, the shape in current model is torch.Size([256, 256]).\n",
      "Model set to train mode\n"
     ]
    }
   ],
   "execution_count": 74
  },
  {
   "cell_type": "markdown",
   "id": "d38b09e853311dec",
   "metadata": {},
   "source": [
    "### OpenAI also shares the weights of larger models: 355M, 774M, and 1558M \n",
    "![image](../data/model_arch_stack.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "68e160927f07d23a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-02T18:27:30.811958Z",
     "start_time": "2025-02-02T18:27:26.741985Z"
    }
   },
   "source": [
    "# Load the downloaded GPT Data\n",
    "import os, sys\n",
    "import urllib.request\n",
    "from src.chapter05.gd import download_and_load_gpt2\n",
    "\n",
    "url = (\n",
    "    \"https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/ch05/01_main-chapter-code/gpt_download.py\"\n",
    ")\n",
    "filename = url.split('/')[-1]\n",
    "# print(filename)\n",
    "filename = \"./chapter05/\"+filename\n",
    "# print(filename)\n",
    "if not os.path.exists(filename):\n",
    "    try: \n",
    "        urllib.request.urlretrieve(url, filename)\n",
    "        print(f\"Downloaded {filename}\")\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        sys.exit()\n",
    "\n",
    "settings, gpt_params = download_and_load_gpt2(model_size=\"124M\", models_dir=\"../data/gpt2\")\n",
    "print(f\"\\nParams: {gpt_params.keys()}\")\n",
    "print(f\"Settings: {settings}\")\n",
    "print(f\"Token embedding layer weight tensor dimensions: {gpt_params[\"wte\"].shape}\")    \n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File already exists and is up-to-date: ../data/gpt2/124M/checkpoint\n",
      "File already exists and is up-to-date: ../data/gpt2/124M/encoder.json\n",
      "File already exists and is up-to-date: ../data/gpt2/124M/hparams.json\n",
      "File already exists and is up-to-date: ../data/gpt2/124M/model.ckpt.data-00000-of-00001\n",
      "File already exists and is up-to-date: ../data/gpt2/124M/model.ckpt.index\n",
      "File already exists and is up-to-date: ../data/gpt2/124M/model.ckpt.meta\n",
      "File already exists and is up-to-date: ../data/gpt2/124M/vocab.bpe\n",
      "\n",
      "Params: dict_keys(['blocks', 'b', 'g', 'wpe', 'wte'])\n",
      "Settings: {'n_vocab': 50257, 'n_ctx': 1024, 'n_embd': 768, 'n_head': 12, 'n_layer': 12}\n",
      "Token embedding layer weight tensor dimensions: (50257, 768)\n"
     ]
    }
   ],
   "execution_count": 51
  },
  {
   "cell_type": "code",
   "id": "ba48ffe795182755",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-02T18:27:31.774590Z",
     "start_time": "2025-02-02T18:27:30.812694Z"
    }
   },
   "source": [
    "\n",
    "# After loading the GPT-2 model weights into Python, we still need to transfer \n",
    "# them from the settings and params dictionaries into our GPTModel instance. \n",
    "# First, we create a dictionary that lists the differences between the \n",
    "# different GPT model sizes\n",
    "model_configs = {\n",
    "    \"gpt2-small (124M)\": {\"emb_dim\": 768, \"n_layers\": 12, \"n_heads\": 12},\n",
    "    \"gpt2-medium (355M)\": {\"emb_dim\": 1024, \"n_layers\": 24, \"n_heads\": 16},\n",
    "    \"gpt2-large (774M)\": {\"emb_dim\": 1280, \"n_layers\": 36, \"n_heads\": 20},\n",
    "    \"gpt2-xl (1558M)\": {\"emb_dim\": 1600, \"n_layers\": 48, \"n_heads\": 25},\n",
    "}\n",
    "model_name=\"gpt2-small (124M)\"\n",
    "# \n",
    "NEW_GPT_CONFIG = GPT_CONFIG_124M.copy()\n",
    "NEW_GPT_CONFIG.update({\"model_name\": model_name})\n",
    "# Update the value ex. {\"emb_dim\": 768, \"n_layers\": 12, \"n_heads\": 12},\n",
    "NEW_GPT_CONFIG.update(model_configs[model_name]) \n",
    "\n",
    "NEW_GPT_CONFIG.update({\"context_length\": 1024})\n",
    "NEW_GPT_CONFIG.update({\"qkv_bias\": True})\n",
    "# \n",
    "print(f\"{model_name}: {model_configs[model_name]}\")\n",
    "print(\"NEW_GPT_CONFIG:\\n\"+\"\".join(f\"\\t{k}: {v}\\n\" for k, v in sorted(NEW_GPT_CONFIG.items())))\n",
    "# \n",
    "newgpt = GPTModel(NEW_GPT_CONFIG).to(device)\n",
    "newgpt.eval()\n",
    "# \n",
    "# Before we assign the loaded openai weights into the model, we will first define \n",
    "# a small assign utility function that checks whether two tensors or arrays \n",
    "# (left and right) have the same dimensions or shape and returns the right tensor \n",
    "# as trainable PyTorch parameters\n",
    "def assign(left: Tensor, right: Tensor) -> Tensor:\n",
    "    if left.shape != right.shape:\n",
    "        raise ValueError(f\"Shape mismatch Left shape: {left.shape} Right shape: {right.shape}\")\n",
    "    return torch.nn.Parameter(torch.tensor(right).to(device))\n",
    "# \n",
    "print(f\"Params: {gpt_params.keys()}\")\n",
    "# print(f\"Params: {gpt_params}\")\n",
    "print(f\"GPT Parameter Blocks Count: {len(gpt_params[\"blocks\"])}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpt2-small (124M): {'emb_dim': 768, 'n_layers': 12, 'n_heads': 12}\n",
      "NEW_GPT_CONFIG:\n",
      "\tcontext_length: 1024\n",
      "\tdrop_rate: 0.1\n",
      "\temb_dim: 768\n",
      "\tlr: 0.0005\n",
      "\tmodel_name: gpt2-small (124M)\n",
      "\tn_heads: 12\n",
      "\tn_layers: 12\n",
      "\tqkv_bias: True\n",
      "\tvocab_size: 50257\n",
      "\tweight_decay: 0.1\n",
      "\n",
      "Params: dict_keys(['blocks', 'b', 'g', 'wpe', 'wte'])\n",
      "GPT Parameter Blocks Count: 12\n"
     ]
    }
   ],
   "execution_count": 52
  },
  {
   "cell_type": "code",
   "id": "dbc4d3fb2c383826",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-02T18:27:31.780456Z",
     "start_time": "2025-02-02T18:27:31.775303Z"
    }
   },
   "source": [
    "#\n",
    "# Load OpenAI Weights into our GPTModel code\n",
    "#\n",
    "import numpy as np\n",
    "\n",
    "def load_weights_into_gpt(gpt, params):\n",
    "    gpt.pos_emb.weight = assign(gpt.pos_emb.weight, params['wpe'])\n",
    "    gpt.tok_emb.weight = assign(gpt.tok_emb.weight, params['wte'])\n",
    "\n",
    "    for b in range(len(params[\"blocks\"])):\n",
    "        q_w, k_w, v_w = np.split(\n",
    "            (params[\"blocks\"][b][\"attn\"][\"c_attn\"])[\"w\"], 3, axis=-1)\n",
    "        gpt.trf_blocks[b].att.W_query.weight = assign(\n",
    "            gpt.trf_blocks[b].att.W_query.weight, q_w.T)\n",
    "        gpt.trf_blocks[b].att.W_key.weight = assign(\n",
    "            gpt.trf_blocks[b].att.W_key.weight, k_w.T)\n",
    "        gpt.trf_blocks[b].att.W_value.weight = assign(\n",
    "            gpt.trf_blocks[b].att.W_value.weight, v_w.T)\n",
    "\n",
    "        q_b, k_b, v_b = np.split(\n",
    "            (params[\"blocks\"][b][\"attn\"][\"c_attn\"])[\"b\"], 3, axis=-1)\n",
    "        gpt.trf_blocks[b].att.W_query.bias = assign(\n",
    "            gpt.trf_blocks[b].att.W_query.bias, q_b)\n",
    "        gpt.trf_blocks[b].att.W_key.bias = assign(\n",
    "            gpt.trf_blocks[b].att.W_key.bias, k_b)\n",
    "        gpt.trf_blocks[b].att.W_value.bias = assign(\n",
    "            gpt.trf_blocks[b].att.W_value.bias, v_b)\n",
    "\n",
    "        gpt.trf_blocks[b].att.out_proj.weight = assign(\n",
    "            gpt.trf_blocks[b].att.out_proj.weight, \n",
    "            params[\"blocks\"][b][\"attn\"][\"c_proj\"][\"w\"].T)\n",
    "        gpt.trf_blocks[b].att.out_proj.bias = assign(\n",
    "            gpt.trf_blocks[b].att.out_proj.bias, \n",
    "            params[\"blocks\"][b][\"attn\"][\"c_proj\"][\"b\"])\n",
    "\n",
    "        gpt.trf_blocks[b].sff.layers[0].weight = assign(\n",
    "            gpt.trf_blocks[b].sff.layers[0].weight, \n",
    "            params[\"blocks\"][b][\"mlp\"][\"c_fc\"][\"w\"].T)\n",
    "        gpt.trf_blocks[b].sff.layers[0].bias = assign(\n",
    "            gpt.trf_blocks[b].sff.layers[0].bias, \n",
    "            params[\"blocks\"][b][\"mlp\"][\"c_fc\"][\"b\"])\n",
    "        gpt.trf_blocks[b].sff.layers[2].weight = assign(\n",
    "            gpt.trf_blocks[b].sff.layers[2].weight, \n",
    "            params[\"blocks\"][b][\"mlp\"][\"c_proj\"][\"w\"].T)\n",
    "        gpt.trf_blocks[b].sff.layers[2].bias = assign(\n",
    "            gpt.trf_blocks[b].sff.layers[2].bias, \n",
    "            params[\"blocks\"][b][\"mlp\"][\"c_proj\"][\"b\"])\n",
    "\n",
    "        gpt.trf_blocks[b].norm1.scale = assign(\n",
    "            gpt.trf_blocks[b].norm1.scale, \n",
    "            params[\"blocks\"][b][\"ln_1\"][\"g\"])\n",
    "        gpt.trf_blocks[b].norm1.shift = assign(\n",
    "            gpt.trf_blocks[b].norm1.shift, \n",
    "            params[\"blocks\"][b][\"ln_1\"][\"b\"])\n",
    "        gpt.trf_blocks[b].norm2.scale = assign(\n",
    "            gpt.trf_blocks[b].norm2.scale, \n",
    "            params[\"blocks\"][b][\"ln_2\"][\"g\"])\n",
    "        gpt.trf_blocks[b].norm2.shift = assign(\n",
    "            gpt.trf_blocks[b].norm2.shift, \n",
    "            params[\"blocks\"][b][\"ln_2\"][\"b\"])\n",
    "\n",
    "    gpt.final_norm.scale = assign(gpt.final_norm.scale, params[\"g\"])\n",
    "    gpt.final_norm.shift = assign(gpt.final_norm.shift, params[\"b\"])\n",
    "    gpt.out_head.weight = assign(gpt.out_head.weight, params[\"wte\"])\n",
    "    "
   ],
   "outputs": [],
   "execution_count": 53
  },
  {
   "cell_type": "code",
   "id": "c0eeef9e5cadf037",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-02T18:27:31.897394Z",
     "start_time": "2025-02-02T18:27:31.781170Z"
    }
   },
   "source": [
    "# Now lets try to load the weights and see\n",
    "load_weights_into_gpt(newgpt, gpt_params)\n",
    "newgpt.to(device)\n",
    "print(\"Loaded weights into GPTModel..\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded weights into GPTModel..\n"
     ]
    }
   ],
   "execution_count": 54
  },
  {
   "cell_type": "code",
   "id": "53993f0b2ef4021a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-02T18:27:33.369200Z",
     "start_time": "2025-02-02T18:27:31.898328Z"
    }
   },
   "source": [
    "# Now let's generate using the actual GPT trained weights\n",
    "torch.manual_seed(123)\n",
    "token_ids = generate(\n",
    "    model=newgpt,\n",
    "    idx=text_to_token_ids(\"Every effort moves you\", tokenizer).to(device),\n",
    "    max_new_tokens=25,\n",
    "    context_size=NEW_GPT_CONFIG[\"context_length\"],\n",
    "    top_k=50,\n",
    "    temperature=1.5\n",
    ")\n",
    "print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output text:\n",
      " Every effort moves you toward more efficient and efficient processes, like in the car's oil and gas operation,\" the study said. To see if that\n"
     ]
    }
   ],
   "execution_count": 55
  },
  {
   "cell_type": "markdown",
   "id": "850187dca8e7693f",
   "metadata": {},
   "source": [
    "# Fine-tuning for Classification"
   ]
  },
  {
   "cell_type": "code",
   "id": "533a90b85f4e1468",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-02T18:27:33.630200Z",
     "start_time": "2025-02-02T18:27:33.369928Z"
    }
   },
   "source": [
    "from pathlib import Path\n",
    "from src.chapter06 import DownloadDataset\n",
    "# \n",
    "# Download ehtSPAM Dataset\n",
    "# \n",
    "url = \"https://archive.ics.uci.edu/static/public/228/sms+spam+collection.zip\"\n",
    "zip_path = \"../data/sms_spam_collection.zip\"\n",
    "extracted_path = \"../data/sms_spam_collection\"\n",
    "data_file_path = Path(extracted_path) / \"SMSSpamCollection.tsv\"\n",
    "\n",
    "# \n",
    "DownloadDataset.download_and_unzip_spam_data(url, \n",
    "                                             zip_path, \n",
    "                                             extracted_path, \n",
    "                                             data_file_path)\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and unzipping spam dataset...\n",
      "Didnt find existing spam dataset, ...\n",
      "URL:  https://archive.ics.uci.edu/static/public/228/sms+spam+collection.zip\n",
      "ZIP path:  ../data/sms_spam_collection.zip\n",
      "../data/sms_spam_collection.zip doesnt exist. Downloading...\n",
      "File downloaded at: ../data/sms_spam_collection.zip\n",
      "../data/sms_spam_collection doesn't exist. Extracting...\n",
      "Extracted SPAM dataset successfully...\n",
      "Renaming ../data/sms_spam_collection/SMSSpamCollection to ../data/sms_spam_collection/SMSSpamCollection.tsv\n",
      "File downloaded and saved as ../data/sms_spam_collection/SMSSpamCollection.tsv\n"
     ]
    }
   ],
   "execution_count": 56
  },
  {
   "cell_type": "code",
   "id": "bcc8d30ddb5d1c8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-02T18:27:33.643618Z",
     "start_time": "2025-02-02T18:27:33.631199Z"
    }
   },
   "source": [
    "from pandas import DataFrame\n",
    "import pandas as pd\n",
    "\n",
    "# At this point the spam dataset should have been downloaded at {data_file_path}\n",
    "df: DataFrame = None\n",
    "\n",
    "if data_file_path.exists() and data_file_path.is_file():\n",
    "    df = pd.read_csv(data_file_path, sep=\"\\t\", header=None, names=[\"Label\", \"Text\"])\n",
    "   \n",
    "# df.head(10)\n",
    "\n",
    "#Let's take a look at class distributions\n",
    "print(f\"Class counts: {df[\"Label\"].value_counts()}\")\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class counts: Label\n",
      "ham     4825\n",
      "spam     747\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "execution_count": 57
  },
  {
   "cell_type": "code",
   "id": "eb7fba4d33fca94f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-02T18:27:33.658324Z",
     "start_time": "2025-02-02T18:27:33.644383Z"
    }
   },
   "source": [
    "# Considering there are so many more hams than spams we need to create a somewhat balanced dataset\n",
    "import os\n",
    "def create_balanced_dataset(df: DataFrame) -> DataFrame:\n",
    "    # print(df.shape)\n",
    "    num_spam = df[df[\"Label\"] == \"spam\"].shape[0]\n",
    "    num_ham = df[df[\"Label\"] == \"ham\"].shape[0]\n",
    "    print(f\"Spam and Ham counts: {num_spam}, {num_ham} \\n\")\n",
    "    # If num_spam is a lot less than num_ham\n",
    "    if num_spam < num_ham or num_spam == 0:\n",
    "        ham_subset = df[df[\"Label\"] == \"ham\"].sample(num_spam, random_state=123)\n",
    "    \n",
    "    bal_df = pd.concat([ham_subset, df[df[\"Label\"] == \"spam\"]])\n",
    "    return bal_df\n",
    "\n",
    "\n",
    "balanced_df = create_balanced_dataset(df)\n",
    "# print(f\"Rebalanced dataset \\n {balanced_df[\"Label\"].value_counts()}\")\n",
    "\n",
    "# Now we are going to change the string class labels to ints\n",
    "balanced_df[\"Label\"] = balanced_df[\"Label\"].map({\"ham\": 0, \"spam\": 1})\n",
    "\n",
    "# This process is similar to converting text into token IDs. However, instead \n",
    "# of using the GPT vocabulary, which consists of more than 50,000 words, we \n",
    "# are dealing with just two token IDs: 0 and 1.\n",
    "# \n",
    "def random_split(df, train_frac, validation_frac):\n",
    "    df = df.sample(frac=1, random_state=123).reset_index(drop=True)\n",
    "    train_end = int(len(df) * train_frac) # * 0.7\n",
    "    validation_end = train_end + int(len(df) * validation_frac) \n",
    "    \n",
    "    train_df = df[:train_end] # 70%\n",
    "    valid_df = df[train_end:validation_end] # 10%\n",
    "    test_df = df[validation_end:]   # 20%\n",
    "    \n",
    "    return train_df, valid_df, test_df\n",
    "\n",
    "# Next, we create a random_split function to split the dataset into three parts: \n",
    "# 70% for training, 10% for validation, and 20% for testing\n",
    "training_df, validation_df, testing_df = random_split(balanced_df, 0.7, 0.1)\n",
    "# \n",
    "print(f\"Training dataset \\n {training_df['Label'].value_counts()}\")\n",
    "print(f\"Validation dataset \\n {validation_df['Label'].value_counts()}\")\n",
    "print(f\"Training dataset \\n {testing_df['Label'].value_counts()}\")\n",
    "# \n",
    "# Save the files\n",
    "if not os.path.exists(\"../data/train.csv\"):\n",
    "    training_df.to_csv(\"../data/train.csv\", index=None)\n",
    "    \n",
    "if not os.path.exists(\"../data/validation.csv\"):   \n",
    "    validation_df.to_csv(\"../data/validate.csv\", index=None)\n",
    "    \n",
    "if not os.path.exists(\"../data/test.csv\"):    \n",
    "    testing_df.to_csv(\"../data/test.csv\", index=None)\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spam and Ham counts: 747, 4825 \n",
      "\n",
      "Training dataset \n",
      " Label\n",
      "0    528\n",
      "1    517\n",
      "Name: count, dtype: int64\n",
      "Validation dataset \n",
      " Label\n",
      "1    79\n",
      "0    70\n",
      "Name: count, dtype: int64\n",
      "Training dataset \n",
      " Label\n",
      "1    151\n",
      "0    149\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "execution_count": 58
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 6.3 Setting up PyTorch Data Loaders",
   "id": "75ffdb033fba532f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-02T18:27:33.660794Z",
     "start_time": "2025-02-02T18:27:33.659033Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import tiktoken\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "print(tokenizer.encode(\"<|endoftext|>\", allowed_special={\"<|endoftext|>\"}))\n"
   ],
   "id": "a13c23eaebccca8a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[50256]\n"
     ]
    }
   ],
   "execution_count": 59
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-02T18:27:33.686694Z",
     "start_time": "2025-02-02T18:27:33.661369Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from src.chapter06.SpamDataset import SpamDataset\n",
    "\n",
    "# Since each row of training data has varying length, we are going to padd all \n",
    "# rows to the size of the max length of the longest row using \"<|endoftext|>\" or rather \n",
    "# its token equivalent i.e. 50256\n",
    "train_dataset = SpamDataset(\n",
    "    csv_file=\"../data/train.csv\",\n",
    "    max_length=None,\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "print(f\"Max length of training set : {train_dataset.max_length}\\n\")\n",
    "\n",
    "val_dataset = SpamDataset(\n",
    "    csv_file=\"../data/validate.csv\",\n",
    "    max_length=train_dataset.max_length,\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "print(f\"Max length of validate set : {val_dataset.max_length}\\n\")\n",
    "\n",
    "test_dataset = SpamDataset(\n",
    "    csv_file=\"../data/test.csv\",\n",
    "    max_length=train_dataset.max_length,\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "print(f\"Max length of test set : {test_dataset.max_length}\")\n"
   ],
   "id": "dfb2d42b723f5113",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of ../data/train.csv: (1045, 2)\n",
      "Max length of training set : 120\n",
      "\n",
      "Shape of ../data/validate.csv: (149, 2)\n",
      "Max length of validate set : 120\n",
      "\n",
      "Shape of ../data/test.csv: (300, 2)\n",
      "Max length of test set : 120\n"
     ]
    }
   ],
   "execution_count": 60
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### NOTE: The difference with text prediction here is that for each sample we have a class label\n",
    "#### associated with it using the datasets as inputs, we can now instantiate the data loaders\n",
    "#### similarly to when we were working with text data. However, in this case,  the targets\n",
    "#### represent class labels rather than the next tokens in the text. For instance, if we \n",
    "#### choose a batch size of 8, each batch will consist of eight training examples of length \n",
    "#### '120' and the corresponding class label of each example"
   ],
   "id": "828481d7cb91412c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-02T18:27:33.722051Z",
     "start_time": "2025-02-02T18:27:33.687294Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "torch.manual_seed(123)\n",
    "num_workers = 0\n",
    "batch_size = 8\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    dataset=train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=num_workers,\n",
    "    drop_last=True,\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    dataset=val_dataset,\n",
    "    batch_size=batch_size,\n",
    "    num_workers=num_workers,\n",
    "    drop_last=False,\n",
    ")\n",
    "test_loader = DataLoader(\n",
    "    dataset=test_dataset,\n",
    "    batch_size=batch_size,\n",
    "    num_workers=num_workers,\n",
    "    drop_last=False,\n",
    ")\n",
    "\n",
    "for input_batch, target_batch in train_loader:\n",
    "    pass\n",
    "print(\"[training] Input batch dimensions:\", input_batch.shape)\n",
    "print(\"[training] Label batch dimensions\", target_batch.shape)\n",
    "\n",
    "# Number of batches in each dataset\n",
    "print(f\"{len(train_loader)} training batches\")\n",
    "print(f\"{len(val_loader)} validation batches\")\n",
    "print(f\"{len(test_loader)} test batches\")"
   ],
   "id": "8a0f692e09c92e85",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[training] Input batch dimensions: torch.Size([8, 120])\n",
      "[training] Label batch dimensions torch.Size([8])\n",
      "130 training batches\n",
      "19 validation batches\n",
      "38 test batches\n"
     ]
    }
   ],
   "execution_count": 61
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-02T18:27:36.825927Z",
     "start_time": "2025-02-02T18:27:33.722919Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from src.chapter05.gd import download_and_load_gpt2\n",
    "#\n",
    "# Now we start initializing our model and load the pretrained weights\n",
    "#\n",
    "from src.chapter04.GPTModel import GPTModel\n",
    "# \n",
    "model_name = \"gpt2-small (124M)\"\n",
    "# INPUT_PROMPT = \"Every effort moves\"\n",
    "BASE_CONFIG = {\n",
    "    \"vocab_size\" : 50257,\n",
    "    \"context_length\" : 1024,\n",
    "    \"drop_rate\" : 0.0,\n",
    "    \"qkv_bias\" : True,   \n",
    "    \"lr\": 5e-4,\n",
    "    \"weight_decay\": 0.1\n",
    "}\n",
    "model_configs = {\n",
    "    \"gpt2-small (124M)\": {\"emb_dim\": 768, \"n_layers\": 12, \"n_heads\": 12},\n",
    "    \"gpt2-medium (355M)\": {\"emb_dim\": 1024, \"n_layers\": 24, \"n_heads\": 16},\n",
    "    \"gpt2-large (774M)\": {\"emb_dim\": 1280, \"n_layers\": 36, \"n_heads\": 20},\n",
    "    \"gpt2-xl (1558M)\": {\"emb_dim\": 1600, \"n_layers\": 48, \"n_heads\": 25},\n",
    "}\n",
    "model_name=\"gpt2-small (124M)\"\n",
    "# \n",
    "BASE_CONFIG.update({\"model_name\": model_name})\n",
    "BASE_CONFIG.update(model_configs[model_name]) \n",
    "# print(\"BASE_CONFIG:\\n\"+\"\".join(f\"\\t{k}: {v}\\n\" for k, v in sorted(BASE_CONFIG.items())))\n",
    "# \n",
    "model_size = model_name.split(\" \")[-1].lstrip(\"(\").rstrip(\")\")\n",
    "small_gpt_model = GPTModel(BASE_CONFIG).to(device)\n",
    "settings, params = download_and_load_gpt2(\n",
    "    model_size=model_size,\n",
    "    models_dir=\"../data/gpt2\"\n",
    ")\n",
    "load_weights_into_gpt(small_gpt_model, params)\n",
    "# small_gpt_model.eval()\n",
    "print(\"Loaded weights on to small_gpt_model\")"
   ],
   "id": "177359b3787f5cf7",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File already exists and is up-to-date: ../data/gpt2/124M/checkpoint\n",
      "File already exists and is up-to-date: ../data/gpt2/124M/encoder.json\n",
      "File already exists and is up-to-date: ../data/gpt2/124M/hparams.json\n",
      "File already exists and is up-to-date: ../data/gpt2/124M/model.ckpt.data-00000-of-00001\n",
      "File already exists and is up-to-date: ../data/gpt2/124M/model.ckpt.index\n",
      "File already exists and is up-to-date: ../data/gpt2/124M/model.ckpt.meta\n",
      "File already exists and is up-to-date: ../data/gpt2/124M/vocab.bpe\n",
      "Loaded weights on to small_gpt_model\n"
     ]
    }
   ],
   "execution_count": 62
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-02T18:27:36.868812Z",
     "start_time": "2025-02-02T18:27:36.826636Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# To test after loading the model weights into the GPTModel, we reuse the text generation utility\n",
    "# to generate coherent text\n",
    "#\n",
    "text_1 = \"Every effort moves you\"\n",
    "token_ids = generate_text_simple(\n",
    "    input_model=small_gpt_model,\n",
    "    tokenids=text_to_token_ids(text_1, tokenizer),\n",
    "    max_new_tokens=15,\n",
    "    context_size=BASE_CONFIG[\"context_length\"]\n",
    ")\n",
    "print(token_ids_to_text(token_ids, tokenizer))"
   ],
   "id": "b49206bdd6a269b6",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Every effort moves you forward\n"
     ]
    }
   ],
   "execution_count": 63
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-02T18:27:36.984633Z",
     "start_time": "2025-02-02T18:27:36.869533Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Before we train the classifier lets try the model as is\n",
    "text_2 = (\n",
    "    \"Is the following text 'spam'? Answer with 'yes' or 'no':\"\n",
    "    \" 'You are a winner you have been specially\"\n",
    "    \" selected to receive $1000 cash or a $2000 award.'\"\n",
    ")\n",
    "token_ids = generate_text_simple(\n",
    "    input_model=small_gpt_model,\n",
    "    tokenids=text_to_token_ids(text_2, tokenizer),\n",
    "    max_new_tokens=15,\n",
    "    context_size=BASE_CONFIG[\"context_length\"]\n",
    ")\n",
    "print(token_ids_to_text(token_ids, tokenizer))\n",
    "# Clearly the model is unable to answer"
   ],
   "id": "7e2d6f94b699b652",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is the following text 'spam'? Answer with 'yes' or 'no': 'You are a winner you have been specially selected to receive $1000 cash or a $2000 award.'\n",
      "\n"
     ]
    }
   ],
   "execution_count": 64
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Classification finetuning - Adding a Classification Head\n",
    "![image](../data/classification_tuning.png)\n"
   ],
   "id": "72d040c09f0a6a96"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-02T18:27:37.084918Z",
     "start_time": "2025-02-02T18:27:36.985563Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#\n",
    "# Essentially we will replace the last Sequential layer or Head which mapped \n",
    "# from 768 dimentions to 50257 vocabulary dimensions with a layer that maps \n",
    "# the 768 dimensions to just 2 i.e. 1 and 0\n",
    "#\n",
    "# print(small_gpt_model)\n",
    "\n",
    "# First freeze the model\n",
    "for param in small_gpt_model.parameters():\n",
    "    param.requires_grad = False\n",
    "    \n",
    "# Now replace the output layer i.e. out_head with new one\n",
    "torch.manual_seed(123)\n",
    "num_classes = 2 # Spam or Ham\n",
    "\n",
    "# Remember each layer must be on GPU\n",
    "small_gpt_model.out_head = torch.nn.Linear(\n",
    "    in_features = BASE_CONFIG[\"emb_dim\"],\n",
    "    out_features = num_classes\n",
    ").to(device)\n",
    "#\n",
    "# NOTE: This new layer will have the requires_grad set to True by default\n",
    "# that means if we train this model only this layer will be trained. \n",
    "# \n",
    "# While this is sufficient, as per Sebastian the accuracy improves if we  \n",
    "# also train the last transformer block and the last LayerNorm module. \n",
    "#\n",
    "# So Set the requires_grad for the last transformer block\n",
    "for param in small_gpt_model.trf_blocks[-1].parameters():\n",
    "    param.requires_grad = True\n",
    "    \n",
    "# Set the requires_grad for the last LayerNorm\n",
    "for param in small_gpt_model.final_norm.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "# To test it we can feed it an example text\n",
    "inputs = tokenizer.encode(\"Do you have time\")\n",
    "inputs = torch.tensor(inputs).unsqueeze(0).to(device)\n",
    "# \n",
    "print(\"Inputs: \", inputs)\n",
    "print(\"Inputs dimensions: \", inputs.shape)\n",
    "\n",
    "with torch.no_grad():\n",
    "     outputs = small_gpt_model(inputs)\n",
    "#     \n",
    "print(\"Outputs dimensions:\", outputs.shape)\n",
    "assert(outputs.shape[-1] == num_classes) # Number of output classes\n",
    "print(\"Outputs:\\n\", outputs)\n",
    "# Number of Output rows now correspond to input token count which is 4 in this case\n",
    "# but the embeddings dimension is only 2 instead of 50257 because of the new \n",
    "# output head\n",
    "\n",
    "# We don’t need to fine-tune all four output rows; instead, we can focus on a \n",
    "# single output token. In particular, we will focus on the last row corresponding \n",
    "# to the last output token BECAUSE LAST TOKEN IS THE ONLY ONE WITH ALL THE ATTENTION OF \n",
    "# ALL OF ITS PREVIOUS TOKENS\n",
    "print(\"Last output token:\", outputs[:, -1, :])\n"
   ],
   "id": "defa84126df77d59",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inputs:  tensor([[5211,  345,  423,  640]], device='mps:0')\n",
      "Inputs dimensions:  torch.Size([1, 4])\n",
      "Outputs dimensions: torch.Size([1, 4, 2])\n",
      "Outputs:\n",
      " tensor([[[-1.5854,  0.9904],\n",
      "         [-3.7235,  7.4548],\n",
      "         [-2.2661,  6.6049],\n",
      "         [-3.5983,  3.9902]]], device='mps:0')\n",
      "Last output token: tensor([[-3.5983,  3.9902]], device='mps:0')\n"
     ]
    }
   ],
   "execution_count": 65
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-02T18:27:38.762225Z",
     "start_time": "2025-02-02T18:27:37.085490Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Before we finetune the model we need to implement the model evaluation functions\n",
    "# Similar to our previous approach we take the next token id generated, calculate \n",
    "# probabilities and use argmax to get the highest probability. Only here its in 2\n",
    "# instead of 50257 dimensions\n",
    "print(f\"Last output token: {outputs[:, -1, :]}\")\n",
    "\n",
    "# Now we can get the class label\n",
    "probas = torch.softmax(outputs[:, -1, :], dim=-1)\n",
    "label = torch.argmax(probas)\n",
    "print(f\"Class Label: {label.item()}\")\n",
    "\n",
    "# This concept can be used to design an accuracy loader\n",
    "def calc_accuracy_loader(data_loader, small_gpt_model, device, num_batches=None):\n",
    "    small_gpt_model.eval()\n",
    "    correct_predictions, num_examples = 0, 0\n",
    "    if num_batches is None:\n",
    "        num_batches = len(data_loader)\n",
    "    else:\n",
    "        num_batches = min(num_batches, len(data_loader))\n",
    "        \n",
    "    for i, (input_batch, target_batch) in enumerate(data_loader):\n",
    "        if i < num_batches:\n",
    "            input_batch = input_batch.to(device)\n",
    "            target_batch = target_batch.to(device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                logits = small_gpt_model(input_batch)[:, -1, :]\n",
    "\n",
    "            predicted_labels = torch.argmax(logits, dim=-1)\n",
    "            num_examples += predicted_labels.shape[0]\n",
    "            correct_predictions += (\n",
    "                (predicted_labels == target_batch).sum().item()\n",
    "            )\n",
    "        else:\n",
    "            break\n",
    "    return correct_predictions / num_examples\n",
    "\n",
    "torch.manual_seed(123)\n",
    "small_gpt_model.to(device)\n",
    "train_accuracy = calc_accuracy_loader(\n",
    "    train_loader, small_gpt_model, device, num_batches=10\n",
    ")\n",
    "val_accuracy = calc_accuracy_loader(\n",
    "    val_loader, small_gpt_model, device, num_batches=10\n",
    ")\n",
    "test_accuracy = calc_accuracy_loader(\n",
    "    test_loader, small_gpt_model, device, num_batches=10\n",
    ")\n",
    "print(f\"Training accuracy: {train_accuracy*100:.2f}%\")\n",
    "print(f\"Validation accuracy: {val_accuracy*100:.2f}%\")\n",
    "print(f\"Test accuracy: {test_accuracy*100:.2f}%\")\n",
    "#"
   ],
   "id": "f2aafc8bf0654f7f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last output token: tensor([[-3.5983,  3.9902]], device='mps:0')\n",
      "Class Label: 1\n",
      "Training accuracy: 46.25%\n",
      "Validation accuracy: 45.00%\n",
      "Test accuracy: 48.75%\n"
     ]
    }
   ],
   "execution_count": 66
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-02T18:27:39.556425Z",
     "start_time": "2025-02-02T18:27:38.762934Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# As we can see the prediction accuracy is almost random i.e. 50%\n",
    "# \n",
    "# Before we fine tune the model we need to describe the loss function\n",
    "# Our objective is to maximize the spam classification accuracy of the \n",
    "# model, which means that the preceding code should output the correct \n",
    "# class labels: 0 for non-spam and 1 for spam.\n",
    "# \n",
    "# Because classification accuracy is not a differentiable function, we \n",
    "# can use cross-entropy loss as a proxy to maximize accuracy\n",
    "\n",
    "def calc_batch_loss(input_batch, target_batch, small_gpt_model, device):\n",
    "    input_batch = input_batch.to(device)\n",
    "    target_batch = target_batch.to(device)\n",
    "    logits = small_gpt_model(input_batch)\n",
    "    logits = logits[:, -1, :] # Get the last token\n",
    "    loss = torch.nn.functional.cross_entropy(logits, target_batch)\n",
    "    return loss\n",
    "\n",
    "# Now to calculate loss for all the batches using the above function\n",
    "def calc_all_batch_loss(data_loader, small_gpt_model, device, num_batches=None) -> float:\n",
    "    total_loss: float = 0.\n",
    "    if len(data_loader) == 0:\n",
    "        return float(\"nan\")\n",
    "    elif num_batches is None:\n",
    "        num_batches = len(data_loader)\n",
    "    else:\n",
    "        num_batches = min(num_batches, len(data_loader))\n",
    "        \n",
    "    for i, (input_batch, target_batch) in enumerate(data_loader):\n",
    "        if i < num_batches:\n",
    "            loss = calc_batch_loss(\n",
    "                input_batch, target_batch, small_gpt_model, device\n",
    "            )\n",
    "            total_loss += loss.item()\n",
    "        else:\n",
    "            break\n",
    "            \n",
    "    return total_loss / num_batches\n",
    "\n",
    "# Similar to calculating the training accuracy, we now compute \n",
    "# the initial loss for each data set\n",
    "with torch.no_grad():\n",
    "    train_loss = calc_all_batch_loss(\n",
    "        train_loader, small_gpt_model, device, num_batches=5\n",
    "    )\n",
    "    val_loss = calc_all_batch_loss(val_loader, small_gpt_model, device, num_batches=5)\n",
    "    test_loss = calc_all_batch_loss(test_loader, small_gpt_model, device, num_batches=5)\n",
    "#     \n",
    "print(f\"Training loss: {train_loss:.3f}\")\n",
    "print(f\"Validation loss: {val_loss:.3f}\")\n",
    "print(f\"Test loss: {test_loss:.3f}\")\n",
    "# "
   ],
   "id": "89c11a1d6d0f813c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 2.453\n",
      "Validation loss: 2.583\n",
      "Test loss: 2.322\n"
     ]
    }
   ],
   "execution_count": 67
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-02T18:29:05.409902Z",
     "start_time": "2025-02-02T18:27:39.561204Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Now we implement a training function to fine-tune the model\n",
    "# which means adjusting the model to minimize the training set loss\n",
    "def train_classifier_simple(small_gpt_model, \n",
    "                            train_loader, \n",
    "                            val_loader, \n",
    "                            optimizer, \n",
    "                            device, \n",
    "                            num_epochs, \n",
    "                            eval_freq, \n",
    "                            eval_iter):\n",
    "    train_losses, val_losses, train_accs, val_accs = [], [], [], []\n",
    "    examples_seen, global_step = 0, -1\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        small_gpt_model.train()               # Sets model to training mode\n",
    "        \n",
    "        for input_batch, target_batch in train_loader:\n",
    "            optimizer.zero_grad()       # reset the loss gradients\n",
    "            loss = calc_batch_loss(\n",
    "                input_batch, target_batch, small_gpt_model, device\n",
    "            )\n",
    "            loss.backward()     # calculate loss gradients\n",
    "            optimizer.step()    # backprop updates model weights\n",
    "            examples_seen += input_batch.shape[0] # Tracks examples instead of tokens\n",
    "            global_step += 1\n",
    "\n",
    "            # Optional Evaluation Step\n",
    "            if global_step % eval_freq == 0:\n",
    "                train_loss, val_loss = evaluate_model(\n",
    "                    small_gpt_model, train_loader, val_loader, device, eval_iter)\n",
    "                train_losses.append(train_loss)\n",
    "                val_losses.append(val_loss)\n",
    "                print(f\"Ep {epoch+1} (Step {global_step:06d}): \"\n",
    "                      f\"Train loss {train_loss:.3f}, \"\n",
    "                      f\"Val loss {val_loss:.3f}\"\n",
    "                )\n",
    "                \n",
    "        train_accuracy = calc_accuracy_loader(\n",
    "            train_loader, small_gpt_model, device, num_batches=eval_iter\n",
    "        )\n",
    "        val_accuracy = calc_accuracy_loader(\n",
    "            val_loader, small_gpt_model, device, num_batches=eval_iter\n",
    "        )\n",
    "\n",
    "        print(f\"Training accuracy: {train_accuracy*100:.2f}% | \", end=\"\")\n",
    "        print(f\"Validation accuracy: {val_accuracy*100:.2f}%\")\n",
    "        train_accs.append(train_accuracy)\n",
    "        val_accs.append(val_accuracy)\n",
    "\n",
    "    return train_losses, val_losses, train_accs, val_accs, examples_seen\n",
    "\n",
    "def evaluate_model(small_gpt_model, train_loader, val_loader, device, eval_iter):\n",
    "    small_gpt_model.eval()\n",
    "    with torch.no_grad():\n",
    "        train_loss = calc_all_batch_loss(\n",
    "            train_loader, small_gpt_model, device, num_batches=eval_iter\n",
    "        )\n",
    "        val_loss = calc_all_batch_loss(\n",
    "            val_loader, small_gpt_model, device, num_batches=eval_iter\n",
    "        )\n",
    "    small_gpt_model.train()\n",
    "    return train_loss, val_loss\n",
    "\n",
    "# Now we initialize the optimizer \n",
    "import time\n",
    "# \n",
    "torch.manual_seed(123)\n",
    "start_time = time.time()\n",
    "optimizer = torch.optim.AdamW(small_gpt_model.parameters())\n",
    "num_epochs = 5\n",
    "# \n",
    "train_losses, val_losses, train_accs, val_accs, examples_seen = train_classifier_simple(\n",
    "    small_gpt_model, train_loader, val_loader, optimizer, device, \n",
    "    num_epochs=num_epochs, eval_freq=50, eval_iter=5\n",
    ")\n",
    "# \n",
    "end_time = time.time()\n",
    "execution_time_minutes = (end_time - start_time) / 60\n",
    "print(f\"Training completed in {execution_time_minutes:.2f} minutes.\")\n"
   ],
   "id": "63bdb536dc7a9dcb",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 1 (Step 000000): Train loss 2.281, Val loss 2.021\n",
      "Ep 1 (Step 000050): Train loss 0.170, Val loss 0.188\n",
      "Ep 1 (Step 000100): Train loss 0.123, Val loss 0.056\n",
      "Training accuracy: 97.50% | Validation accuracy: 100.00%\n",
      "Ep 2 (Step 000150): Train loss 0.159, Val loss 0.131\n",
      "Ep 2 (Step 000200): Train loss 0.002, Val loss 0.022\n",
      "Ep 2 (Step 000250): Train loss 0.021, Val loss 0.073\n",
      "Training accuracy: 97.50% | Validation accuracy: 92.50%\n",
      "Ep 3 (Step 000300): Train loss 0.146, Val loss 0.107\n",
      "Ep 3 (Step 000350): Train loss 0.018, Val loss 0.119\n",
      "Training accuracy: 97.50% | Validation accuracy: 95.00%\n",
      "Ep 4 (Step 000400): Train loss 0.004, Val loss 0.014\n",
      "Ep 4 (Step 000450): Train loss 0.013, Val loss 0.038\n",
      "Ep 4 (Step 000500): Train loss 0.121, Val loss 0.022\n",
      "Training accuracy: 97.50% | Validation accuracy: 100.00%\n",
      "Ep 5 (Step 000550): Train loss 0.034, Val loss 0.080\n",
      "Ep 5 (Step 000600): Train loss 0.151, Val loss 0.046\n",
      "Training accuracy: 100.00% | Validation accuracy: 95.00%\n",
      "Training completed in 1.43 minutes.\n"
     ]
    }
   ],
   "execution_count": 68
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-02T18:29:05.568741Z",
     "start_time": "2025-02-02T18:29:05.410667Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot the loss function during training\n",
    "def plot_values(\n",
    "        epochs_seen, examples_seen, train_values, val_values,\n",
    "        label=\"loss\"):\n",
    "    fig, ax1 = plt.subplots(figsize=(5, 3))\n",
    "\n",
    "\n",
    "    ax1.plot(epochs_seen, train_values, label=f\"Training {label}\")\n",
    "    ax1.plot(\n",
    "        epochs_seen, val_values, linestyle=\"-.\",\n",
    "        label=f\"Validation {label}\"\n",
    "    )\n",
    "    ax1.set_xlabel(\"Epochs\")\n",
    "    ax1.set_ylabel(label.capitalize())\n",
    "    ax1.legend()\n",
    "\n",
    "\n",
    "    ax2 = ax1.twiny()\n",
    "    ax2.plot(examples_seen, train_values, alpha=0)\n",
    "    ax2.set_xlabel(\"Examples seen\")\n",
    "\n",
    "    fig.tight_layout()\n",
    "    plt.savefig(f\"{label}-plot.pdf\")\n",
    "    plt.show()\n",
    "\n",
    "epochs_tensor = torch.linspace(0, num_epochs, len(train_losses))\n",
    "examples_seen_tensor = torch.linspace(0, examples_seen, len(train_losses))\n",
    "\n",
    "plot_values(epochs_tensor, examples_seen_tensor, train_losses, val_losses)"
   ],
   "id": "2d461d605119d1de",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 500x300 with 2 Axes>"
      ],
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 69
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-02T18:29:15.563932Z",
     "start_time": "2025-02-02T18:29:05.569404Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Now lets plot the resulting accuracy\n",
    "epochs_tensor = torch.linspace(0, num_epochs, len(train_accs))\n",
    "examples_seen_tensor = torch.linspace(0, examples_seen, len(train_accs))\n",
    "\n",
    "plot_values(\n",
    "    epochs_tensor, examples_seen_tensor, train_accs, val_accs,\n",
    "    label=\"accuracy\"\n",
    ")\n",
    "\n",
    "# Now we must calculate performance metrics for the training\n",
    "train_accuracy = calc_accuracy_loader(train_loader, small_gpt_model, device)\n",
    "val_accuracy = calc_accuracy_loader(val_loader, small_gpt_model, device)\n",
    "test_accuracy = calc_accuracy_loader(test_loader, small_gpt_model, device)\n",
    "\n",
    "print(f\"Training accuracy: {train_accuracy*100:.2f} %\")\n",
    "print(f\"Validation accuracy: {val_accuracy*100:.2f} %\")\n",
    "print(f\"Test accuracy: {test_accuracy*100:.2f} %\")"
   ],
   "id": "968fde4292b581cc",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 500x300 with 2 Axes>"
      ],
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 98.56 %\n",
      "Validation accuracy: 95.97 %\n",
      "Test accuracy: 97.33 %\n"
     ]
    }
   ],
   "execution_count": 70
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-02T18:29:16.414796Z",
     "start_time": "2025-02-02T18:29:15.564659Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Now let's classify spam messages\n",
    "def classify_review(\n",
    "        text, small_gpt_model, tokenizer, device, max_length=None,\n",
    "        pad_token_id=50256):\n",
    "    small_gpt_model.eval()\n",
    "\n",
    "    input_ids = tokenizer.encode(text)\n",
    "    supported_context_length = small_gpt_model.pos_emb.weight.shape[1]\n",
    "\n",
    "    input_ids = input_ids[:min(\n",
    "        max_length, supported_context_length\n",
    "    )]\n",
    "\n",
    "    input_ids += [pad_token_id] * (max_length - len(input_ids))\n",
    "\n",
    "    input_tensor = torch.tensor(\n",
    "        input_ids, device=device\n",
    "    ).unsqueeze(0)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        logits = small_gpt_model(input_tensor)[:, -1, :]\n",
    "    predicted_label = torch.argmax(logits, dim=-1).item()\n",
    "\n",
    "    return \"Spam\" if predicted_label == 1 else \"Not Spam\"\n",
    "\n",
    "text_1 = (\n",
    "    \"You are a winner you have been specially\"\n",
    "    \" selected to receive $1000 cash or a $2000 award.\"\n",
    ")\n",
    "\n",
    "print(f\"{classify_review(\n",
    "    text_1, small_gpt_model, tokenizer, device, max_length=train_dataset.max_length\n",
    ")} : {text_1}\")\n",
    "\n",
    "text_2 = (\n",
    "    \"Hey, just wanted to check if we're still on\"\n",
    "    \" for dinner tonight? Let me know!\"\n",
    ")\n",
    "\n",
    "print(f\"{classify_review(\n",
    "    text_2, small_gpt_model, tokenizer, device, max_length=train_dataset.max_length\n",
    ")}: {text_2}\")\n",
    "\n",
    "\n",
    "# Save the model\n",
    "cls_model_path=\"/Users/amlanchatterjee/Documents/ws/python/PycharmProjects/SimpleLLMProject/models/review_classifier.pth\"\n",
    "torch.save(small_gpt_model.state_dict(), cls_model_path)\n",
    "print(f\"Saved model at {cls_model_path}\")"
   ],
   "id": "99208e2db11a0271",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spam : You are a winner you have been specially selected to receive $1000 cash or a $2000 award.\n",
      "Not Spam: Hey, just wanted to check if we're still on for dinner tonight? Let me know!\n",
      "Saved model at /Users/amlanchatterjee/Documents/ws/python/PycharmProjects/SimpleLLMProject/models/review_classifier.pth\n"
     ]
    }
   ],
   "execution_count": 71
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-02T18:29:16.594610Z",
     "start_time": "2025-02-02T18:29:16.415630Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# We can load this model again as well\n",
    "print(cls_model_path)\n",
    "model_state_dict = torch.load(cls_model_path, map_location=device)\n",
    "small_gpt_model.load_state_dict(model_state_dict)\n"
   ],
   "id": "117ef11dd9ff2827",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/amlanchatterjee/Documents/ws/python/PycharmProjects/SimpleLLMProject/models/review_classifier.pth\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 72
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-02T18:29:16.596532Z",
     "start_time": "2025-02-02T18:29:16.595242Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "89885c38d68e2ab",
   "outputs": [],
   "execution_count": 72
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-02T18:29:16.598246Z",
     "start_time": "2025-02-02T18:29:16.597010Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "cea14e70233ecfc5",
   "outputs": [],
   "execution_count": 72
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
