{
 "cells": [
  {
   "cell_type": "code",
   "id": "849558533e67652f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-10T02:47:59.815277Z",
     "start_time": "2025-02-10T02:47:59.806304Z"
    }
   },
   "source": [
    "import os\n",
    "import urllib\n",
    "import torch\n",
    "from torch import Tensor\n",
    "from torch.utils.data import DataLoader\n",
    "from importlib.metadata import version\n",
    "\n",
    "from DataloaderV1 import DataloaderV1\n",
    "from SimpleTokenizerV1 import SimpleTokenizerV1\n",
    "from SimpleTokenizerV2 import SimpleTokenizerV2\n",
    "\n",
    "print(\"torch version:\", version(\"torch\"))\n",
    "print(\"tiktoken version:\", version(\"tiktoken\"))\n",
    "\n",
    "\n",
    "\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "    x: Tensor = torch.ones(1, device=device)\n",
    "    print(f\"x = {x} using {device} backend\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    x: Tensor = torch.ones(1, device=device)\n",
    "    print(x)\n",
    "# print(f\"Running on : {device}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch version: 2.5.1\n",
      "tiktoken version: 0.8.0\n",
      "x = tensor([1.], device='mps:0') using mps backend\n"
     ]
    }
   ],
   "execution_count": 193
  },
  {
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-02-10T02:47:59.826217Z",
     "start_time": "2025-02-10T02:47:59.823661Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def get_some_text():\n",
    "    # Download a text (book)\n",
    "    bookUrl = \"https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/ch02/01_main-chapter-code/the-verdict.txt\"\n",
    "    filepath = \"../../data/the-verdict.txt\"\n",
    "    # print(file_path)\n",
    "    if not os.path.exists(filepath):\n",
    "        urllib.request.urlretrieve(bookUrl, filepath)\n",
    "\n",
    "    with open(filepath, \"r\", encoding=\"utf-8\") as f:\n",
    "        rawtext = f.read()\n",
    "\n",
    "    print(\"Total characters in the story: \", len(rawtext))\n",
    "    print(\"Total Lines in raw text: \", rawtext.count(\"\\n\"))\n",
    "    return rawtext\n",
    "\n",
    "raw_text = get_some_text()\n",
    "print(\"Some text: \", raw_text[:49])"
   ],
   "id": "initial_id",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total characters in the story:  20479\n",
      "Total Lines in raw text:  164\n",
      "Some text:  I HAD always thought Jack Gisburn rather a cheap \n"
     ]
    }
   ],
   "execution_count": 194
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-10T02:47:59.841983Z",
     "start_time": "2025-02-10T02:47:59.839799Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import re\n",
    "\n",
    "text = \"Hello, world. This, is a test.\"\n",
    "result = re.split(r'(\\s)', text)\n",
    "print(result)"
   ],
   "id": "a3dbe722a83c6aac",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello,', ' ', 'world.', ' ', 'This,', ' ', 'is', ' ', 'a', ' ', 'test.']\n"
     ]
    }
   ],
   "execution_count": 195
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-10T02:47:59.861338Z",
     "start_time": "2025-02-10T02:47:59.859611Z"
    }
   },
   "cell_type": "code",
   "source": [
    "result = re.split(r'([,.]|\\s)', text)\n",
    "print(result)"
   ],
   "id": "36a66be0aba672b9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', ',', '', ' ', 'world', '.', '', ' ', 'This', ',', '', ' ', 'is', ' ', 'a', ' ', 'test', '.', '']\n"
     ]
    }
   ],
   "execution_count": 196
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-10T02:47:59.943529Z",
     "start_time": "2025-02-10T02:47:59.941658Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Strip whitespace from each item and then filter out any empty strings.\n",
    "result = [item for item in result if item.strip()]\n",
    "print(result)"
   ],
   "id": "7979e43483049525",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', ',', 'world', '.', 'This', ',', 'is', 'a', 'test', '.']\n"
     ]
    }
   ],
   "execution_count": 197
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-10T02:48:00.078728Z",
     "start_time": "2025-02-10T02:48:00.075811Z"
    }
   },
   "cell_type": "code",
   "source": [
    "preprocessed = re.split(r'([,.:;?_!\"()\\'\\\\]|--|\\s)', raw_text)\n",
    "preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
    "print(preprocessed[:30])"
   ],
   "id": "efe1498212194c46",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'HAD', 'always', 'thought', 'Jack', 'Gisburn', 'rather', 'a', 'cheap', 'genius', '--', 'though', 'a', 'good', 'fellow', 'enough', '--', 'so', 'it', 'was', 'no', 'great', 'surprise', 'to', 'me', 'to', 'hear', 'that', ',', 'in']\n"
     ]
    }
   ],
   "execution_count": 198
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-10T02:48:00.147773Z",
     "start_time": "2025-02-10T02:48:00.145789Z"
    }
   },
   "cell_type": "code",
   "source": "print(len(preprocessed))",
   "id": "c9199260c1d7115d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4690\n"
     ]
    }
   ],
   "execution_count": 199
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Converting tokens into token IDs\n",
   "id": "8dc9ebf4f458329e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-10T02:48:00.233559Z",
     "start_time": "2025-02-10T02:48:00.231325Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Now we need to generate token IDs\n",
    "# Now let us create a list of all unique tokens and sort them alphabetically to determine the vocabulary size\n",
    "all_uniq_words = sorted(set(preprocessed))\n",
    "vocab_size = len(all_uniq_words)\n",
    "print(\"Vocab size: \", vocab_size)"
   ],
   "id": "530f64344ef2dc9c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size:  1130\n"
     ]
    }
   ],
   "execution_count": 200
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-10T02:48:00.257623Z",
     "start_time": "2025-02-10T02:48:00.255311Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Now that we know the vocabulary size, lets enumerate and assign some numbers to them\n",
    "vocab = {token:integer for integer, token in enumerate(all_uniq_words)}\n",
    "for i, item in enumerate(vocab.items()):\n",
    "    print(item)\n",
    "    if i >= 10:\n",
    "        break"
   ],
   "id": "5bfc30537c13ab0c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('!', 0)\n",
      "('\"', 1)\n",
      "(\"'\", 2)\n",
      "('(', 3)\n",
      "(')', 4)\n",
      "(',', 5)\n",
      "('--', 6)\n",
      "('.', 7)\n",
      "(':', 8)\n",
      "(';', 9)\n",
      "('?', 10)\n"
     ]
    }
   ],
   "execution_count": 201
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-10T02:48:00.338678Z",
     "start_time": "2025-02-10T02:48:00.336648Z"
    }
   },
   "cell_type": "code",
   "source": [
    "tokenizer = SimpleTokenizerV1(vocab)\n",
    "text = \"\"\"\"It's the last he painted, you know,\"\n",
    "           Mrs. Gisburn said with pardonable pride.\"\"\"\n",
    "ids = tokenizer.encode(text)\n",
    "print(ids)"
   ],
   "id": "99adff876d09ded2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 56, 2, 850, 988, 602, 533, 746, 5, 1126, 596, 5, 1, 67, 7, 38, 851, 1108, 754, 793, 7]\n"
     ]
    }
   ],
   "execution_count": 202
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-10T02:48:00.457124Z",
     "start_time": "2025-02-10T02:48:00.454686Z"
    }
   },
   "cell_type": "code",
   "source": "tokenizer.decode(ids)",
   "id": "6d373ea2b27bf398",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\" It\\' s the last he painted, you know,\" Mrs. Gisburn said with pardonable pride.'"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 203
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-10T02:48:00.512902Z",
     "start_time": "2025-02-10T02:48:00.510689Z"
    }
   },
   "cell_type": "code",
   "source": "tokenizer.decode(tokenizer.encode(text))",
   "id": "12c82734b499afec",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\" It\\' s the last he painted, you know,\" Mrs. Gisburn said with pardonable pride.'"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 204
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Some of these special tokens are:\n",
    "- [BOS] (beginning of sequence) marks the beginning of text\n",
    "- [EOS] (end of sequence) marks where the text ends (this is usually used to concatenate multiple unrelated texts, e.g., two different Wikipedia# articles or two different books, and so on)\n",
    "- [PAD] (padding) if we train LLMs with a batch size greater than 1 (we may include multiple texts with different lengths; with the padding token we pad the shorter texts to the longest length so that all texts have an equal length)\n",
    "- [UNK] to represent words that are not included in the vocabulary\n",
    "- Note: GPT-2 does not need any of these tokens mentioned above but only uses an <|endoftext|> token to reduce complexity. <|endoftext|> token is analogous to the [EOS] token mentioned above\n",
    "- *GPT also uses the <|endoftext|> for padding (since we typically use a mask when training on batched inputs, we would not attend padded tokens\n",
    "anyways, so it does not matter what these tokens are)*\n",
    "- GPT-2 does not use an <UNK> token for out-of-vocabulary words; instead,\n",
    "- GPT-2 uses a byte-pair encoding (BPE) tokenizer, which breaks down words into subword units which we will discuss in a later section. We use the <|endoftext|> tokens between two independent sources of text\n"
   ],
   "id": "6ef4f8a2df565f23"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-10T02:48:00.535032Z",
     "start_time": "2025-02-10T02:48:00.532538Z"
    }
   },
   "cell_type": "code",
   "source": [
    "all_tokens = sorted(list(set(preprocessed)))\n",
    "all_tokens.extend([\"<|endoftext|>\", \"<|unk|>\"])         # To handle unknown tokens\n",
    "vocab = {token:integer for integer,token in enumerate(all_tokens)}\n",
    "print(len(vocab.items()))"
   ],
   "id": "a1e12a5ca30772ad",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1132\n"
     ]
    }
   ],
   "execution_count": 205
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-10T02:48:00.591043Z",
     "start_time": "2025-02-10T02:48:00.588879Z"
    }
   },
   "cell_type": "code",
   "source": [
    "tokenizer = SimpleTokenizerV2(vocab)\n",
    "text1 = \"Hello, do you like tea?\"\n",
    "text2 = \"In the sunlit terraces of the palace.\"\n",
    "text = \" <|endoftext|> \".join((text1, text2))\n",
    "print(text)"
   ],
   "id": "4d7b9d47b449386",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, do you like tea? <|endoftext|> In the sunlit terraces of the palace.\n"
     ]
    }
   ],
   "execution_count": 206
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-10T02:48:00.700440Z",
     "start_time": "2025-02-10T02:48:00.698260Z"
    }
   },
   "cell_type": "code",
   "source": "tokenizer.encode(text)",
   "id": "df51dc7db10c97b5",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1131, 5, 355, 1126, 628, 975, 10, 1130, 55, 988, 956, 984, 722, 988, 1131, 7]"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 207
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-10T02:48:00.758989Z",
     "start_time": "2025-02-10T02:48:00.756702Z"
    }
   },
   "cell_type": "code",
   "source": "tokenizer.decode(tokenizer.encode(text))",
   "id": "7a920660439913d2",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|unk|>, do you like tea? <|endoftext|> In the sunlit terraces of the <|unk|>.'"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 208
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Byte Pair Encoding",
   "id": "e5cd0b6e706705cd"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-10T02:48:00.781327Z",
     "start_time": "2025-02-10T02:48:00.778630Z"
    }
   },
   "cell_type": "code",
   "source": [
    "### Byte Pair Encoding\n",
    "\n",
    "from importlib.metadata import version\n",
    "import tiktoken\n",
    "print(\"Tiktoken version: \", version(\"tiktoken\"))"
   ],
   "id": "194be9fd35916a7c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tiktoken version:  0.8.0\n"
     ]
    }
   ],
   "execution_count": 209
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-10T02:48:00.839893Z",
     "start_time": "2025-02-10T02:48:00.837395Z"
    }
   },
   "cell_type": "code",
   "source": [
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "text = (\n",
    "    \"Hello, do you like tea? <|endoftext|> In the sunlit terraces\"\n",
    "     \"of someunknownPlace.\"\n",
    ")\n",
    "\n",
    "# BPE tokenizers break down unknown words into subwords and individual characters.\n",
    "encoded_integers = tokenizer.encode(text, allowed_special={\"<|endoftext|>\"})\n",
    "print(f\"Encoded: {encoded_integers}\")\n",
    "decoded_strings = tokenizer.decode(encoded_integers)\n",
    "print(f\"Decoded: {decoded_strings}\\n\")\n",
    "#\n",
    "print(tokenizer.encode(\"Akwirw ier\"))\n",
    "print(tokenizer.decode(tokenizer.encode(\"Akwirw ier\")))"
   ],
   "id": "939287013f68ac9a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded: [15496, 11, 466, 345, 588, 8887, 30, 220, 50256, 554, 262, 4252, 18250, 8812, 2114, 1659, 617, 34680, 27271, 13]\n",
      "Decoded: Hello, do you like tea? <|endoftext|> In the sunlit terracesof someunknownPlace.\n",
      "\n",
      "[33901, 86, 343, 86, 220, 959]\n",
      "Akwirw ier\n"
     ]
    }
   ],
   "execution_count": 210
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Data sampling with a sliding window",
   "id": "eccf8e65843a3196"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-10T02:48:00.952970Z",
     "start_time": "2025-02-10T02:48:00.949080Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#\n",
    "with open(\"../../data/the-verdict.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    raw_text = f.read()\n",
    "    f.close()\n",
    "\n",
    "# Encoding with BPE tokenizer\n",
    "enc_text = tokenizer.encode(raw_text)\n",
    "print(len(enc_text))"
   ],
   "id": "b5f5a372cfe81287",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5145\n"
     ]
    }
   ],
   "execution_count": 211
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "- For each text chunk, we want the inputs and targets\n",
    "- Since we want the model to predict the next word, the targets are the inputs shifted by one position to the right"
   ],
   "id": "38176c1c96cdcb60"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-10T02:48:01.010275Z",
     "start_time": "2025-02-10T02:48:01.008189Z"
    }
   },
   "cell_type": "code",
   "source": [
    "enc_sample = enc_text[50:]\n",
    "context_size = 4\n",
    "#\n",
    "x = enc_sample[:context_size]\n",
    "y = enc_sample[1:context_size+1]\n",
    "\n",
    "print(f\"x: {x}\")\n",
    "print(f\"y:      {y}\")"
   ],
   "id": "513485e4005c995d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: [290, 4920, 2241, 287]\n",
      "y:      [4920, 2241, 287, 257]\n"
     ]
    }
   ],
   "execution_count": 212
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-10T02:48:01.033779Z",
     "start_time": "2025-02-10T02:48:01.031522Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Next word prediction tasks can now be created by\n",
    "for i in range(1, context_size + 1):\n",
    "    context = enc_sample[:i]\n",
    "    desired = enc_sample[i]\n",
    "    # Now we create the input output target pairs\n",
    "    print(tokenizer.decode(context), \"---->\", tokenizer.decode([desired]))"
   ],
   "id": "17f9db856254dbc1",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " and ---->  established\n",
      " and established ---->  himself\n",
      " and established himself ---->  in\n",
      " and established himself in ---->  a\n"
     ]
    }
   ],
   "execution_count": 213
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "- We will take care of the next-word prediction later chapter after we covered the attention mechanism\n",
    "- For now, we will implement a simple data loader that iterates over an input dataset and returns the inputs and targets shifted by one"
   ],
   "id": "ba28ec8331aae2fe"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-10T03:03:49.148654Z",
     "start_time": "2025-02-10T03:03:49.135435Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Finally we need to create the embeddings for the tokens\n",
    "# If we have a batch size of 8 with 4 tokens each it'll be an 8 x 4 x 256 tensor\n",
    "\n",
    "max_length = 4\n",
    "\n",
    "mydataloader = DataloaderV1(batch_size=8,\n",
    "                            max_length=max_length,\n",
    "                            stride=4,\n",
    "                            shuffle=False,\n",
    "                            drop_last=True,\n",
    "                            num_workers=0)\n",
    "#\n",
    "dataloader:DataLoader = mydataloader.create_dataloader_v1(txt=raw_text)\n",
    "data_iter = iter(dataloader)\n",
    "inputs, targets = next(data_iter)\n",
    "#\n",
    "print(\"Inputs :\\n\", inputs)\n",
    "print(\"Input tensor shape: \", inputs.shape)\n",
    "print(\"Targets:\\n\", targets)\n",
    "print(\"Target tensor shape: \", targets.shape)"
   ],
   "id": "98ee1f87ef42af82",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inputs :\n",
      " tensor([[   40,   367,  2885,  1464],\n",
      "        [ 1807,  3619,   402,   271],\n",
      "        [10899,  2138,   257,  7026],\n",
      "        [15632,   438,  2016,   257],\n",
      "        [  922,  5891,  1576,   438],\n",
      "        [  568,   340,   373,   645],\n",
      "        [ 1049,  5975,   284,   502],\n",
      "        [  284,  3285,   326,    11]])\n",
      "Input tensor shape:  torch.Size([8, 4])\n",
      "Targets:\n",
      " tensor([[  367,  2885,  1464,  1807],\n",
      "        [ 3619,   402,   271, 10899],\n",
      "        [ 2138,   257,  7026, 15632],\n",
      "        [  438,  2016,   257,   922],\n",
      "        [ 5891,  1576,   438,   568],\n",
      "        [  340,   373,   645,  1049],\n",
      "        [ 5975,   284,   502,   284],\n",
      "        [ 3285,   326,    11,   287]])\n",
      "Target tensor shape:  torch.Size([8, 4])\n"
     ]
    }
   ],
   "execution_count": 231
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Creating token embeddings\n",
    "- The data is already almost ready for an LLM\n",
    "- But lastly let us embed the tokens in a continuous vector representation using an embedding layer\n",
    "- Usually, these embedding layers are part of the LLM itself and are updated (trained) during model training\n"
   ],
   "id": "faa4cb77f62b453c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-10T03:06:30.694773Z",
     "start_time": "2025-02-10T03:06:30.690233Z"
    }
   },
   "cell_type": "code",
   "source": [
    "input_ids = torch.tensor([2, 3, 5, 1])\n",
    "vocab_size = 6\n",
    "output_dim = 3\n",
    "\n",
    "# The Embedding layer approach is essentially just a more efficient way of\n",
    "# implementing one-hot encoding followed by matrix multiplication in a\n",
    "# fully-connected layer\n",
    "\n",
    "torch.manual_seed(123)\n",
    "embedding_layer = torch.nn.Embedding(vocab_size, output_dim)\n",
    "print(embedding_layer.weight)"
   ],
   "id": "5a9605bd57550b5d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 0.3374, -0.1778, -0.1690],\n",
      "        [ 0.9178,  1.5810,  1.3010],\n",
      "        [ 1.2753, -0.2010, -0.1606],\n",
      "        [-0.4015,  0.9666, -1.1481],\n",
      "        [-1.1589,  0.3255, -0.6315],\n",
      "        [-2.8400, -0.7849, -1.4096]], requires_grad=True)\n"
     ]
    }
   ],
   "execution_count": 232
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "- Because the embedding layer is just a more efficient implementation that is equivalent to the one-hot encoding and matrix-multiplication approach it can be seen as a neural network layer that can be optimized via backpropagation",
   "id": "e35e2edea5f342ae"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-10T03:12:16.615336Z",
     "start_time": "2025-02-10T03:12:16.611716Z"
    }
   },
   "cell_type": "code",
   "source": [
    "## For instance, To convert a token with id 3 into a 3-dimensional vector, we do the following:\n",
    "print(embedding_layer(torch.tensor([3])))"
   ],
   "id": "8fdcdabc5c8adf7b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.4015,  0.9666, -1.1481]], grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "execution_count": 234
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "- Note that the above is the 4th row in the embedding_layer weight matrix\n",
    "- To embed all four input_ids values above, we do"
   ],
   "id": "d825ee133e36ecea"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-10T03:42:12.018492Z",
     "start_time": "2025-02-10T03:42:12.014830Z"
    }
   },
   "cell_type": "code",
   "source": "print(embedding_layer(input_ids))",
   "id": "21e2bfabf8dfe85b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.2753, -0.2010, -0.1606],\n",
      "        [-0.4015,  0.9666, -1.1481],\n",
      "        [-2.8400, -0.7849, -1.4096],\n",
      "        [ 0.9178,  1.5810,  1.3010]], grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "execution_count": 255
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-10T03:42:45.049866Z",
     "start_time": "2025-02-10T03:42:45.039284Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Since self-attentions are position agnostic, we should add some positional data.\n",
    "# Absolute and relative positional data can be added. So let's create embeddings with say 256 dimensions\n",
    "max_length = 4\n",
    "dataloader = DataloaderV1(batch_size=8, max_length=max_length, stride=max_length, shuffle=False).create_dataloader_v1(raw_text)\n",
    "data_iter = iter(dataloader)\n",
    "inputs, targets = next(data_iter)\n",
    "print(\"Token IDs:\\n\", inputs)\n",
    "print(\"\\nInputs shape:\\n\", inputs.shape)"
   ],
   "id": "ffa24f13008a5b03",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token IDs:\n",
      " tensor([[   40,   367,  2885,  1464],\n",
      "        [ 1807,  3619,   402,   271],\n",
      "        [10899,  2138,   257,  7026],\n",
      "        [15632,   438,  2016,   257],\n",
      "        [  922,  5891,  1576,   438],\n",
      "        [  568,   340,   373,   645],\n",
      "        [ 1049,  5975,   284,   502],\n",
      "        [  284,  3285,   326,    11]])\n",
      "\n",
      "Inputs shape:\n",
      " torch.Size([8, 4])\n"
     ]
    }
   ],
   "execution_count": 256
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "- If we sample data from the dataloader, we embed the tokens in each batch into a 256-dimensional vector\n",
    "- If we have a batch size of 8 with 4 tokens each, this results in a *8 x 4 x 256 tensor*\n"
   ],
   "id": "80ec7dbe1c126794"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-10T03:29:04.069694Z",
     "start_time": "2025-02-10T03:29:03.932269Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Now lets embed the input tensors\n",
    "vocab_size = 50257\n",
    "output_dim = 256\n",
    "token_embedding_layer = torch.nn.Embedding(num_embeddings=vocab_size, embedding_dim=output_dim)\n",
    "token_embeddings = token_embedding_layer(inputs)\n",
    "print(\"Token embeddings shape: \", token_embeddings.shape)"
   ],
   "id": "86e5ab6a9807d82b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token embeddings shape:  torch.Size([8, 4, 256])\n"
     ]
    }
   ],
   "execution_count": 243
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "- GPT-2 uses absolute position embeddings, so we just create another embedding layer that has the same embedding dimension as the token_embedding_ layer",
   "id": "caab528faee04c19"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-10T03:34:32.363591Z",
     "start_time": "2025-02-10T03:34:32.360267Z"
    }
   },
   "cell_type": "code",
   "source": [
    "context_length = max_length # context length is the length of positions we care for attention\n",
    "pos_embedding_layer = torch.nn.Embedding(context_length, output_dim)\n",
    "# How the embedding layer weights look like\n",
    "print(\"Positional Embedding Layer Weights: \\n\", pos_embedding_layer.weight)"
   ],
   "id": "1756b80a3f542aaa",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positional Embedding Weights: \n",
      " Parameter containing:\n",
      "tensor([[-1.0722, -0.3881, -0.9748,  ...,  0.1415,  0.2180,  0.6223],\n",
      "        [ 1.3004,  0.3771, -0.5962,  ..., -0.0475,  0.5450,  0.6105],\n",
      "        [ 1.5109, -2.0059,  0.9972,  ..., -1.6433, -0.1522, -0.7559],\n",
      "        [-0.1626, -1.0717,  0.3817,  ...,  0.1037, -0.4367, -0.8756]],\n",
      "       requires_grad=True)\n"
     ]
    }
   ],
   "execution_count": 248
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-10T03:37:36.587719Z",
     "start_time": "2025-02-10T03:37:36.585711Z"
    }
   },
   "cell_type": "code",
   "source": [
    "pos_embeddings = pos_embedding_layer(torch.arange(context_length))\n",
    "print(\"Positional Embeddings Shape: \", pos_embeddings.shape) # 4x256"
   ],
   "id": "d8a8b6fb25e30f9f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positional Embeddings Shape:  torch.Size([4, 256])\n"
     ]
    }
   ],
   "execution_count": 253
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "- To create the input embeddings used in an LLM, we simply add the token andthe positional embeddings",
   "id": "5a539f95d8186615"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-10T03:37:23.214123Z",
     "start_time": "2025-02-10T03:37:23.212029Z"
    }
   },
   "cell_type": "code",
   "source": [
    "input_embeddings = token_embeddings + pos_embeddings\n",
    "print(\"Position Merged Input Embeddings Shape: \", input_embeddings.shape)"
   ],
   "id": "a3868daf607a4d52",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Position Merged Input Embeddings Shape:  torch.Size([8, 4, 256])\n"
     ]
    }
   ],
   "execution_count": 251
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-10T03:14:33.758203Z",
     "start_time": "2025-02-10T03:14:33.752764Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Now lets look at the dataloader\n",
    "for batch in dataloader:\n",
    "    inputs, targets = batch\n",
    "    token_embeddings = token_embedding_layer(inputs)\n",
    "    pos_embeddings = pos_embedding_layer(torch.arange(max_length))\n",
    "    input_embeddings = token_embeddings + pos_embeddings\n",
    "    break\n",
    "\n",
    "print(\"Input Embeddings Shape: \", input_embeddings.shape)\n",
    "print(\"Inputs tensor: \", x)\n",
    "print(\"Targets tensor: \", y)"
   ],
   "id": "746203c6444aa375",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Embeddings Shape:  torch.Size([8, 4, 256])\n",
      "Inputs tensor:  [290, 4920, 2241, 287]\n",
      "Targets tensor:  [4920, 2241, 287, 257]\n"
     ]
    }
   ],
   "execution_count": 238
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-10T02:48:01.510245Z",
     "start_time": "2025-02-10T02:48:01.508802Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "eef339cc9b5a0b2c",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
